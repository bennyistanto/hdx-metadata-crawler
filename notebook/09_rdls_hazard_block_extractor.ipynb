{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 09: RDLS Hazard Block Extractor\n",
    "\n",
    "**Purpose**: Extract and populate RDLS v0.3 Hazard component blocks from HDX metadata using the Signal Dictionary.\n",
    "\n",
    "**Input**:\n",
    "- HDX dataset metadata JSON files\n",
    "- Signal Dictionary (`config/signal_dictionary.yaml`)\n",
    "- RDLS Schema (`rdls/schema/rdls_schema_v0.3.json`)\n",
    "\n",
    "**Output**:\n",
    "- Hazard block extractions with confidence scores\n",
    "- Extraction QA report\n",
    "- Updated RDLS records with populated hazard blocks\n",
    "\n",
    "**RDLS Hazard Block Structure**:\n",
    "```json\n",
    "\"hazard\": {\n",
    "  \"event_sets\": [{\n",
    "    \"id\": \"...\",\n",
    "    \"analysis_type\": \"probabilistic|deterministic|empirical\",\n",
    "    \"hazards\": [{\n",
    "      \"id\": \"...\",\n",
    "      \"type\": \"flood|earthquake|...\",\n",
    "      \"hazard_process\": \"fluvial_flood|ground_motion|...\",\n",
    "      \"intensity_measure\": \"...\"\n",
    "    }],\n",
    "    \"events\": [{\n",
    "      \"occurrence\": {\n",
    "        \"probabilistic\": { \"return_period\": 100 },\n",
    "        \"empirical\": { \"temporal\": {...} }\n",
    "      }\n",
    "    }]\n",
    "  }]\n",
    "}\n",
    "```\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Define Paths\n",
    "\"\"\"\n",
    "\n",
    "# Repository root\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input paths\n",
    "DATASET_METADATA_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'dataset_metadata'\n",
    "SIGNAL_DICT_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'config' / 'signal_dictionary.yaml'\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "RDLS_TEMPLATE_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'template' / 'rdls_template_v03.json'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'extracted'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify paths\n",
    "assert DATASET_METADATA_DIR.exists(), f\"Not found: {DATASET_METADATA_DIR}\"\n",
    "assert SIGNAL_DICT_PATH.exists(), f\"Not found: {SIGNAL_DICT_PATH}\"\n",
    "assert RDLS_SCHEMA_PATH.exists(), f\"Not found: {RDLS_SCHEMA_PATH}\"\n",
    "\n",
    "print(f\"Base: {BASE_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Load Signal Dictionary\n",
    "\n",
    "The Signal Dictionary contains pattern-to-codelist mappings.\n",
    "\"\"\"\n",
    "\n",
    "def load_signal_dictionary(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load and parse the Signal Dictionary YAML.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Path to signal_dictionary.yaml\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Parsed signal dictionary\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "SIGNAL_DICT = load_signal_dictionary(SIGNAL_DICT_PATH)\n",
    "\n",
    "print(\"Signal Dictionary loaded successfully.\")\n",
    "print(f\"Sections: {list(SIGNAL_DICT.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Extraction Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Data Classes for Extraction Results\n",
    "\n",
    "Strongly-typed containers for extraction outputs.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ExtractionMatch:\n",
    "    \"\"\"\n",
    "    Represents a single pattern match with confidence.\n",
    "    \"\"\"\n",
    "    value: str                    # RDLS codelist value\n",
    "    confidence: float             # 0.0 to 1.0\n",
    "    source_field: str             # HDX field where match was found\n",
    "    matched_text: str             # Actual text that matched\n",
    "    pattern: str                  # Pattern that matched\n",
    "\n",
    "@dataclass\n",
    "class HazardExtraction:\n",
    "    \"\"\"\n",
    "    Complete hazard extraction for a dataset.\n",
    "    \"\"\"\n",
    "    hazard_types: List[ExtractionMatch] = field(default_factory=list)\n",
    "    process_types: List[ExtractionMatch] = field(default_factory=list)\n",
    "    analysis_type: Optional[ExtractionMatch] = None\n",
    "    return_periods: List[int] = field(default_factory=list)\n",
    "    intensity_measures: List[str] = field(default_factory=list)\n",
    "    overall_confidence: float = 0.0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            'hazard_types': [asdict(m) for m in self.hazard_types],\n",
    "            'process_types': [asdict(m) for m in self.process_types],\n",
    "            'analysis_type': asdict(self.analysis_type) if self.analysis_type else None,\n",
    "            'return_periods': self.return_periods,\n",
    "            'intensity_measures': self.intensity_measures,\n",
    "            'overall_confidence': self.overall_confidence\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class RDLSHazardBlock:\n",
    "    \"\"\"\n",
    "    RDLS-compliant hazard block structure.\n",
    "    \"\"\"\n",
    "    event_sets: List[Dict[str, Any]] = field(default_factory=list)\n",
    "    \n",
    "    def to_rdls(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to RDLS hazard block format.\"\"\"\n",
    "        return {'event_sets': self.event_sets}\n",
    "\n",
    "print(\"Data classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Hazard Extractor Class\n",
    "\n",
    "Main extraction engine using Signal Dictionary patterns.\n",
    "\"\"\"\n",
    "\n",
    "class HazardExtractor:\n",
    "    \"\"\"\n",
    "    Extracts RDLS Hazard block components from HDX metadata.\n",
    "    \n",
    "    Uses pattern matching against the Signal Dictionary to identify:\n",
    "    - Hazard types (flood, earthquake, etc.)\n",
    "    - Hazard process types (fluvial_flood, ground_motion, etc.)\n",
    "    - Analysis types (probabilistic, deterministic, empirical)\n",
    "    - Return periods (numeric values)\n",
    "    - Intensity measures (where identifiable)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal_dict : Dict[str, Any]\n",
    "        Loaded signal dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    # Confidence score mappings\n",
    "    CONFIDENCE_MAP = {'high': 0.9, 'medium': 0.7, 'low': 0.5}\n",
    "    \n",
    "    def __init__(self, signal_dict: Dict[str, Any]):\n",
    "        self.signal_dict = signal_dict\n",
    "        self._compile_patterns()\n",
    "    \n",
    "    def _compile_patterns(self) -> None:\n",
    "        \"\"\"Pre-compile regex patterns for efficiency.\"\"\"\n",
    "        self.hazard_patterns = {}\n",
    "        self.process_patterns = {}\n",
    "        self.analysis_patterns = {}\n",
    "        self.return_period_patterns = []\n",
    "        \n",
    "        # Compile hazard type patterns\n",
    "        for hazard_type, config in self.signal_dict.get('hazard_type', {}).items():\n",
    "            patterns = config.get('patterns', [])\n",
    "            confidence = self.CONFIDENCE_MAP.get(config.get('confidence', 'medium'), 0.7)\n",
    "            self.hazard_patterns[hazard_type] = {\n",
    "                'compiled': [re.compile(p, re.IGNORECASE) for p in patterns],\n",
    "                'confidence': confidence\n",
    "            }\n",
    "        \n",
    "        # Compile process type patterns\n",
    "        for process_type, config in self.signal_dict.get('process_type', {}).items():\n",
    "            patterns = config.get('patterns', [])\n",
    "            confidence = self.CONFIDENCE_MAP.get(config.get('confidence', 'medium'), 0.7)\n",
    "            self.process_patterns[process_type] = {\n",
    "                'compiled': [re.compile(p, re.IGNORECASE) for p in patterns],\n",
    "                'confidence': confidence,\n",
    "                'parent_hazard': config.get('parent_hazard')\n",
    "            }\n",
    "        \n",
    "        # Compile analysis type patterns\n",
    "        for analysis_type, config in self.signal_dict.get('analysis_type', {}).items():\n",
    "            patterns = config.get('patterns', [])\n",
    "            confidence = self.CONFIDENCE_MAP.get(config.get('confidence', 'medium'), 0.7)\n",
    "            self.analysis_patterns[analysis_type] = {\n",
    "                'compiled': [re.compile(p, re.IGNORECASE) for p in patterns],\n",
    "                'confidence': confidence\n",
    "            }\n",
    "        \n",
    "        # Compile return period patterns\n",
    "        rp_config = self.signal_dict.get('return_period', {})\n",
    "        for pattern in rp_config.get('patterns', []):\n",
    "            try:\n",
    "                self.return_period_patterns.append(re.compile(pattern, re.IGNORECASE))\n",
    "            except re.error:\n",
    "                pass  # Skip invalid patterns\n",
    "    \n",
    "    def _extract_text_fields(self, hdx_record: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Extract all text fields from HDX record for pattern matching.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hdx_record : Dict[str, Any]\n",
    "            HDX metadata record\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, str]\n",
    "            Field name -> text content mapping\n",
    "        \"\"\"\n",
    "        fields = {\n",
    "            'title': hdx_record.get('title', ''),\n",
    "            'name': hdx_record.get('name', ''),\n",
    "            'notes': hdx_record.get('notes', ''),\n",
    "            'tags': ' '.join(hdx_record.get('tags', [])),\n",
    "            'methodology': hdx_record.get('methodology_other', '') or '',\n",
    "        }\n",
    "        \n",
    "        # Add resource names and descriptions\n",
    "        resources = hdx_record.get('resources', [])\n",
    "        resource_text = ' '.join(\n",
    "            f\"{r.get('name', '')} {r.get('description', '')}\" \n",
    "            for r in resources\n",
    "        )\n",
    "        fields['resources'] = resource_text\n",
    "        \n",
    "        return fields\n",
    "    \n",
    "    def _match_patterns(\n",
    "        self, \n",
    "        text_fields: Dict[str, str], \n",
    "        patterns: Dict[str, Dict]\n",
    "    ) -> List[ExtractionMatch]:\n",
    "        \"\"\"\n",
    "        Match text against pattern dictionary.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_fields : Dict[str, str]\n",
    "            Field name -> text content\n",
    "        patterns : Dict[str, Dict]\n",
    "            Pattern configuration dictionary\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[ExtractionMatch]\n",
    "            All matches found, deduplicated by value\n",
    "        \"\"\"\n",
    "        matches = {}\n",
    "        \n",
    "        for value_name, config in patterns.items():\n",
    "            for compiled_pattern in config['compiled']:\n",
    "                for field_name, text in text_fields.items():\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    match = compiled_pattern.search(text)\n",
    "                    if match:\n",
    "                        # Only keep highest confidence match for each value\n",
    "                        if value_name not in matches or config['confidence'] > matches[value_name].confidence:\n",
    "                            matches[value_name] = ExtractionMatch(\n",
    "                                value=value_name,\n",
    "                                confidence=config['confidence'],\n",
    "                                source_field=field_name,\n",
    "                                matched_text=match.group(0),\n",
    "                                pattern=compiled_pattern.pattern\n",
    "                            )\n",
    "                        break  # Found match for this pattern, move to next\n",
    "        \n",
    "        return list(matches.values())\n",
    "    \n",
    "    def _extract_return_periods(self, text_fields: Dict[str, str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Extract numeric return period values from text.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_fields : Dict[str, str]\n",
    "            Field name -> text content\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        List[int]\n",
    "            Sorted list of unique return period values\n",
    "        \"\"\"\n",
    "        rp_values = set()\n",
    "        rp_config = self.signal_dict.get('return_period', {})\n",
    "        valid_min = rp_config.get('valid_range', {}).get('min', 1)\n",
    "        valid_max = rp_config.get('valid_range', {}).get('max', 100000)\n",
    "        \n",
    "        all_text = ' '.join(text_fields.values())\n",
    "        \n",
    "        for pattern in self.return_period_patterns:\n",
    "            for match in pattern.finditer(all_text):\n",
    "                # Try each capture group\n",
    "                for group in match.groups():\n",
    "                    if group:\n",
    "                        try:\n",
    "                            value = int(group)\n",
    "                            if valid_min <= value <= valid_max:\n",
    "                                rp_values.add(value)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "        \n",
    "        return sorted(rp_values)\n",
    "    \n",
    "    def extract(self, hdx_record: Dict[str, Any]) -> HazardExtraction:\n",
    "        \"\"\"\n",
    "        Extract hazard information from HDX record.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hdx_record : Dict[str, Any]\n",
    "            HDX metadata record\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        HazardExtraction\n",
    "            Extraction results with confidence scores\n",
    "        \"\"\"\n",
    "        text_fields = self._extract_text_fields(hdx_record)\n",
    "        \n",
    "        # Extract each component\n",
    "        hazard_types = self._match_patterns(text_fields, self.hazard_patterns)\n",
    "        process_types = self._match_patterns(text_fields, self.process_patterns)\n",
    "        analysis_matches = self._match_patterns(text_fields, self.analysis_patterns)\n",
    "        return_periods = self._extract_return_periods(text_fields)\n",
    "        \n",
    "        # Select best analysis type match\n",
    "        analysis_type = None\n",
    "        if analysis_matches:\n",
    "            analysis_type = max(analysis_matches, key=lambda x: x.confidence)\n",
    "        \n",
    "        # If return periods found but no analysis type, assume probabilistic\n",
    "        if return_periods and not analysis_type:\n",
    "            analysis_type = ExtractionMatch(\n",
    "                value='probabilistic',\n",
    "                confidence=0.8,\n",
    "                source_field='inferred',\n",
    "                matched_text='return_period_present',\n",
    "                pattern='inferred_from_rp'\n",
    "            )\n",
    "        \n",
    "        # Calculate overall confidence\n",
    "        confidences = [m.confidence for m in hazard_types]\n",
    "        if analysis_type:\n",
    "            confidences.append(analysis_type.confidence)\n",
    "        if return_periods:\n",
    "            confidences.append(0.9)  # Return periods are strong signals\n",
    "        \n",
    "        overall_confidence = np.mean(confidences) if confidences else 0.0\n",
    "        \n",
    "        return HazardExtraction(\n",
    "            hazard_types=hazard_types,\n",
    "            process_types=process_types,\n",
    "            analysis_type=analysis_type,\n",
    "            return_periods=return_periods,\n",
    "            intensity_measures=[],  # TODO: Implement intensity extraction\n",
    "            overall_confidence=overall_confidence\n",
    "        )\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = HazardExtractor(SIGNAL_DICT)\n",
    "print(f\"HazardExtractor initialized.\")\n",
    "print(f\"  - Hazard types: {len(extractor.hazard_patterns)}\")\n",
    "print(f\"  - Process types: {len(extractor.process_patterns)}\")\n",
    "print(f\"  - Analysis types: {len(extractor.analysis_patterns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RDLS Hazard Block Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Build RDLS Hazard Block from Extraction\n",
    "\n",
    "Convert extraction results into RDLS-compliant hazard block structure.\n",
    "\"\"\"\n",
    "\n",
    "def build_hazard_block(\n",
    "    extraction: HazardExtraction,\n",
    "    dataset_id: str\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS hazard block from extraction results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    extraction : HazardExtraction\n",
    "        Extraction results from HazardExtractor\n",
    "    dataset_id : str\n",
    "        Dataset identifier for building IDs\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Optional[Dict[str, Any]]\n",
    "        RDLS hazard block or None if insufficient data\n",
    "    \"\"\"\n",
    "    # Need at least one hazard type to create a block\n",
    "    if not extraction.hazard_types:\n",
    "        return None\n",
    "    \n",
    "    # Build hazards array\n",
    "    hazards = []\n",
    "    for i, ht in enumerate(extraction.hazard_types):\n",
    "        hazard_entry = {\n",
    "            'id': f\"hazard_{dataset_id[:8]}_{i+1}\",\n",
    "            'type': ht.value\n",
    "        }\n",
    "        \n",
    "        # Find matching process type\n",
    "        for pt in extraction.process_types:\n",
    "            # Check if process type is related to this hazard\n",
    "            if pt.value.startswith(ht.value) or ht.value in pt.value:\n",
    "                hazard_entry['hazard_process'] = pt.value\n",
    "                break\n",
    "        \n",
    "        hazards.append(hazard_entry)\n",
    "    \n",
    "    # Build event set\n",
    "    event_set = {\n",
    "        'id': f\"event_set_{dataset_id[:8]}\",\n",
    "        'hazards': hazards\n",
    "    }\n",
    "    \n",
    "    # Add analysis type if available\n",
    "    if extraction.analysis_type:\n",
    "        event_set['analysis_type'] = extraction.analysis_type.value\n",
    "    \n",
    "    # Build events from return periods\n",
    "    if extraction.return_periods:\n",
    "        events = []\n",
    "        for rp in extraction.return_periods:\n",
    "            event = {\n",
    "                'id': f\"event_rp{rp}_{dataset_id[:8]}\",\n",
    "                'calculation_method': 'simulated',\n",
    "                'hazard': hazards[0] if hazards else {},  # Reference first hazard\n",
    "                'occurrence': {\n",
    "                    'probabilistic': {\n",
    "                        'return_period': rp\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            events.append(event)\n",
    "        \n",
    "        event_set['events'] = events\n",
    "        event_set['event_count'] = len(events)\n",
    "        event_set['occurrence_range'] = f\"Return periods: {min(extraction.return_periods)} to {max(extraction.return_periods)} years\"\n",
    "    \n",
    "    return {\n",
    "        'event_sets': [event_set]\n",
    "    }\n",
    "\n",
    "print(\"Hazard block builder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Extraction on Sample Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Load Sample HDX Records for Testing\n",
    "\n",
    "Load records with known hazard content for validation.\n",
    "\"\"\"\n",
    "\n",
    "# Sample records with expected hazard content\n",
    "SAMPLE_FILES = [\n",
    "    '0ab99df0-17d4-4582-9e16-790308905993__tsunami-hazard-run-up-rp-500-years.json',  # Tsunami, RP500\n",
    "    '02265908-5038-4021-bb65-d2b123a1431c__gar15-global-exposure-dataset-for-papua-new-guinea.json',  # Exposure\n",
    "    '0454eb6a-d0df-4025-9e69-43fa918beb0c__bangladesh-level-1-exposure-data.json',  # Exposure\n",
    "]\n",
    "\n",
    "# Also find some flood-related files\n",
    "flood_files = list(DATASET_METADATA_DIR.glob('*flood*.json'))[:5]\n",
    "cyclone_files = list(DATASET_METADATA_DIR.glob('*cyclone*.json'))[:5]\n",
    "earthquake_files = list(DATASET_METADATA_DIR.glob('*earthquake*.json'))[:5]\n",
    "\n",
    "# Load samples\n",
    "sample_records = []\n",
    "\n",
    "for filename in SAMPLE_FILES:\n",
    "    filepath = DATASET_METADATA_DIR / filename\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            sample_records.append(json.load(f))\n",
    "\n",
    "for filepath in flood_files + cyclone_files + earthquake_files:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        sample_records.append(json.load(f))\n",
    "\n",
    "print(f\"Loaded {len(sample_records)} sample records for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 Run Extraction on Samples\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HAZARD EXTRACTION TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "extraction_results = []\n",
    "\n",
    "for record in sample_records:\n",
    "    extraction = extractor.extract(record)\n",
    "    \n",
    "    result = {\n",
    "        'id': record.get('id'),\n",
    "        'title': record.get('title', '')[:70],\n",
    "        'extraction': extraction\n",
    "    }\n",
    "    extraction_results.append(result)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(f\"Title: {record.get('title', '')[:75]}\")\n",
    "    print(f\"ID: {record.get('id')}\")\n",
    "    print(f\"\\nExtraction Results (confidence: {extraction.overall_confidence:.2f}):\")\n",
    "    \n",
    "    if extraction.hazard_types:\n",
    "        print(f\"  Hazard Types:\")\n",
    "        for ht in extraction.hazard_types:\n",
    "            print(f\"    - {ht.value} (conf: {ht.confidence:.1f}, from: {ht.source_field}, match: '{ht.matched_text}')\")\n",
    "    else:\n",
    "        print(f\"  Hazard Types: None detected\")\n",
    "    \n",
    "    if extraction.process_types:\n",
    "        print(f\"  Process Types:\")\n",
    "        for pt in extraction.process_types:\n",
    "            print(f\"    - {pt.value} (conf: {pt.confidence:.1f})\")\n",
    "    \n",
    "    if extraction.analysis_type:\n",
    "        print(f\"  Analysis Type: {extraction.analysis_type.value} (conf: {extraction.analysis_type.confidence:.1f})\")\n",
    "    \n",
    "    if extraction.return_periods:\n",
    "        print(f\"  Return Periods: {extraction.return_periods}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.3 Build RDLS Hazard Blocks for Samples\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED RDLS HAZARD BLOCKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for result in extraction_results:\n",
    "    if result['extraction'].hazard_types:\n",
    "        hazard_block = build_hazard_block(\n",
    "            result['extraction'],\n",
    "            result['id']\n",
    "        )\n",
    "        \n",
    "        if hazard_block:\n",
    "            print(f\"\\n{'─' * 80}\")\n",
    "            print(f\"Dataset: {result['title']}\")\n",
    "            print(f\"\\nHazard Block:\")\n",
    "            print(json.dumps(hazard_block, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Process All HDX Records\n",
    "\n",
    "Run extraction on full corpus and collect statistics.\n",
    "\"\"\"\n",
    "\n",
    "def process_all_records(\n",
    "    metadata_dir: Path,\n",
    "    extractor: HazardExtractor,\n",
    "    limit: Optional[int] = None\n",
    ") -> Tuple[pd.DataFrame, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Process all HDX records and extract hazard information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata_dir : Path\n",
    "        Directory containing HDX JSON files\n",
    "    extractor : HazardExtractor\n",
    "        Initialized extractor instance\n",
    "    limit : Optional[int]\n",
    "        Maximum files to process (None = all)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, List[Dict]]\n",
    "        - Summary DataFrame with extraction statistics\n",
    "        - List of full extraction results\n",
    "    \"\"\"\n",
    "    json_files = list(metadata_dir.glob('*.json'))\n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "    \n",
    "    results = []\n",
    "    iterator = tqdm(json_files, desc=\"Extracting\") if HAS_TQDM else json_files\n",
    "    \n",
    "    for filepath in iterator:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                record = json.load(f)\n",
    "            \n",
    "            extraction = extractor.extract(record)\n",
    "            \n",
    "            results.append({\n",
    "                'id': record.get('id'),\n",
    "                'title': record.get('title'),\n",
    "                'organization': record.get('organization'),\n",
    "                'hazard_types': [m.value for m in extraction.hazard_types],\n",
    "                'process_types': [m.value for m in extraction.process_types],\n",
    "                'analysis_type': extraction.analysis_type.value if extraction.analysis_type else None,\n",
    "                'return_periods': extraction.return_periods,\n",
    "                'overall_confidence': extraction.overall_confidence,\n",
    "                'has_hazard': len(extraction.hazard_types) > 0,\n",
    "                'has_return_period': len(extraction.return_periods) > 0,\n",
    "                'extraction': extraction\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'id': filepath.stem,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df, results\n",
    "\n",
    "# Process (set limit for testing)\n",
    "PROCESS_LIMIT = 1000  # Set to None for full corpus\n",
    "\n",
    "print(f\"Processing {'all' if PROCESS_LIMIT is None else PROCESS_LIMIT} records...\")\n",
    "df_results, full_results = process_all_records(DATASET_METADATA_DIR, extractor, limit=PROCESS_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Extraction Statistics Summary\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXTRACTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_results)\n",
    "with_hazard = df_results['has_hazard'].sum()\n",
    "with_rp = df_results['has_return_period'].sum()\n",
    "\n",
    "print(f\"\\nTotal records processed: {total:,}\")\n",
    "print(f\"Records with hazard extraction: {with_hazard:,} ({with_hazard/total*100:.1f}%)\")\n",
    "print(f\"Records with return periods: {with_rp:,} ({with_rp/total*100:.1f}%)\")\n",
    "\n",
    "# Hazard type distribution\n",
    "hazard_counts = {}\n",
    "for hazards in df_results['hazard_types'].dropna():\n",
    "    for h in hazards:\n",
    "        hazard_counts[h] = hazard_counts.get(h, 0) + 1\n",
    "\n",
    "print(f\"\\nHazard Type Distribution:\")\n",
    "for hazard, count in sorted(hazard_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {hazard}: {count}\")\n",
    "\n",
    "# Confidence distribution\n",
    "print(f\"\\nConfidence Score Distribution:\")\n",
    "conf_bins = df_results[df_results['has_hazard']]['overall_confidence']\n",
    "print(f\"  Mean: {conf_bins.mean():.2f}\")\n",
    "print(f\"  Median: {conf_bins.median():.2f}\")\n",
    "print(f\"  High (>=0.8): {(conf_bins >= 0.8).sum()}\")\n",
    "print(f\"  Medium (0.5-0.8): {((conf_bins >= 0.5) & (conf_bins < 0.8)).sum()}\")\n",
    "print(f\"  Low (<0.5): {(conf_bins < 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Export Extraction Summary\n",
    "\"\"\"\n",
    "\n",
    "# Prepare export DataFrame\n",
    "export_df = df_results[[\n",
    "    'id', 'title', 'organization', 'hazard_types', 'process_types',\n",
    "    'analysis_type', 'return_periods', 'overall_confidence', 'has_hazard'\n",
    "]].copy()\n",
    "\n",
    "# Convert lists to strings\n",
    "export_df['hazard_types'] = export_df['hazard_types'].apply(\n",
    "    lambda x: '|'.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "export_df['process_types'] = export_df['process_types'].apply(\n",
    "    lambda x: '|'.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "export_df['return_periods'] = export_df['return_periods'].apply(\n",
    "    lambda x: '|'.join(map(str, x)) if isinstance(x, list) else ''\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_file = OUTPUT_DIR / 'hazard_extraction_results.csv'\n",
    "export_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Save high-confidence extractions\n",
    "high_conf = export_df[export_df['has_hazard'] & (df_results['overall_confidence'] >= 0.8)]\n",
    "high_conf_file = OUTPUT_DIR / 'hazard_extraction_high_confidence.csv'\n",
    "high_conf.to_csv(high_conf_file, index=False)\n",
    "print(f\"Saved: {high_conf_file} ({len(high_conf)} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.2 Generate Sample RDLS Records with Hazard Blocks\n",
    "\n",
    "Create complete RDLS JSON records for high-confidence extractions.\n",
    "\"\"\"\n",
    "\n",
    "# Select top 10 high-confidence records\n",
    "top_records = df_results[\n",
    "    df_results['has_hazard'] & \n",
    "    (df_results['overall_confidence'] >= 0.8)\n",
    "].nlargest(10, 'overall_confidence')\n",
    "\n",
    "print(f\"\\nGenerating {len(top_records)} sample RDLS records...\")\n",
    "\n",
    "for idx, row in top_records.iterrows():\n",
    "    extraction = row['extraction']\n",
    "    hazard_block = build_hazard_block(extraction, row['id'])\n",
    "    \n",
    "    if hazard_block:\n",
    "        # Create minimal RDLS record\n",
    "        rdls_record = {\n",
    "            'datasets': [{\n",
    "                'id': f\"rdls_hzd-hdx_{row['id'][:8]}\",\n",
    "                'title': row['title'],\n",
    "                'risk_data_type': ['hazard'],\n",
    "                'hazard': hazard_block,\n",
    "                'links': [{\n",
    "                    'href': 'https://docs.riskdatalibrary.org/en/0__3__0/rdls_schema.json',\n",
    "                    'rel': 'describedby'\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        # Save\n",
    "        output_path = OUTPUT_DIR / f\"rdls_hzd-hdx_{row['id'][:8]}.json\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rdls_record, f, indent=2)\n",
    "        \n",
    "        print(f\"  Created: {output_path.name}\")\n",
    "\n",
    "print(f\"\\nSample RDLS records saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "This notebook demonstrates the Hazard extraction pipeline. Next steps:\n",
    "\n",
    "1. **Notebook 10**: Exposure Block Extractor (similar pattern-based approach)\n",
    "2. **Notebook 11**: Vulnerability/Loss Block Extractor (more complex, lower coverage expected)\n",
    "3. **Notebook 12**: Integration - merge extractions with existing general metadata\n",
    "4. **Notebook 13**: Validation against RDLS schema and QA reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
