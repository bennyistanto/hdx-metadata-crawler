{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e837bcd-642b-45c9-8a05-ae9b29ef5c0f",
   "metadata": {},
   "source": [
    "# Step 1 - HDX Metadata Crawler\n",
    "\n",
    "## Purpose\n",
    "This notebook crawls the **Humanitarian Data Exchange (HDX)** catalogue and downloads HDX metadata as **JSON**, supporting two export levels:\n",
    "- **Resource-level metadata** (one JSON per resource file)\n",
    "- **Dataset-level metadata** (one JSON per dataset, including its resources list)\n",
    "\n",
    "## What this notebook does\n",
    "1) Enumerates datasets using the **CKAN Action API** (`package_search`).\n",
    "2) For each dataset, downloads:\n",
    "   - **Resource-level exports** via:\n",
    "     `/dataset/{dataset_id}/resource/{resource_id}/download_metadata?format=json`\n",
    "   - **Dataset-level export** via:\n",
    "     `/dataset/{dataset_id}/download_metadata?format=json`\n",
    "3) Writes JSON outputs using **stable, unique filenames** (UUID-based) and optional human-readable slugs.\n",
    "4) Produces audit logs:\n",
    "   - `manifest*.jsonl` (successful downloads)\n",
    "   - `errors*.jsonl` (failures, HTTP errors, bot checks, parse errors)\n",
    "\n",
    "## Design notes\n",
    "- **Resume-safe**: existing files are skipped (safe to re-run).\n",
    "- **Polite crawling**: configurable throttling, retries, and exponential backoff.\n",
    "- **Robustness**: if `download_metadata` returns non-JSON or is blocked, the workflow can fall back to CKAN:\n",
    "  - `resource_show` for resource-level metadata\n",
    "  - `package_show` for dataset-level metadata\n",
    "\n",
    "## Outputs (typical)\n",
    "- `dataset_metadata/` : dataset-level metadata JSON exports (one per dataset)\n",
    "- `resource_metadata/`: resource-level metadata JSON exports (one per resource)\n",
    "- `manifest_*.jsonl`  : one line per successful download (dataset/resource)\n",
    "- `errors_*.jsonl`    : one line per failure with identifiers and error messages\n",
    "\n",
    "## Configuration\n",
    "Adjust before running:\n",
    "- Pagination: `ROWS`\n",
    "- Network behavior: `REQUESTS_PER_SECOND`, `MAX_RETRIES`, `TIMEOUT`\n",
    "- Test limits: `MAX_DATASETS`, `MAX_RESOURCES`\n",
    "- Filenames: `ADD_SLUG_TO_FILENAME`, `SLUG_MAXLEN`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2944e6-a97b-4d05-b865-d35a9b1139f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resource-level metadata (per file)\n",
    "\n",
    "Downloads one JSON per **resource** (i.e., per downloadable file attached to a dataset).  \n",
    "Endpoint pattern:\n",
    "`/dataset/{dataset_id}/resource/{resource_id}/download_metadata?format=json`  \n",
    "Output naming uses `{dataset_uuid}__{resource_uuid}[__slug].json` to guarantee uniqueness and stable reruns. Use this mode when you need metadata tied to individual files (CSV, GeoPackage, Shapefile ZIPs, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489d626-cd95-4131-8ab9-f449446e642d",
   "metadata": {},
   "source": [
    "### Cell 1 — Config + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61012db8-6d95-4518-a2e2-2f0240cdd224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "# ----------------------------\n",
    "# User config\n",
    "# ----------------------------\n",
    "BASE = \"https://data.humdata.org\"\n",
    "CKAN_API = f\"{BASE}/api/3/action\"\n",
    "\n",
    "OUT_DIR = Path(\"hdx_metadata_dump\")\n",
    "DATASET_DIR = OUT_DIR / \"datasets\"\n",
    "RESOURCE_DIR = OUT_DIR / \"resources\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESOURCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest.jsonl\"\n",
    "ERRORS_PATH = OUT_DIR / \"errors.jsonl\"\n",
    "\n",
    "# Pagination: CKAN package_search supports rows/start. Rows is often capped (commonly 1000).  :contentReference[oaicite:3]{index=3}\n",
    "ROWS = 500\n",
    "\n",
    "# Throttling (be nice)\n",
    "REQUESTS_PER_SECOND = 2.0  # lower = gentler\n",
    "MAX_RETRIES = 6\n",
    "TIMEOUT = 60\n",
    "\n",
    "# For testing, you can stop early:\n",
    "MAX_DATASETS: Optional[int] = None     # e.g., 50\n",
    "MAX_RESOURCES: Optional[int] = None    # e.g., 2000\n",
    "\n",
    "# Optional: include a readable name in filename (still unique due to IDs)\n",
    "ADD_SLUG_TO_FILENAME = True\n",
    "SLUG_MAXLEN = 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125bfb74-1015-4bf7-ba89-a19f3495afc5",
   "metadata": {},
   "source": [
    "### Cell 2 — HTTP helpers + CKAN wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37444e19-96db-4e47-b9fc-e541881ed26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"hdx-metadata-crawler/1.0 (contact: you@example.com)\",\n",
    "    \"Accept\": \"application/json,text/plain,*/*\",\n",
    "})\n",
    "\n",
    "def _looks_like_bot_check(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    return (\"verify that you're not a robot\" in t) or (\"javascript is disabled\" in t)\n",
    "\n",
    "def _sleep_rate_limited(last_ts: float) -> float:\n",
    "    \"\"\"Sleep to respect REQUESTS_PER_SECOND. Returns new timestamp.\"\"\"\n",
    "    if REQUESTS_PER_SECOND <= 0:\n",
    "        return time.time()\n",
    "    min_dt = 1.0 / REQUESTS_PER_SECOND\n",
    "    now = time.time()\n",
    "    dt = now - last_ts\n",
    "    if dt < min_dt:\n",
    "        time.sleep(min_dt - dt)\n",
    "    return time.time()\n",
    "\n",
    "def get_json(url: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    GET JSON with retries, backoff, and basic bot-check detection.\n",
    "    Returns parsed JSON dict.\n",
    "    Raises RuntimeError on repeated failure.\n",
    "    \"\"\"\n",
    "    last_ts = getattr(get_json, \"_last_ts\", 0.0)\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        last_ts = _sleep_rate_limited(last_ts)\n",
    "        get_json._last_ts = last_ts\n",
    "\n",
    "        try:\n",
    "            r = session.get(url, params=params, timeout=TIMEOUT)\n",
    "        except requests.RequestException as e:\n",
    "            wait = min(60, (2 ** attempt) + random.random())\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "\n",
    "        # Retry on transient errors / rate limiting\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            retry_after = r.headers.get(\"Retry-After\")\n",
    "            if retry_after and retry_after.isdigit():\n",
    "                time.sleep(int(retry_after))\n",
    "            else:\n",
    "                time.sleep(min(60, (2 ** attempt) + random.random()))\n",
    "            continue\n",
    "\n",
    "        # Hard failure\n",
    "        if r.status_code >= 400:\n",
    "            raise RuntimeError(f\"HTTP {r.status_code} for {r.url}\")\n",
    "\n",
    "        # Bot-check HTML sometimes comes back 200\n",
    "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"json\" not in ctype:\n",
    "            text = r.text[:5000]\n",
    "            if _looks_like_bot_check(text):\n",
    "                raise RuntimeError(f\"Bot-check page returned for {r.url}\")\n",
    "            # Sometimes servers omit JSON content-type; try parse anyway.\n",
    "        try:\n",
    "            return r.json()\n",
    "        except Exception:\n",
    "            text = r.text[:5000]\n",
    "            raise RuntimeError(f\"Non-JSON response for {r.url}: {text[:200]}\")\n",
    "\n",
    "    raise RuntimeError(f\"Failed after {MAX_RETRIES} retries: {url}\")\n",
    "\n",
    "def ckan_action(action: str, **params: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Call CKAN Action API:\n",
    "      GET {CKAN_API}/{action}?...\n",
    "    CKAN responses have {success: bool, result: ..., error: ...}. :contentReference[oaicite:4]{index=4}\n",
    "    \"\"\"\n",
    "    url = f\"{CKAN_API}/{action}\"\n",
    "    resp = get_json(url, params=params)\n",
    "    if not resp.get(\"success\", False):\n",
    "        raise RuntimeError(f\"CKAN action failed: {action} params={params} error={resp.get('error')}\")\n",
    "    return resp[\"result\"]\n",
    "\n",
    "_slug_re = re.compile(r\"[^a-zA-Z0-9]+\")\n",
    "def slugify(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = _slug_re.sub(\"-\", s).strip(\"-\").lower()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aefe1d-8b80-4795-b944-05db70e4331b",
   "metadata": {},
   "source": [
    "### Cell 3 — Dataset iterator + download logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2b917-55a4-4f15-b09d-98bf7363603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_datasets(q: str = \"*:*\", rows: int = ROWS) -> Iterable[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Stream all datasets using package_search pagination. :contentReference[oaicite:5]{index=5}\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    yielded = 0\n",
    "    while True:\n",
    "        result = ckan_action(\n",
    "            \"package_search\",\n",
    "            q=q,\n",
    "            rows=rows,\n",
    "            start=start,\n",
    "            sort=\"metadata_modified desc\",\n",
    "            facet=\"false\",\n",
    "        )\n",
    "        count = result.get(\"count\", 0)\n",
    "        datasets = result.get(\"results\", [])\n",
    "\n",
    "        if not datasets:\n",
    "            break\n",
    "\n",
    "        for ds in datasets:\n",
    "            yield ds\n",
    "            yielded += 1\n",
    "            if MAX_DATASETS is not None and yielded >= MAX_DATASETS:\n",
    "                return\n",
    "\n",
    "        start += rows\n",
    "        if start >= count:\n",
    "            break\n",
    "\n",
    "def dataset_filename(ds: Dict[str, Any]) -> Path:\n",
    "    ds_id = ds.get(\"id\", \"unknown-dataset-id\")\n",
    "    ds_name = slugify(ds.get(\"name\", \"\")) or \"dataset\"\n",
    "    return DATASET_DIR / f\"{ds_id}__{ds_name}.json\"\n",
    "\n",
    "def resource_filename(dataset_id: str, resource_id: str, resource_name: str = \"\") -> Path:\n",
    "    base = f\"{dataset_id}__{resource_id}\"\n",
    "    if ADD_SLUG_TO_FILENAME:\n",
    "        s = slugify(resource_name)[:SLUG_MAXLEN]\n",
    "        if s:\n",
    "            base += f\"__{s}\"\n",
    "    return RESOURCE_DIR / f\"{base}.json\"\n",
    "\n",
    "def write_json(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def download_resource_metadata(dataset_id: str, resource: Dict[str, Any]) -> Tuple[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Prefer HDX's download_metadata export. If blocked, fall back to CKAN resource_show.\n",
    "    Returns (metadata_json, source_label).\n",
    "    \"\"\"\n",
    "    resource_id = resource[\"id\"]\n",
    "    export_url = f\"{BASE}/dataset/{dataset_id}/resource/{resource_id}/download_metadata\"\n",
    "    try:\n",
    "        meta = get_json(export_url, params={\"format\": \"json\"})\n",
    "        return meta, \"download_metadata\"\n",
    "    except Exception as e:\n",
    "        # Fallback: CKAN resource_show is still structured metadata for the resource\n",
    "        # (not necessarily identical to HDX export format).\n",
    "        meta = ckan_action(\"resource_show\", id=resource_id)\n",
    "        return {\n",
    "            \"_fallback_reason\": str(e),\n",
    "            \"_note\": \"Fallback used: CKAN resource_show (may differ from HDX download_metadata export).\",\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"resource\": meta\n",
    "        }, \"ckan_resource_show_fallback\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6b315-8102-45ab-81c2-5d3e8b366493",
   "metadata": {},
   "source": [
    "### Cell 4 — Main runner (resume-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692279dc-95a9-4f41-b706-4a58ccd9630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_crawl(q: str = \"*:*\") -> None:\n",
    "    total_resources = 0\n",
    "\n",
    "    for ds in iter_datasets(q=q):\n",
    "        ds_id = ds.get(\"id\")\n",
    "        if not ds_id:\n",
    "            continue\n",
    "\n",
    "        # Save dataset snapshot (from package_search results)\n",
    "        ds_path = dataset_filename(ds)\n",
    "        if not ds_path.exists():\n",
    "            write_json(ds_path, ds)\n",
    "\n",
    "        resources = ds.get(\"resources\") or []\n",
    "        for res in resources:\n",
    "            if \"id\" not in res:\n",
    "                continue\n",
    "\n",
    "            res_id = res[\"id\"]\n",
    "            name_for_slug = res.get(\"name\") or res.get(\"description\") or \"\"\n",
    "            out_path = resource_filename(ds_id, res_id, name_for_slug)\n",
    "\n",
    "            if out_path.exists():\n",
    "                continue  # resume\n",
    "\n",
    "            try:\n",
    "                meta, source = download_resource_metadata(ds_id, res)\n",
    "                write_json(out_path, meta)\n",
    "\n",
    "                append_jsonl(MANIFEST_PATH, {\n",
    "                    \"dataset_id\": ds_id,\n",
    "                    \"dataset_name\": ds.get(\"name\"),\n",
    "                    \"dataset_title\": ds.get(\"title\"),\n",
    "                    \"resource_id\": res_id,\n",
    "                    \"resource_name\": res.get(\"name\"),\n",
    "                    \"resource_format\": res.get(\"format\"),\n",
    "                    \"metadata_source\": source,\n",
    "                    \"metadata_file\": str(out_path),\n",
    "                    \"metadata_url\": f\"{BASE}/dataset/{ds_id}/resource/{res_id}/download_metadata?format=json\",\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                append_jsonl(ERRORS_PATH, {\n",
    "                    \"dataset_id\": ds_id,\n",
    "                    \"dataset_name\": ds.get(\"name\"),\n",
    "                    \"resource_id\": res_id,\n",
    "                    \"error\": str(e),\n",
    "                })\n",
    "\n",
    "            total_resources += 1\n",
    "            if MAX_RESOURCES is not None and total_resources >= MAX_RESOURCES:\n",
    "                return\n",
    "\n",
    "print(\"Ready. Next: run_full_crawl()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a71c2a1-8a36-4e63-9cb2-3c3fe6b623e8",
   "metadata": {},
   "source": [
    "### Cell 5 — Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03340ea0-ec22-411d-8375-e06f07a92b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full crawl of all public datasets:\n",
    "# run_full_crawl(q=\"*:*\")\n",
    "\n",
    "# Or target a subset first (recommended):\n",
    "run_full_crawl(q=\"cod-ab-global\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb6dd65-39b6-434b-a92d-79febe02df31",
   "metadata": {},
   "source": [
    "## Dataset-level metadata (overall dataset)\n",
    "\n",
    "Downloads one JSON per **dataset** representing the dataset’s overall metadata (title, tags, license, maintainer info, and a list of associated resources).  \n",
    "Endpoint pattern:\n",
    "`/dataset/{dataset_id}/download_metadata?format=json`  \n",
    "Output naming uses `{dataset_uuid}[__slug].json`. Use this mode when you want a single “master” metadata record per dataset, without splitting into per-resource JSON files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537091c2-680c-4977-9d89-bf96856ff50a",
   "metadata": {},
   "source": [
    "### Cell 1 — Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb128243-03b5-4f3e-89d9-c8ec4c653b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "BASE = \"https://data.humdata.org\"\n",
    "CKAN_API = f\"{BASE}/api/3/action\"\n",
    "\n",
    "OUT_DIR = Path(\"hdx_dataset_metadata_dump\")\n",
    "DATASET_META_DIR = OUT_DIR / \"dataset_metadata\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MANIFEST_PATH = OUT_DIR / \"manifest_datasets.jsonl\"\n",
    "ERRORS_PATH = OUT_DIR / \"errors_datasets.jsonl\"\n",
    "\n",
    "# CKAN pagination\n",
    "ROWS = 500  # often safe; CKAN instances may cap max rows\n",
    "\n",
    "# Politeness + robustness\n",
    "REQUESTS_PER_SECOND = 2.0\n",
    "MAX_RETRIES = 6\n",
    "TIMEOUT = 60\n",
    "\n",
    "# For testing\n",
    "MAX_DATASETS: Optional[int] = None  # e.g. 50\n",
    "\n",
    "# Filename readability\n",
    "ADD_SLUG_TO_FILENAME = True\n",
    "SLUG_MAXLEN = 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7aabd0-07fd-4a09-9461-2af91a191a3b",
   "metadata": {},
   "source": [
    "### Cell 2 — HTTP + CKAN helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5849043a-e4f0-4a80-828e-a86dc205ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"hdx-dataset-metadata-crawler/1.0\",\n",
    "    \"Accept\": \"application/json,text/plain,*/*\",\n",
    "})\n",
    "\n",
    "def _looks_like_bot_check(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    return (\"verify that you're not a robot\" in t) or (\"javascript is disabled\" in t)\n",
    "\n",
    "def _sleep_rate_limited(last_ts: float) -> float:\n",
    "    if REQUESTS_PER_SECOND <= 0:\n",
    "        return time.time()\n",
    "    min_dt = 1.0 / REQUESTS_PER_SECOND\n",
    "    now = time.time()\n",
    "    dt = now - last_ts\n",
    "    if dt < min_dt:\n",
    "        time.sleep(min_dt - dt)\n",
    "    return time.time()\n",
    "\n",
    "def get_json(url: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    last_ts = getattr(get_json, \"_last_ts\", 0.0)\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        last_ts = _sleep_rate_limited(last_ts)\n",
    "        get_json._last_ts = last_ts\n",
    "\n",
    "        try:\n",
    "            r = session.get(url, params=params, timeout=TIMEOUT)\n",
    "        except requests.RequestException:\n",
    "            time.sleep(min(60, (2 ** attempt) + random.random()))\n",
    "            continue\n",
    "\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            retry_after = r.headers.get(\"Retry-After\")\n",
    "            if retry_after and retry_after.isdigit():\n",
    "                time.sleep(int(retry_after))\n",
    "            else:\n",
    "                time.sleep(min(60, (2 ** attempt) + random.random()))\n",
    "            continue\n",
    "\n",
    "        if r.status_code >= 400:\n",
    "            raise RuntimeError(f\"HTTP {r.status_code} for {r.url}\")\n",
    "\n",
    "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if \"json\" not in ctype:\n",
    "            text = r.text[:5000]\n",
    "            if _looks_like_bot_check(text):\n",
    "                raise RuntimeError(f\"Bot-check page returned for {r.url}\")\n",
    "            # Sometimes JSON is returned with odd content-type; try parse anyway.\n",
    "\n",
    "        try:\n",
    "            return r.json()\n",
    "        except Exception:\n",
    "            text = r.text[:2000]\n",
    "            raise RuntimeError(f\"Non-JSON response for {r.url}: {text[:200]}\")\n",
    "\n",
    "    raise RuntimeError(f\"Failed after {MAX_RETRIES} retries: {url}\")\n",
    "\n",
    "def ckan_action(action: str, **params: Any) -> Dict[str, Any]:\n",
    "    url = f\"{CKAN_API}/{action}\"\n",
    "    resp = get_json(url, params=params)\n",
    "    if not resp.get(\"success\", False):\n",
    "        raise RuntimeError(f\"CKAN action failed: {action} params={params} error={resp.get('error')}\")\n",
    "    return resp[\"result\"]\n",
    "\n",
    "_slug_re = re.compile(r\"[^a-zA-Z0-9]+\")\n",
    "def slugify(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = _slug_re.sub(\"-\", s).strip(\"-\").lower()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e144c08-428c-46c9-bf54-731b06701f1c",
   "metadata": {},
   "source": [
    "### Cell 3 — Enumerate datasets + download dataset-level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970c5d80-4ed3-4dec-aaa3-e2815128af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_datasets(q: str = \"*:*\", rows: int = ROWS) -> Iterable[Dict[str, Any]]:\n",
    "    start = 0\n",
    "    yielded = 0\n",
    "\n",
    "    while True:\n",
    "        result = ckan_action(\n",
    "            \"package_search\",\n",
    "            q=q,\n",
    "            rows=rows,\n",
    "            start=start,\n",
    "            sort=\"metadata_modified desc\",\n",
    "            facet=\"false\",\n",
    "        )\n",
    "        count = result.get(\"count\", 0)\n",
    "        datasets = result.get(\"results\", [])\n",
    "        if not datasets:\n",
    "            break\n",
    "\n",
    "        for ds in datasets:\n",
    "            yield ds\n",
    "            yielded += 1\n",
    "            if MAX_DATASETS is not None and yielded >= MAX_DATASETS:\n",
    "                return\n",
    "\n",
    "        start += rows\n",
    "        if start >= count:\n",
    "            break\n",
    "\n",
    "def dataset_meta_filename(dataset_id: str, dataset_name: str = \"\") -> Path:\n",
    "    base = dataset_id\n",
    "    if ADD_SLUG_TO_FILENAME:\n",
    "        s = slugify(dataset_name)[:SLUG_MAXLEN]\n",
    "        if s:\n",
    "            base += f\"__{s}\"\n",
    "    return DATASET_META_DIR / f\"{base}.json\"\n",
    "\n",
    "def write_json(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def download_dataset_metadata(dataset_id: str) -> Tuple[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Prefer HDX dataset export: /dataset/{dataset_id}/download_metadata?format=json\n",
    "    Fallback: CKAN package_show (reliable automation API)\n",
    "    \"\"\"\n",
    "    export_url = f\"{BASE}/dataset/{dataset_id}/download_metadata\"\n",
    "    try:\n",
    "        meta = get_json(export_url, params={\"format\": \"json\"})\n",
    "        return meta, \"download_metadata\"\n",
    "    except Exception as e:\n",
    "        # Fallback to CKAN dataset metadata\n",
    "        pkg = ckan_action(\"package_show\", id=dataset_id)\n",
    "        return {\n",
    "            \"_fallback_reason\": str(e),\n",
    "            \"_note\": \"Fallback used: CKAN package_show (may differ from HDX download_metadata export).\",\n",
    "            \"dataset\": pkg,\n",
    "        }, \"ckan_package_show_fallback\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16911821-2971-4bf1-b09c-3cfff3f8ad3f",
   "metadata": {},
   "source": [
    "### Cell 4 — Main runner (dataset-level only, resume-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a994cc-bc0e-4cb5-a35e-d9b9b152381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready: run_dataset_metadata_crawl()\n"
     ]
    }
   ],
   "source": [
    "def run_dataset_metadata_crawl(q: str = \"*:*\") -> None:\n",
    "    for ds in iter_datasets(q=q):\n",
    "        ds_id = ds.get(\"id\")\n",
    "        ds_name = ds.get(\"name\") or \"\"\n",
    "        ds_title = ds.get(\"title\") or \"\"\n",
    "\n",
    "        if not ds_id:\n",
    "            continue\n",
    "\n",
    "        out_path = dataset_meta_filename(ds_id, ds_name)\n",
    "        if out_path.exists():\n",
    "            continue  # resume-safe\n",
    "\n",
    "        try:\n",
    "            meta, source = download_dataset_metadata(ds_id)\n",
    "            write_json(out_path, meta)\n",
    "\n",
    "            append_jsonl(MANIFEST_PATH, {\n",
    "                \"dataset_id\": ds_id,\n",
    "                \"dataset_name\": ds_name,\n",
    "                \"dataset_title\": ds_title,\n",
    "                \"metadata_source\": source,\n",
    "                \"metadata_file\": str(out_path),\n",
    "                \"metadata_url\": f\"{BASE}/dataset/{ds_id}/download_metadata?format=json\",\n",
    "            })\n",
    "        except Exception as e:\n",
    "            append_jsonl(ERRORS_PATH, {\n",
    "                \"dataset_id\": ds_id,\n",
    "                \"dataset_name\": ds_name,\n",
    "                \"dataset_title\": ds_title,\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "\n",
    "print(\"Ready: run_dataset_metadata_crawl()\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a3f0e-9207-4bee-8a46-6933ce582e61",
   "metadata": {},
   "source": [
    "### Cell 5 — Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd0483b-87f5-47c3-bbb5-bcade22a131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full crawl:\n",
    "run_dataset_metadata_crawl(q=\"*:*\")\n",
    "\n",
    "# Or test with your example dataset name:\n",
    "# run_dataset_metadata_crawl(q=\"cod-ab-global\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43753ef-ba02-404b-96e3-1e2889d973c9",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
