{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 01: HDX Metadata Crawler\n",
    "\n",
    "**Purpose**: Crawl the Humanitarian Data Exchange (HDX) catalogue and download dataset-level metadata as JSON.\n",
    "\n",
    "**Process**:\n",
    "1. Enumerate datasets using CKAN Action API (`package_search`)\n",
    "2. Download dataset-level metadata via HDX export endpoint\n",
    "3. Fall back to CKAN `package_show` if export fails\n",
    "4. Produce audit logs (manifest + errors)\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\n",
    "Core libraries for HTTP requests, JSON handling, and file operations.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Progress will be shown via print statements.\")\n",
    "    print(\"Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Configuration\n",
    "\n",
    "All configurable parameters are centralized here for easy adjustment.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class CrawlerConfig:\n",
    "    \"\"\"\n",
    "    Configuration for HDX metadata crawler.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    base_url : str\n",
    "        HDX base URL\n",
    "    rows_per_page : int\n",
    "        Number of datasets per API page (max typically 1000)\n",
    "    requests_per_second : float\n",
    "        Rate limiting (be polite to the server)\n",
    "    max_retries : int\n",
    "        Maximum retry attempts for failed requests\n",
    "    timeout : int\n",
    "        HTTP request timeout in seconds\n",
    "    max_datasets : Optional[int]\n",
    "        Limit for testing (None = crawl all)\n",
    "    add_slug_to_filename : bool\n",
    "        Include human-readable slug in filenames\n",
    "    slug_max_length : int\n",
    "        Maximum length of slug in filename\n",
    "    \"\"\"\n",
    "    base_url: str = \"https://data.humdata.org\"\n",
    "    rows_per_page: int = 500\n",
    "    requests_per_second: float = 2.0\n",
    "    max_retries: int = 6\n",
    "    timeout: int = 60\n",
    "    max_datasets: Optional[int] = None  # Set to e.g. 100 for testing\n",
    "    add_slug_to_filename: bool = True\n",
    "    slug_max_length: int = 80\n",
    "    \n",
    "    @property\n",
    "    def ckan_api_url(self) -> str:\n",
    "        \"\"\"CKAN Action API endpoint.\"\"\"\n",
    "        return f\"{self.base_url}/api/3/action\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = CrawlerConfig(\n",
    "    max_datasets=None,  # Set to small number (e.g., 50) for testing\n",
    ")\n",
    "\n",
    "print(f\"Base URL: {config.base_url}\")\n",
    "print(f\"Rate limit: {config.requests_per_second} req/sec\")\n",
    "print(f\"Max datasets: {config.max_datasets or 'All'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Configure Paths\n",
    "\n",
    "Output directories for metadata JSON files and audit logs.\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Output directories\n",
    "OUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DATASET_META_DIR = OUT_DIR / 'dataset_metadata'\n",
    "\n",
    "# Audit logs\n",
    "MANIFEST_PATH = OUT_DIR / 'manifest_datasets.jsonl'\n",
    "ERRORS_PATH = OUT_DIR / 'errors_datasets.jsonl'\n",
    "\n",
    "# Create directories\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASET_META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Output directory: {OUT_DIR}\")\n",
    "print(f\"Dataset metadata: {DATASET_META_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. HTTP Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 HTTP Session and Helpers\n",
    "\n",
    "Robust HTTP client with rate limiting, retries, and bot-check detection.\n",
    "\"\"\"\n",
    "\n",
    "class HDXClient:\n",
    "    \"\"\"\n",
    "    HTTP client for HDX API with built-in politeness and error handling.\n",
    "    \n",
    "    Features:\n",
    "    - Rate limiting to respect server resources\n",
    "    - Exponential backoff on failures\n",
    "    - Bot-check page detection\n",
    "    - Session persistence for connection reuse\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: CrawlerConfig):\n",
    "        \"\"\"\n",
    "        Initialize HTTP client.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        config : CrawlerConfig\n",
    "            Crawler configuration object\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self._last_request_time = 0.0\n",
    "        \n",
    "        # Initialize session with appropriate headers\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": \"hdx-metadata-crawler/1.0 (RDLS pipeline)\",\n",
    "            \"Accept\": \"application/json,text/plain,*/*\",\n",
    "        })\n",
    "    \n",
    "    def _looks_like_bot_check(self, text: str) -> bool:\n",
    "        \"\"\"Detect bot-check/captcha pages.\"\"\"\n",
    "        t = text.lower()\n",
    "        return (\"verify that you're not a robot\" in t) or (\"javascript is disabled\" in t)\n",
    "    \n",
    "    def _rate_limit(self) -> None:\n",
    "        \"\"\"Enforce rate limiting between requests.\"\"\"\n",
    "        if self.config.requests_per_second <= 0:\n",
    "            return\n",
    "        \n",
    "        min_interval = 1.0 / self.config.requests_per_second\n",
    "        elapsed = time.time() - self._last_request_time\n",
    "        \n",
    "        if elapsed < min_interval:\n",
    "            time.sleep(min_interval - elapsed)\n",
    "        \n",
    "        self._last_request_time = time.time()\n",
    "    \n",
    "    def get_json(self, url: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        GET JSON with retries and error handling.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        url : str\n",
    "            URL to fetch\n",
    "        params : Optional[Dict[str, Any]]\n",
    "            Query parameters\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Parsed JSON response\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If request fails after all retries\n",
    "        \"\"\"\n",
    "        for attempt in range(self.config.max_retries):\n",
    "            self._rate_limit()\n",
    "            \n",
    "            try:\n",
    "                response = self.session.get(url, params=params, timeout=self.config.timeout)\n",
    "            except requests.RequestException as e:\n",
    "                wait = min(60, (2 ** attempt) + random.random())\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            \n",
    "            # Retry on transient errors\n",
    "            if response.status_code in (429, 500, 502, 503, 504):\n",
    "                retry_after = response.headers.get(\"Retry-After\")\n",
    "                if retry_after and retry_after.isdigit():\n",
    "                    time.sleep(int(retry_after))\n",
    "                else:\n",
    "                    time.sleep(min(60, (2 ** attempt) + random.random()))\n",
    "                continue\n",
    "            \n",
    "            # Hard failure\n",
    "            if response.status_code >= 400:\n",
    "                raise RuntimeError(f\"HTTP {response.status_code} for {response.url}\")\n",
    "            \n",
    "            # Check for bot-check pages\n",
    "            content_type = (response.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            if \"json\" not in content_type:\n",
    "                if self._looks_like_bot_check(response.text[:5000]):\n",
    "                    raise RuntimeError(f\"Bot-check page returned for {response.url}\")\n",
    "            \n",
    "            try:\n",
    "                return response.json()\n",
    "            except Exception:\n",
    "                raise RuntimeError(f\"Non-JSON response for {response.url}: {response.text[:200]}\")\n",
    "        \n",
    "        raise RuntimeError(f\"Failed after {self.config.max_retries} retries: {url}\")\n",
    "    \n",
    "    def ckan_action(self, action: str, **params: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Call CKAN Action API.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : str\n",
    "            CKAN action name (e.g., 'package_search')\n",
    "        **params : Any\n",
    "            Action parameters\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Action result\n",
    "        \"\"\"\n",
    "        url = f\"{self.config.ckan_api_url}/{action}\"\n",
    "        response = self.get_json(url, params=params)\n",
    "        \n",
    "        if not response.get(\"success\", False):\n",
    "            raise RuntimeError(\n",
    "                f\"CKAN action failed: {action} params={params} error={response.get('error')}\"\n",
    "            )\n",
    "        \n",
    "        return response[\"result\"]\n",
    "\n",
    "# Initialize client\n",
    "client = HDXClient(config)\n",
    "print(\"HTTP client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Crawler Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Utility Functions\n",
    "\n",
    "Helper functions for filename generation and file I/O.\n",
    "\"\"\"\n",
    "\n",
    "_SLUG_RE = re.compile(r\"[^a-zA-Z0-9]+\")\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert string to URL-safe slug.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        Input string\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Slugified string\n",
    "    \"\"\"\n",
    "    s = (s or \"\").strip()\n",
    "    s = _SLUG_RE.sub(\"-\", s).strip(\"-\").lower()\n",
    "    return s\n",
    "\n",
    "\n",
    "def dataset_filename(dataset_id: str, dataset_name: str = \"\") -> Path:\n",
    "    \"\"\"\n",
    "    Generate filename for dataset metadata JSON.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        HDX dataset UUID\n",
    "    dataset_name : str\n",
    "        Optional dataset name for readable slug\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Output file path\n",
    "    \"\"\"\n",
    "    base = dataset_id\n",
    "    if config.add_slug_to_filename:\n",
    "        slug = slugify(dataset_name)[:config.slug_max_length]\n",
    "        if slug:\n",
    "            base = f\"{dataset_id}__{slug}\"\n",
    "    return DATASET_META_DIR / f\"{base}.json\"\n",
    "\n",
    "\n",
    "def write_json(path: Path, obj: Any) -> None:\n",
    "    \"\"\"\n",
    "    Write object as JSON file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Output file path\n",
    "    obj : Any\n",
    "        Object to serialize\n",
    "    \"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Append object as JSON line.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Output JSONL file path\n",
    "    obj : Dict[str, Any]\n",
    "        Object to append\n",
    "    \"\"\"\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.2 Dataset Iterator\n",
    "\n",
    "Paginated iteration over all HDX datasets using CKAN API.\n",
    "\"\"\"\n",
    "\n",
    "def iter_datasets(query: str = \"*:*\") -> Iterable[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Iterate over all datasets matching query.\n",
    "    \n",
    "    Uses CKAN package_search with pagination.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        CKAN search query (default: all datasets)\n",
    "        \n",
    "    Yields\n",
    "    ------\n",
    "    Dict[str, Any]\n",
    "        Dataset metadata dict\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    yielded = 0\n",
    "    \n",
    "    while True:\n",
    "        result = client.ckan_action(\n",
    "            \"package_search\",\n",
    "            q=query,\n",
    "            rows=config.rows_per_page,\n",
    "            start=start,\n",
    "            sort=\"metadata_modified desc\",\n",
    "            facet=\"false\",\n",
    "        )\n",
    "        \n",
    "        count = result.get(\"count\", 0)\n",
    "        datasets = result.get(\"results\", [])\n",
    "        \n",
    "        if not datasets:\n",
    "            break\n",
    "        \n",
    "        for ds in datasets:\n",
    "            yield ds\n",
    "            yielded += 1\n",
    "            \n",
    "            if config.max_datasets is not None and yielded >= config.max_datasets:\n",
    "                return\n",
    "        \n",
    "        start += config.rows_per_page\n",
    "        if start >= count:\n",
    "            break\n",
    "\n",
    "\n",
    "print(\"Dataset iterator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.3 Metadata Downloader\n",
    "\n",
    "Download dataset metadata with fallback strategy.\n",
    "\"\"\"\n",
    "\n",
    "def download_dataset_metadata(dataset_id: str) -> Tuple[Dict[str, Any], str]:\n",
    "    \"\"\"\n",
    "    Download dataset-level metadata.\n",
    "    \n",
    "    Tries HDX export endpoint first, falls back to CKAN package_show.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        HDX dataset UUID\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[str, Any], str]\n",
    "        (metadata_dict, source_label)\n",
    "    \"\"\"\n",
    "    export_url = f\"{config.base_url}/dataset/{dataset_id}/download_metadata\"\n",
    "    \n",
    "    try:\n",
    "        meta = client.get_json(export_url, params={\"format\": \"json\"})\n",
    "        return meta, \"download_metadata\"\n",
    "    except Exception as e:\n",
    "        # Fallback to CKAN API\n",
    "        pkg = client.ckan_action(\"package_show\", id=dataset_id)\n",
    "        return {\n",
    "            \"_fallback_reason\": str(e),\n",
    "            \"_note\": \"Fallback used: CKAN package_show (may differ from HDX export).\",\n",
    "            \"dataset\": pkg,\n",
    "        }, \"ckan_package_show_fallback\"\n",
    "\n",
    "\n",
    "print(\"Metadata downloader defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Run Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Main Crawl Function\n",
    "\n",
    "Resume-safe crawler with progress tracking.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class CrawlStats:\n",
    "    \"\"\"Statistics for crawl run.\"\"\"\n",
    "    total_datasets: int = 0\n",
    "    downloaded: int = 0\n",
    "    skipped_existing: int = 0\n",
    "    errors: int = 0\n",
    "    fallbacks: int = 0\n",
    "\n",
    "\n",
    "def run_dataset_crawl(query: str = \"*:*\") -> CrawlStats:\n",
    "    \"\"\"\n",
    "    Crawl HDX datasets and download metadata.\n",
    "    \n",
    "    Resume-safe: skips existing files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        CKAN search query\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    CrawlStats\n",
    "        Crawl statistics\n",
    "    \"\"\"\n",
    "    stats = CrawlStats()\n",
    "    \n",
    "    # Get total count for progress bar\n",
    "    initial_result = client.ckan_action(\"package_search\", q=query, rows=0)\n",
    "    total_count = initial_result.get(\"count\", 0)\n",
    "    \n",
    "    if config.max_datasets:\n",
    "        total_count = min(total_count, config.max_datasets)\n",
    "    \n",
    "    print(f\"Total datasets to process: {total_count:,}\")\n",
    "    \n",
    "    # Create iterator with optional progress bar\n",
    "    dataset_iter = iter_datasets(query)\n",
    "    if HAS_TQDM:\n",
    "        dataset_iter = tqdm(dataset_iter, total=total_count, desc=\"Crawling datasets\")\n",
    "    \n",
    "    for ds in dataset_iter:\n",
    "        stats.total_datasets += 1\n",
    "        \n",
    "        ds_id = ds.get(\"id\")\n",
    "        ds_name = ds.get(\"name\") or \"\"\n",
    "        ds_title = ds.get(\"title\") or \"\"\n",
    "        \n",
    "        if not ds_id:\n",
    "            continue\n",
    "        \n",
    "        out_path = dataset_filename(ds_id, ds_name)\n",
    "        \n",
    "        # Resume-safe: skip existing\n",
    "        if out_path.exists():\n",
    "            stats.skipped_existing += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            meta, source = download_dataset_metadata(ds_id)\n",
    "            write_json(out_path, meta)\n",
    "            \n",
    "            if \"fallback\" in source:\n",
    "                stats.fallbacks += 1\n",
    "            \n",
    "            # Log success\n",
    "            append_jsonl(MANIFEST_PATH, {\n",
    "                \"dataset_id\": ds_id,\n",
    "                \"dataset_name\": ds_name,\n",
    "                \"dataset_title\": ds_title,\n",
    "                \"metadata_source\": source,\n",
    "                \"metadata_file\": str(out_path),\n",
    "                \"metadata_url\": f\"{config.base_url}/dataset/{ds_id}/download_metadata?format=json\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            })\n",
    "            \n",
    "            stats.downloaded += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            stats.errors += 1\n",
    "            append_jsonl(ERRORS_PATH, {\n",
    "                \"dataset_id\": ds_id,\n",
    "                \"dataset_name\": ds_name,\n",
    "                \"dataset_title\": ds_title,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            })\n",
    "        \n",
    "        # Progress update for non-tqdm\n",
    "        if not HAS_TQDM and stats.total_datasets % 500 == 0:\n",
    "            print(f\"  Processed {stats.total_datasets:,} datasets...\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "print(\"Crawler function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 Execute Crawl\n",
    "\n",
    "Run the full dataset metadata crawl.\n",
    "\"\"\"\n",
    "\n",
    "# Full crawl of all public datasets\n",
    "stats = run_dataset_crawl(query=\"*:*\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CRAWL COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total processed: {stats.total_datasets:,}\")\n",
    "print(f\"Downloaded: {stats.downloaded:,}\")\n",
    "print(f\"Skipped (existing): {stats.skipped_existing:,}\")\n",
    "print(f\"Fallbacks used: {stats.fallbacks:,}\")\n",
    "print(f\"Errors: {stats.errors:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Verify Output\n",
    "\n",
    "Quick sanity check of downloaded files.\n",
    "\"\"\"\n",
    "\n",
    "json_files = list(DATASET_META_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OUTPUT SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset metadata files: {len(json_files):,}\")\n",
    "print(f\"Manifest: {MANIFEST_PATH}\")\n",
    "print(f\"Errors: {ERRORS_PATH}\")\n",
    "\n",
    "if json_files:\n",
    "    print(f\"\\nSample files (first 5):\")\n",
    "    for f in sorted(json_files)[:5]:\n",
    "        print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
