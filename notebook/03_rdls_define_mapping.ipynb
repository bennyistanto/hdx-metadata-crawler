{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 03: Define RDLS Mapping Model\n",
    "\n",
    "**Purpose**: Create mapping configuration for classifying HDX datasets into RDLS components.\n",
    "\n",
    "**Process**:\n",
    "1. Scan corpus to collect tag/org/format statistics\n",
    "2. Generate weighted mapping configs (tags → RDLS components)\n",
    "3. Create keyword pattern rules for title/notes\n",
    "4. Produce review sample for calibration\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started: 2026-02-10T21:11:41.780502\n",
      "Progress bars: Available\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dir: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/dataset_metadata\n",
      "Config dir: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config\n",
      "Reference dir: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/reference\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input directories\n",
    "DUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DATASET_DIR = DUMP_DIR / 'dataset_metadata'\n",
    "POLICY_DIR = DUMP_DIR / 'policy'\n",
    "OSM_EXCLUDED_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n",
    "\n",
    "# Output directories\n",
    "CONFIG_DIR = DUMP_DIR / 'config'\n",
    "REFERENCE_DIR = DUMP_DIR / 'reference'  # Pipeline reference materials (mapping docs, samples)\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_SIZE = 400\n",
    "MAX_NOTES_CHARS = 350\n",
    "OVERWRITE_CONFIG = True\n",
    "\n",
    "# Create directories\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REFERENCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"Config dir: {CONFIG_DIR}\")\n",
    "print(f\"Reference dir: {REFERENCE_DIR}\")\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "# Default \"skip\" because config/reference files are semi-static.\n",
    "CLEANUP_MODE = \"replace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-1-3-clean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 03 Config Files]:\n",
      "  tag_to_rdls_component.yaml              : 1 files\n",
      "  keyword_to_rdls_component.yaml          : 1 files\n",
      "  org_hints.yaml                          : 1 files\n",
      "  Cleaned 3 files. Ready for fresh output.\n",
      "\n",
      "Output cleanup [NB 03 Reference Files]:\n",
      "  mapping_rules.md                        : 1 files\n",
      "  samples_for_mapping.csv                 : 1 files\n",
      "  Cleaned 2 files. Ready for fresh output.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deleted': 2, 'skipped': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Clean Previous Outputs\n",
    "\n",
    "Remove stale output files from previous runs (controlled by CLEANUP_MODE).\n",
    "Default is \"skip\" because config files are semi-static and curated.\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice, defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "# ── Run cleanup ────────────────────────────────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    CONFIG_DIR,\n",
    "    patterns=[\n",
    "        \"tag_to_rdls_component.yaml\",\n",
    "        \"keyword_to_rdls_component.yaml\",\n",
    "        \"org_hints.yaml\",\n",
    "    ],\n",
    "    label=\"NB 03 Config Files\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n",
    "\n",
    "clean_previous_outputs(\n",
    "    REFERENCE_DIR,\n",
    "    patterns=[\n",
    "        \"mapping_rules.md\",\n",
    "        \"samples_for_mapping.csv\",\n",
    "    ],\n",
    "    label=\"NB 03 Reference Files\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 JSON and Data Extraction Helpers\n",
    "\"\"\"\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in folder, sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "def read_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Read JSON with UTF-8 encoding.\"\"\"\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "\n",
    "def normalize_dataset_record(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Handle possible wrapper {'dataset': {...}}.\"\"\"\n",
    "    if isinstance(raw, dict) and \"id\" in raw:\n",
    "        return raw\n",
    "    if isinstance(raw, dict) and \"dataset\" in raw and isinstance(raw[\"dataset\"], dict):\n",
    "        return raw[\"dataset\"]\n",
    "    return raw\n",
    "\n",
    "\n",
    "def get_org_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract organization title.\"\"\"\n",
    "    org = ds.get(\"organization\")\n",
    "    if isinstance(org, dict):\n",
    "        return (org.get(\"title\") or org.get(\"name\") or \"\").strip()\n",
    "    return (org or \"\").strip()\n",
    "\n",
    "\n",
    "def get_tags(ds: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Extract tags as lowercase strings.\"\"\"\n",
    "    tags = ds.get(\"tags\") or []\n",
    "    out: List[str] = []\n",
    "    if isinstance(tags, list):\n",
    "        for t in tags:\n",
    "            if isinstance(t, dict):\n",
    "                name = t.get(\"name\") or \"\"\n",
    "                if name:\n",
    "                    out.append(name.strip().lower())\n",
    "            elif isinstance(t, str):\n",
    "                out.append(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_resource_formats(ds: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Extract resource formats.\"\"\"\n",
    "    formats: List[str] = []\n",
    "    for r in (ds.get(\"resources\") or []):\n",
    "        if isinstance(r, dict):\n",
    "            fmt = (r.get(\"format\") or \"\").strip().lower()\n",
    "            if fmt:\n",
    "                formats.append(fmt)\n",
    "    return formats\n",
    "\n",
    "\n",
    "def short_text(s: str, max_len: int) -> str:\n",
    "    \"\"\"Truncate text with ellipsis.\"\"\"\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s[:max_len] + (\"…\" if len(s) > max_len else \"\")\n",
    "\n",
    "\n",
    "def load_excluded_ids(path: Path) -> set:\n",
    "    \"\"\"Load OSM exclusion list.\"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: OSM exclusion list not found: {path}\")\n",
    "        return set()\n",
    "    ids = set()\n",
    "    for line in path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            ids.add(line)\n",
    "    return ids\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Scan Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,649 excluded OSM dataset IDs\n",
      "Scanning 26,246 JSON files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7983c55e97514178873bc124374a864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning corpus:   0%|          | 0/26246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CORPUS SUMMARY (non-OSM)\n",
      "============================================================\n",
      "Total files: 26,246\n",
      "Excluded (OSM): 3,649\n",
      "Kept (non-OSM): 22,597\n",
      "Unique tags: 142\n",
      "Unique orgs: 357\n",
      "Unique formats: 47\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.1 Load Exclusion List and Scan Files\n",
    "\n",
    "Collect statistics on tags, organizations, and formats.\n",
    "\"\"\"\n",
    "\n",
    "excluded_ids = load_excluded_ids(OSM_EXCLUDED_IDS_TXT)\n",
    "print(f\"Loaded {len(excluded_ids):,} excluded OSM dataset IDs\")\n",
    "\n",
    "# Counters\n",
    "tag_counter = Counter()\n",
    "org_counter = Counter()\n",
    "fmt_counter = Counter()\n",
    "\n",
    "# Records for sampling\n",
    "records_for_sampling: List[Dict[str, Any]] = []\n",
    "\n",
    "# Scan files\n",
    "files = list(iter_json_files(DATASET_DIR))\n",
    "total = len(files)\n",
    "kept = 0\n",
    "skipped_osm = 0\n",
    "\n",
    "print(f\"Scanning {total:,} JSON files...\")\n",
    "\n",
    "iterator = tqdm(files, desc=\"Scanning corpus\") if HAS_TQDM else files\n",
    "\n",
    "for i, path in enumerate(iterator, start=1):\n",
    "    if not HAS_TQDM and i % 5000 == 0:\n",
    "        print(f\"  Processed {i:,}/{total:,}\")\n",
    "    \n",
    "    raw = read_json(path)\n",
    "    ds = normalize_dataset_record(raw)\n",
    "    \n",
    "    ds_id = (ds.get(\"id\") or \"\").strip()\n",
    "    if not ds_id:\n",
    "        continue\n",
    "    \n",
    "    if ds_id in excluded_ids:\n",
    "        skipped_osm += 1\n",
    "        continue\n",
    "    \n",
    "    kept += 1\n",
    "    \n",
    "    tags = get_tags(ds)\n",
    "    org = get_org_title(ds)\n",
    "    fmts = get_resource_formats(ds)\n",
    "    \n",
    "    tag_counter.update(tags)\n",
    "    if org:\n",
    "        org_counter.update([org])\n",
    "    fmt_counter.update(fmts)\n",
    "    \n",
    "    records_for_sampling.append({\n",
    "        \"dataset_id\": ds_id,\n",
    "        \"title\": ds.get(\"title\") or \"\",\n",
    "        \"name\": ds.get(\"name\") or \"\",\n",
    "        \"organization\": org,\n",
    "        \"dataset_source\": ds.get(\"dataset_source\") or \"\",\n",
    "        \"license_title\": ds.get(\"license_title\") or ds.get(\"license_id\") or \"\",\n",
    "        \"tags\": tags,\n",
    "        \"formats\": fmts,\n",
    "        \"notes\": ds.get(\"notes\") or \"\",\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CORPUS SUMMARY (non-OSM)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total files: {total:,}\")\n",
    "print(f\"Excluded (OSM): {skipped_osm:,}\")\n",
    "print(f\"Kept (non-OSM): {kept:,}\")\n",
    "print(f\"Unique tags: {len(tag_counter):,}\")\n",
    "print(f\"Unique orgs: {len(org_counter):,}\")\n",
    "print(f\"Unique formats: {len(fmt_counter):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-3-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP 40 TAGS\n",
      "============================================================\n",
      "  hxl                                            9,764\n",
      "  indicators                                     7,387\n",
      "  geodata                                        5,761\n",
      "  health                                         2,925\n",
      "  baseline population                            2,301\n",
      "  food security                                  2,147\n",
      "  economics                                      2,113\n",
      "  education                                      2,092\n",
      "  development                                    1,496\n",
      "  environment                                    1,483\n",
      "  demographics                                   1,393\n",
      "  facilities-infrastructure                      1,349\n",
      "  conflict-violence                              1,265\n",
      "  climate-weather                                1,169\n",
      "  population                                     1,141\n",
      "  internally displaced persons-idp               1,134\n",
      "  socioeconomics                                 1,099\n",
      "  nutrition                                      1,066\n",
      "  funding                                          932\n",
      "  transportation                                   915\n",
      "  displacement                                     862\n",
      "  flooding                                         842\n",
      "  cyclones-hurricanes-typhoons                     803\n",
      "  populated places-settlements                     803\n",
      "  integrated food security phase classification-ipc    797\n",
      "  agriculture-livestock                            747\n",
      "  sustainable development goals-sdg                704\n",
      "  gender                                           702\n",
      "  poverty                                          686\n",
      "  administrative boundaries-divisions              653\n",
      "  trade                                            652\n",
      "  who is doing what and where-3w-4w-5w             637\n",
      "  maternity                                        628\n",
      "  refugees                                         627\n",
      "  water sanitation and hygiene-wash                625\n",
      "  disease                                          619\n",
      "  livelihoods                                      617\n",
      "  natural disasters                                589\n",
      "  children                                         581\n",
      "  hazards and risk                                 567\n",
      "\n",
      "============================================================\n",
      "TOP 20 ORGANIZATIONS\n",
      "============================================================\n",
      "  World Bank Group                                         4,792\n",
      "  WorldPop                                                 1,569\n",
      "  United Nations Satellite Centre (UNOSAT)                 1,452\n",
      "  UNHCR - The UN Refugee Agency                            1,132\n",
      "  FEWS NET                                                   833\n",
      "  World Health Organization                                  676\n",
      "  HDX                                                        602\n",
      "  WFP - World Food Programme                                 490\n",
      "  Copernicus                                                 478\n",
      "  Food and Agriculture Organization (FAO) of the United Nations    441\n",
      "  Internal Displacement Monitoring Centre (IDMC)             426\n",
      "  UNICEF Data and Analytics (HQ)                             292\n",
      "  United Nations Human Settlements Programmes, Data and Analytics Section    284\n",
      "  International Organization for Migration (IOM)             273\n",
      "  Kontur                                                     254\n",
      "  UNESCO                                                     251\n",
      "  Who's On First                                             248\n",
      "  Armed Conflict Location & Event Data Project (ACLED)       246\n",
      "  International Aid Transparency Initiative                  242\n",
      "  UNDP Human Development Reports Office (HDRO)               228\n",
      "\n",
      "============================================================\n",
      "TOP 20 FORMATS\n",
      "============================================================\n",
      "  geotiff           78,304\n",
      "  csv               49,553\n",
      "  xlsx               8,184\n",
      "  shp                4,532\n",
      "  geojson            4,317\n",
      "  kml                3,304\n",
      "  pdf                1,448\n",
      "  zip                1,369\n",
      "  geodatabase        1,332\n",
      "  geopackage         1,292\n",
      "  web app            1,000\n",
      "  json                 592\n",
      "  xls                  405\n",
      "  png                  369\n",
      "  xml                  297\n",
      "  kmz                  117\n",
      "  emf                   80\n",
      "  txt                   67\n",
      "  docx                  55\n",
      "  geoservice            42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.2 Display Top Statistics\n",
    "\"\"\"\n",
    "\n",
    "TOP_N = 40\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOP {TOP_N} TAGS\")\n",
    "print(f\"{'='*60}\")\n",
    "for t, c in tag_counter.most_common(TOP_N):\n",
    "    print(f\"  {t:<45} {c:>6,}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOP 20 ORGANIZATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "for o, c in org_counter.most_common(20):\n",
    "    print(f\"  {o:<55} {c:>6,}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOP 20 FORMATS\")\n",
    "print(f\"{'='*60}\")\n",
    "for f, c in fmt_counter.most_common(20):\n",
    "    print(f\"  {f:<15} {c:>8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Create Review Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/reference/samples_for_mapping.csv\n",
      "Sample rows: 440\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.1 Generate Review Sample CSV\n",
    "\n",
    "Random sample plus examples for top tags.\n",
    "\"\"\"\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Random sample\n",
    "sample = random.sample(records_for_sampling, k=min(SAMPLE_SIZE, len(records_for_sampling)))\n",
    "\n",
    "# Ensure examples for top tags\n",
    "top_tags = [t for t, _ in tag_counter.most_common(40)]\n",
    "present_ids = {r[\"dataset_id\"] for r in sample}\n",
    "\n",
    "for tag in top_tags:\n",
    "    for r in records_for_sampling:\n",
    "        if r[\"dataset_id\"] in present_ids:\n",
    "            continue\n",
    "        if tag in r[\"tags\"]:\n",
    "            sample.append(r)\n",
    "            present_ids.add(r[\"dataset_id\"])\n",
    "            break\n",
    "\n",
    "# Write sample CSV\n",
    "out_sample_csv = REFERENCE_DIR / \"samples_for_mapping.csv\"\n",
    "\n",
    "with out_sample_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"dataset_id\", \"title\", \"organization\", \"dataset_source\", \n",
    "            \"license_title\", \"tags\", \"formats\", \"notes_snippet\"\n",
    "        ],\n",
    "    )\n",
    "    w.writeheader()\n",
    "    for r in sample:\n",
    "        w.writerow({\n",
    "            \"dataset_id\": r[\"dataset_id\"],\n",
    "            \"title\": r[\"title\"],\n",
    "            \"organization\": r[\"organization\"],\n",
    "            \"dataset_source\": r[\"dataset_source\"],\n",
    "            \"license_title\": r[\"license_title\"],\n",
    "            \"tags\": \";\".join(r[\"tags\"]),\n",
    "            \"formats\": \";\".join(sorted(set(r[\"formats\"]))),\n",
    "            \"notes_snippet\": short_text(r[\"notes\"], MAX_NOTES_CHARS),\n",
    "        })\n",
    "\n",
    "print(f\"Wrote: {out_sample_csv}\")\n",
    "print(f\"Sample rows: {len(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Generate Mapping Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML writer defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.1 Minimal YAML Writer\n",
    "\n",
    "Safe YAML serializer without external dependencies.\n",
    "\"\"\"\n",
    "\n",
    "def yaml_escape(s: str) -> str:\n",
    "    \"\"\"Escape string for YAML if needed.\"\"\"\n",
    "    if s == \"\" or any(ch in s for ch in [\":\", \"#\", \"{\", \"}\", \"[\", \"]\", \",\", \"\\n\", \"\\r\", \"\\t\"]) or s.strip() != s:\n",
    "        return '\"' + s.replace('\"', '\\\\\"') + '\"'\n",
    "    if s.lower() in {\"true\", \"false\", \"null\", \"~\"}:\n",
    "        return '\"' + s + '\"'\n",
    "    return s\n",
    "\n",
    "\n",
    "def dump_yaml(obj: Any, indent: int = 0) -> str:\n",
    "    \"\"\"Convert object to YAML string.\"\"\"\n",
    "    sp = \"  \" * indent\n",
    "    if isinstance(obj, dict):\n",
    "        lines = []\n",
    "        for k in sorted(obj.keys()):\n",
    "            v = obj[k]\n",
    "            key = yaml_escape(str(k))\n",
    "            if isinstance(v, (dict, list)):\n",
    "                lines.append(f\"{sp}{key}:\")\n",
    "                lines.append(dump_yaml(v, indent + 1))\n",
    "            else:\n",
    "                lines.append(f\"{sp}{key}: {yaml_escape(str(v))}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    if isinstance(obj, list):\n",
    "        lines = []\n",
    "        for item in obj:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                lines.append(f\"{sp}-\")\n",
    "                lines.append(dump_yaml(item, indent + 1))\n",
    "            else:\n",
    "                lines.append(f\"{sp}- {yaml_escape(str(item))}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    return f\"{sp}{yaml_escape(str(obj))}\"\n",
    "\n",
    "\n",
    "def write_yaml(path: Path, obj: Any, overwrite: bool = False) -> None:\n",
    "    \"\"\"Write object to YAML file.\"\"\"\n",
    "    if path.exists() and not overwrite:\n",
    "        print(f\"SKIP (exists): {path}\")\n",
    "        return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(dump_yaml(obj) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"Wrote: {path}\")\n",
    "\n",
    "\n",
    "print(\"YAML writer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-5-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping rules defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.2 Define Mapping Rules\n",
    "\n",
    "Tag weights, keyword patterns, and organization hints.\n",
    "\"\"\"\n",
    "\n",
    "# Tag to RDLS component mapping (weights: 1=weak, 5=strong)\n",
    "tag_to_rdls = {\n",
    "    \"hazard\": {\n",
    "        \"flooding\": 5,\n",
    "        \"drought\": 5,\n",
    "        \"cyclones-hurricanes-typhoons\": 5,\n",
    "        \"earthquake-tsunami\": 5,\n",
    "        \"climate hazards\": 4,\n",
    "        \"hydrology\": 3,\n",
    "        \"natural disasters\": 3,\n",
    "        \"hazards and risk\": 3,\n",
    "        \"forecasting\": 2,\n",
    "        \"topography\": 2,\n",
    "    },\n",
    "    \"loss_impact\": {\n",
    "        \"damage assessment\": 5,\n",
    "        \"casualties\": 5,\n",
    "        \"fatalities\": 5,\n",
    "        \"mortality\": 4,\n",
    "        \"affected population\": 4,\n",
    "        \"affected area\": 4,\n",
    "        \"people in need-pin\": 3,\n",
    "        \"severity\": 3,\n",
    "    },\n",
    "    \"exposure\": {\n",
    "        \"facilities-infrastructure\": 5,\n",
    "        \"populated places-settlements\": 4,\n",
    "        \"population\": 4,\n",
    "        \"roads\": 4,\n",
    "        \"railways\": 3,\n",
    "        \"ports\": 3,\n",
    "        \"aviation\": 3,\n",
    "        \"points of interest-poi\": 3,\n",
    "        \"health facilities\": 4,\n",
    "        \"education facilities-schools\": 4,\n",
    "        \"energy\": 3,\n",
    "        \"gazetteer\": 2,\n",
    "        \"geodata\": 2,\n",
    "    },\n",
    "    \"vulnerability_proxy\": {\n",
    "        \"demographics\": 4,\n",
    "        \"poverty\": 4,\n",
    "        \"socioeconomics\": 4,\n",
    "        \"disability\": 3,\n",
    "        \"gender\": 3,\n",
    "        \"food security\": 3,\n",
    "        \"health\": 2,\n",
    "        \"education\": 2,\n",
    "        \"livelihoods\": 2,\n",
    "        \"nutrition\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Keyword patterns for title/notes (regex)\n",
    "keyword_to_rdls = {\n",
    "    \"hazard\": [\n",
    "        r\"\\bflood(s|ing)?\\b\",\n",
    "        r\"\\bdrought\\b\",\n",
    "        r\"\\bcyclone(s)?\\b\",\n",
    "        r\"\\bhurricane(s)?\\b\",\n",
    "        r\"\\btyphoon(s)?\\b\",\n",
    "        r\"\\bearthquake(s)?\\b\",\n",
    "        r\"\\btsunami\\b\",\n",
    "        r\"\\breturn period\\b\",\n",
    "        r\"\\bhazard\\b\",\n",
    "    ],\n",
    "    \"loss_impact\": [\n",
    "        r\"\\bdamage\\b\",\n",
    "        r\"\\bloss(es)?\\b\",\n",
    "        r\"\\bcost(s)?\\b\",\n",
    "        r\"\\bfatalit(y|ies)\\b\",\n",
    "        r\"\\bcasualt(y|ies)\\b\",\n",
    "        r\"\\baffected\\b\",\n",
    "    ],\n",
    "    \"exposure\": [\n",
    "        r\"\\bairport(s)?\\b\",\n",
    "        r\"\\broad(s)?\\b\",\n",
    "        r\"\\bbridge(s)?\\b\",\n",
    "        r\"\\bport(s)?\\b\",\n",
    "        r\"\\bhospital(s)?\\b\",\n",
    "        r\"\\bschool(s)?\\b\",\n",
    "        r\"\\bfacilit(y|ies)\\b\",\n",
    "        r\"\\binfrastructure\\b\",\n",
    "        r\"\\bbuildings?\\b\",\n",
    "        r\"\\bsettlement(s)?\\b\",\n",
    "    ],\n",
    "    \"vulnerability_proxy\": [\n",
    "        r\"\\bpoverty\\b\",\n",
    "        r\"\\bdisabilit(y|ies)\\b\",\n",
    "        r\"\\bmalnutrition\\b\",\n",
    "        r\"\\bfood security\\b\",\n",
    "        r\"\\bhealth indicator(s)?\\b\",\n",
    "        r\"\\bdemographic(s)?\\b\",\n",
    "        r\"\\bvulnerability\\b\",\n",
    "        r\"\\bhousehold(s)?\\b\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Organization hints\n",
    "org_hints = {\n",
    "    \"World Bank Group\": {\"vulnerability_proxy\": 2},\n",
    "    \"The DHS Program\": {\"vulnerability_proxy\": 4},\n",
    "    \"Food and Agriculture Organization\": {\"vulnerability_proxy\": 3},\n",
    "    \"UNICEF\": {\"vulnerability_proxy\": 3},\n",
    "}\n",
    "\n",
    "print(\"Mapping rules defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-5-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config/tag_to_rdls_component.yaml\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config/keyword_to_rdls_component.yaml\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config/org_hints.yaml\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.3 Write Mapping Config Files\n",
    "\"\"\"\n",
    "\n",
    "# Output paths\n",
    "path_tag_yaml = CONFIG_DIR / \"tag_to_rdls_component.yaml\"\n",
    "path_kw_yaml = CONFIG_DIR / \"keyword_to_rdls_component.yaml\"\n",
    "path_org_yaml = CONFIG_DIR / \"org_hints.yaml\"\n",
    "\n",
    "# Write configs\n",
    "write_yaml(path_tag_yaml, tag_to_rdls, overwrite=OVERWRITE_CONFIG)\n",
    "write_yaml(path_kw_yaml, keyword_to_rdls, overwrite=OVERWRITE_CONFIG)\n",
    "write_yaml(path_org_yaml, org_hints, overwrite=OVERWRITE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-5-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/reference/mapping_rules.md\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.4 Write Mapping Rules Documentation\n",
    "\"\"\"\n",
    "\n",
    "rules_md = REFERENCE_DIR / \"mapping_rules.md\"\n",
    "\n",
    "if (not rules_md.exists()) or OVERWRITE_CONFIG:\n",
    "    content = f\"\"\"# Mapping Rules (Draft)\n",
    "\n",
    "This document describes the mapping used to translate HDX dataset metadata into RDLS components.\n",
    "\n",
    "## Components\n",
    "\n",
    "- **hazard**: Datasets describing hazard events/intensity/footprints or hazard model inputs\n",
    "- **exposure**: Datasets describing exposed elements (population, facilities, infrastructure)\n",
    "- **vulnerability_proxy**: Indicator datasets used as proxies for vulnerability/sensitivity\n",
    "- **loss_impact**: Datasets describing observed/estimated impacts (damage, fatalities)\n",
    "\n",
    "## Evidence Sources (Priority Order)\n",
    "\n",
    "1. **HDX tags** (weighted)\n",
    "2. **Keywords** in title/notes (regex patterns)\n",
    "3. **Organization hints** (helps with indicator series)\n",
    "\n",
    "## OSM Policy\n",
    "\n",
    "OSM-derived datasets are excluded using the exclusion list in:\n",
    "`{OSM_EXCLUDED_IDS_TXT.as_posix()}`\n",
    "\n",
    "## Configuration Files\n",
    "\n",
    "- `config/tag_to_rdls_component.yaml` - Tag weights\n",
    "- `config/keyword_to_rdls_component.yaml` - Keyword patterns\n",
    "- `config/org_hints.yaml` - Organization hints\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review `reference/samples_for_mapping.csv` to calibrate weights\n",
    "2. Expand tag mappings for common tags in corpus\n",
    "3. Add organization mappings for frequent publishers\n",
    "\n",
    "---\n",
    "*Generated by HDX-RDLS Pipeline Notebook 03*\n",
    "\"\"\"\n",
    "    rules_md.write_text(content, encoding=\"utf-8\")\n",
    "    print(f\"Wrote: {rules_md}\")\n",
    "else:\n",
    "    print(f\"SKIP (exists): {rules_md}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-6-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MAPPING CONFIG COMPLETE\n",
      "============================================================\n",
      "\n",
      "Corpus statistics:\n",
      "  Non-OSM datasets: 22,597\n",
      "  Unique tags: 142\n",
      "  Unique organizations: 357\n",
      "\n",
      "Outputs:\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config/tag_to_rdls_component.yaml\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config/keyword_to_rdls_component.yaml\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config/org_hints.yaml\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/reference/mapping_rules.md\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/reference/samples_for_mapping.csv\n",
      "\n",
      "Next: Run Notebook 04 to classify datasets using these configs.\n",
      "\n",
      "Notebook completed: 2026-02-10T21:15:43.194697\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.1 Display Summary\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MAPPING CONFIG COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nCorpus statistics:\")\n",
    "print(f\"  Non-OSM datasets: {kept:,}\")\n",
    "print(f\"  Unique tags: {len(tag_counter):,}\")\n",
    "print(f\"  Unique organizations: {len(org_counter):,}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - {path_tag_yaml}\")\n",
    "print(f\"  - {path_kw_yaml}\")\n",
    "print(f\"  - {path_org_yaml}\")\n",
    "print(f\"  - {rules_md}\")\n",
    "print(f\"  - {out_sample_csv}\")\n",
    "\n",
    "print(f\"\\nNext: Run Notebook 04 to classify datasets using these configs.\")\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3f86a-4343-4c53-a235-5519a14f3118",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
