{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2eea5b7",
   "metadata": {},
   "source": [
    "# Step 3 - Define RDLS Mapping Model (Inclusive)\n",
    "\n",
    "This notebook creates the **mapping configuration** that Step 4 (classifier) will use to label HDX datasets as:\n",
    "- `hazard`\n",
    "- `exposure`\n",
    "- `vulnerability_proxy`\n",
    "- `loss_impact`\n",
    "\n",
    "It is **inclusive** by design (so indicator datasets can be treated as vulnerability proxies), and it respects your team’s policy by **excluding OSM-derived datasets** using the Step 2 exclusion list.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- Dataset metadata folder from Step 1  \n",
    "  `hdx_dataset_metadata_dump/dataset_metadata/*.json`\n",
    "- OSM exclusion list from Step 2  \n",
    "  `hdx_dataset_metadata_dump/policy/osm_excluded_dataset_ids.txt`\n",
    "\n",
    "## Outputs\n",
    "This notebook writes configuration + documentation files used downstream:\n",
    "- `config/tag_to_rdls_component.yaml` — weighted mapping from HDX tags → RDLS components\n",
    "- `config/keyword_to_rdls_component.yaml` — keyword patterns over title/notes → RDLS components\n",
    "- `config/org_hints.yaml` — organization/source hints → RDLS components (especially for indicator packs)\n",
    "- `docs/mapping_rules.md` — human-readable explanation of the mapping logic\n",
    "- `docs/samples_for_mapping.csv` — a review sample of datasets to calibrate mapping\n",
    "\n",
    "## How you iterate\n",
    "1. Run this notebook once to generate draft mapping files and tag statistics.\n",
    "2. Open the YAML files and refine weights / add missing tags and keywords.\n",
    "3. Re-run this notebook (it can append/update stats) or move directly to Step 4.\n",
    "\n",
    "> Tip: Keep mapping rules conservative, then expand. The QA loop (Step 5) is where you lock edge cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21848784",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "03_define_rdls_mapping.ipynb\n",
    "\n",
    "Build a transparent, reviewable mapping model from HDX metadata to RDLS components.\n",
    "\n",
    "This notebook is designed to create *configuration artifacts* (YAML + docs) for use in later steps.\n",
    "It does not attempt to fully classify all datasets—that happens in Step 4.\n",
    "\n",
    "Design principles:\n",
    "- Inclusive RDLS labeling (supports vulnerability proxy indicators)\n",
    "- Policy-aware: excludes OSM-derived datasets via Step 2 list\n",
    "- Deterministic outputs: stable ordering and fixed random seed for samples\n",
    "- Audit-friendly: produces statistics and a review sample for calibration\n",
    "- Minimal dependencies: standard library only (writes YAML via a tiny safe serializer)\n",
    "\n",
    "Author: <YOUR NAME/ORG>\n",
    "License: <YOUR LICENSE>\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305a34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\dataset_metadata\n",
      "OSM_EXCLUDED_IDS_TXT: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\policy\\osm_excluded_dataset_ids.txt\n",
      "CONFIG_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\n",
      "DOCS_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\docs\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration (EDIT HERE)\n",
    "# =========================\n",
    "\n",
    "# Root directory from Step 1 output (relative to notebooks/)\n",
    "DUMP_DIR = Path(\"../hdx_dataset_metadata_dump\")\n",
    "DATASET_DIR = DUMP_DIR / \"dataset_metadata\"\n",
    "\n",
    "# Step 2 output (OSM exclusion list) — inside the dump folder\n",
    "POLICY_DIR = DUMP_DIR / \"policy\"\n",
    "OSM_EXCLUDED_IDS_TXT = POLICY_DIR / \"osm_excluded_dataset_ids.txt\"\n",
    "\n",
    "# Where to write mapping config files\n",
    "CONFIG_DIR = DUMP_DIR / \"config\"\n",
    "DOCS_DIR = DUMP_DIR / \"docs\"\n",
    "\n",
    "\n",
    "# Sampling\n",
    "RANDOM_SEED = 42\n",
    "SAMPLE_SIZE = 400   # review sample size (200–500 is typical)\n",
    "MAX_NOTES_CHARS = 350  # keep review CSV readable\n",
    "\n",
    "# Overwrite existing YAML/docs?\n",
    "OVERWRITE_CONFIG = False\n",
    "\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DATASET_DIR:\", DATASET_DIR.resolve())\n",
    "print(\"OSM_EXCLUDED_IDS_TXT:\", OSM_EXCLUDED_IDS_TXT.resolve())\n",
    "print(\"CONFIG_DIR:\", CONFIG_DIR.resolve())\n",
    "print(\"DOCS_DIR:\", DOCS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77edacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers: JSON + extraction\n",
    "# =========================\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in a folder (non-recursive), sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "def read_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Read JSON with UTF-8; ignore decoding errors.\"\"\"\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "def normalize_dataset_record(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Handle possible wrapper {'dataset': {...}}.\"\"\"\n",
    "    if isinstance(raw, dict) and \"id\" in raw:\n",
    "        return raw\n",
    "    if isinstance(raw, dict) and \"dataset\" in raw and isinstance(raw[\"dataset\"], dict):\n",
    "        return raw[\"dataset\"]\n",
    "    return raw\n",
    "\n",
    "def get_org_title(ds: Dict[str, Any]) -> str:\n",
    "    org = ds.get(\"organization\")\n",
    "    if isinstance(org, dict):\n",
    "        return (org.get(\"title\") or org.get(\"name\") or \"\").strip()\n",
    "    return (org or \"\").strip()\n",
    "\n",
    "def get_tags(ds: Dict[str, Any]) -> List[str]:\n",
    "    tags = ds.get(\"tags\") or []\n",
    "    out: List[str] = []\n",
    "    if isinstance(tags, list):\n",
    "        for t in tags:\n",
    "            if isinstance(t, dict):\n",
    "                name = t.get(\"name\") or \"\"\n",
    "                if name:\n",
    "                    out.append(name.strip().lower())\n",
    "            elif isinstance(t, str):\n",
    "                out.append(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "def get_resource_formats(ds: Dict[str, Any]) -> List[str]:\n",
    "    formats: List[str] = []\n",
    "    for r in (ds.get(\"resources\") or []):\n",
    "        if isinstance(r, dict):\n",
    "            fmt = (r.get(\"format\") or \"\").strip().lower()\n",
    "            if fmt:\n",
    "                formats.append(fmt)\n",
    "    return formats\n",
    "\n",
    "def short_text(s: str, max_len: int) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s[:max_len] + (\"…\" if len(s) > max_len else \"\")\n",
    "\n",
    "def load_excluded_ids(path: Path) -> set[str]:\n",
    "    if not path.exists():\n",
    "        # For safety: if policy file missing, treat as no exclusions but warn.\n",
    "        print(f\"WARNING: OSM exclusion list not found: {path}. Proceeding with empty exclusion set.\")\n",
    "        return set()\n",
    "    ids = set()\n",
    "    for line in path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            ids.add(line)\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ee9007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 2,000/26,246\n",
      "processed 4,000/26,246\n",
      "processed 6,000/26,246\n",
      "processed 8,000/26,246\n",
      "processed 10,000/26,246\n",
      "processed 12,000/26,246\n",
      "processed 14,000/26,246\n",
      "processed 16,000/26,246\n",
      "processed 18,000/26,246\n",
      "processed 20,000/26,246\n",
      "processed 22,000/26,246\n",
      "processed 24,000/26,246\n",
      "processed 26,000/26,246\n",
      "processed 26,246/26,246\n",
      "\n",
      "Summary (non-OSM corpus):\n",
      "  total files:     26,246\n",
      "  excluded (OSM):  3,649\n",
      "  kept (non-OSM):  22,597\n",
      "  unique tags:     142\n",
      "  unique orgs:     357\n",
      "  unique formats:  47\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Scan corpus (excluding OSM)\n",
    "# =========================\n",
    "\n",
    "excluded_ids = load_excluded_ids(OSM_EXCLUDED_IDS_TXT)\n",
    "\n",
    "tag_counter = Counter()\n",
    "org_counter = Counter()\n",
    "fmt_counter = Counter()\n",
    "\n",
    "records_for_sampling: List[Dict[str, Any]] = []\n",
    "\n",
    "files = list(iter_json_files(DATASET_DIR))\n",
    "total = len(files)\n",
    "kept = 0\n",
    "skipped_osm = 0\n",
    "\n",
    "for i, path in enumerate(files, start=1):\n",
    "    if i % 2000 == 0 or i == total:\n",
    "        print(f\"processed {i:,}/{total:,}\")\n",
    "\n",
    "    raw = read_json(path)\n",
    "    ds = normalize_dataset_record(raw)\n",
    "\n",
    "    ds_id = (ds.get(\"id\") or \"\").strip()\n",
    "    if not ds_id:\n",
    "        continue\n",
    "\n",
    "    if ds_id in excluded_ids:\n",
    "        skipped_osm += 1\n",
    "        continue\n",
    "\n",
    "    kept += 1\n",
    "\n",
    "    tags = get_tags(ds)\n",
    "    org = get_org_title(ds)\n",
    "    fmts = get_resource_formats(ds)\n",
    "\n",
    "    tag_counter.update(tags)\n",
    "    if org:\n",
    "        org_counter.update([org])\n",
    "    fmt_counter.update(fmts)\n",
    "\n",
    "    # Keep minimal fields for later sampling export\n",
    "    records_for_sampling.append({\n",
    "        \"dataset_id\": ds_id,\n",
    "        \"title\": ds.get(\"title\") or \"\",\n",
    "        \"name\": ds.get(\"name\") or \"\",\n",
    "        \"organization\": org,\n",
    "        \"dataset_source\": ds.get(\"dataset_source\") or \"\",\n",
    "        \"license_title\": ds.get(\"license_title\") or ds.get(\"license_id\") or \"\",\n",
    "        \"tags\": tags,\n",
    "        \"formats\": fmts,\n",
    "        \"notes\": ds.get(\"notes\") or \"\",\n",
    "    })\n",
    "\n",
    "print(\"\\nSummary (non-OSM corpus):\")\n",
    "print(f\"  total files:     {total:,}\")\n",
    "print(f\"  excluded (OSM):  {skipped_osm:,}\")\n",
    "print(f\"  kept (non-OSM):  {kept:,}\")\n",
    "print(f\"  unique tags:     {len(tag_counter):,}\")\n",
    "print(f\"  unique orgs:     {len(org_counter):,}\")\n",
    "print(f\"  unique formats:  {len(fmt_counter):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6103d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top tags (non-OSM):\n",
      "  hxl                                       9,764\n",
      "  indicators                                7,387\n",
      "  geodata                                   5,761\n",
      "  health                                    2,925\n",
      "  baseline population                       2,301\n",
      "  food security                             2,147\n",
      "  economics                                 2,113\n",
      "  education                                 2,092\n",
      "  development                               1,496\n",
      "  environment                               1,483\n",
      "  demographics                              1,393\n",
      "  facilities-infrastructure                 1,349\n",
      "  conflict-violence                         1,265\n",
      "  climate-weather                           1,169\n",
      "  population                                1,141\n",
      "  internally displaced persons-idp          1,134\n",
      "  socioeconomics                            1,099\n",
      "  nutrition                                 1,066\n",
      "  funding                                     932\n",
      "  transportation                              915\n",
      "  displacement                                862\n",
      "  flooding                                    842\n",
      "  cyclones-hurricanes-typhoons                803\n",
      "  populated places-settlements                803\n",
      "  integrated food security phase classification-ipc    797\n",
      "  agriculture-livestock                       747\n",
      "  sustainable development goals-sdg           704\n",
      "  gender                                      702\n",
      "  poverty                                     686\n",
      "  administrative boundaries-divisions         653\n",
      "  trade                                       652\n",
      "  who is doing what and where-3w-4w-5w        637\n",
      "  maternity                                   628\n",
      "  refugees                                    627\n",
      "  water sanitation and hygiene-wash           625\n",
      "  disease                                     619\n",
      "  livelihoods                                 617\n",
      "  natural disasters                           589\n",
      "  children                                    581\n",
      "  hazards and risk                            567\n",
      "  energy                                      538\n",
      "  needs assessment                            532\n",
      "  complex emergency-conflict-security         495\n",
      "  covid-19                                    483\n",
      "  mental health                               466\n",
      "  women                                       457\n",
      "  protection                                  456\n",
      "  disability                                  453\n",
      "  aid effectiveness                           445\n",
      "  health facilities                           440\n",
      "  gender and age disaggregated data-gadd      436\n",
      "  vaccination-immunization                    430\n",
      "  fatalities                                  398\n",
      "  affected population                         396\n",
      "  disaster risk reduction-drr                 356\n",
      "  gazetteer                                   328\n",
      "  survey                                      324\n",
      "  earthquake-tsunami                          305\n",
      "  drought                                     298\n",
      "  aviation                                    271\n",
      "\n",
      "Top organizations (non-OSM):\n",
      "  World Bank Group                                         4,792\n",
      "  WorldPop                                                 1,569\n",
      "  United Nations Satellite Centre (UNOSAT)                 1,452\n",
      "  UNHCR - The UN Refugee Agency                            1,132\n",
      "  FEWS NET                                                   833\n",
      "  World Health Organization                                  676\n",
      "  HDX                                                        602\n",
      "  WFP - World Food Programme                                 490\n",
      "  Copernicus                                                 478\n",
      "  Food and Agriculture Organization (FAO) of the United Nations    441\n",
      "  Internal Displacement Monitoring Centre (IDMC)             426\n",
      "  UNICEF Data and Analytics (HQ)                             292\n",
      "  United Nations Human Settlements Programmes, Data and Analytics Section    284\n",
      "  International Organization for Migration (IOM)             273\n",
      "  Kontur                                                     254\n",
      "  UNESCO                                                     251\n",
      "  Who's On First                                             248\n",
      "  Armed Conflict Location & Event Data Project (ACLED)       246\n",
      "  International Aid Transparency Initiative                  242\n",
      "  UNDP Human Development Reports Office (HDRO)               228\n",
      "  OurAirports                                                225\n",
      "  AI and Data for Good at Meta                               223\n",
      "  OCHA Field Information Services Section (FISS)             219\n",
      "  United Nations Office for Disaster Risk Reduction (UNDRR)    208\n",
      "  International Federation of Red Cross and Red Crescent Societies (IFRC)    198\n",
      "  Violence & Impacts Early-Warning System                    193\n",
      "  HeiGIT (Heidelberg Institute for Geoinformation Technology)    192\n",
      "  OCHA Financial Tracking System (FTS)                       191\n",
      "  iMMAP Inc.                                                 185\n",
      "  PortWatch                                                  184\n",
      "\n",
      "Top resource formats (non-OSM):\n",
      "  geotiff      78,304\n",
      "  csv          49,553\n",
      "  xlsx          8,184\n",
      "  shp           4,532\n",
      "  geojson       4,317\n",
      "  kml           3,304\n",
      "  pdf           1,448\n",
      "  zip           1,369\n",
      "  geodatabase   1,332\n",
      "  geopackage    1,292\n",
      "  web app       1,000\n",
      "  json            592\n",
      "  xls             405\n",
      "  png             369\n",
      "  xml             297\n",
      "  kmz             117\n",
      "  emf              80\n",
      "  txt              67\n",
      "  docx             55\n",
      "  geoservice       42\n",
      "  google sheet     39\n",
      "  arc/info grid     24\n",
      "  stata data file     20\n",
      "  jpeg             19\n",
      "  mbtiles          16\n",
      "  html             15\n",
      "  tsv              11\n",
      "  qlikview data file     11\n",
      "  esri arcmap project file      9\n",
      "  zipped tif        8\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Inspect top tags/orgs/formats\n",
    "# =========================\n",
    "\n",
    "TOP_N = 60\n",
    "\n",
    "print(\"Top tags (non-OSM):\")\n",
    "for t, c in tag_counter.most_common(TOP_N):\n",
    "    print(f\"  {t:<40} {c:>6,}\")\n",
    "\n",
    "print(\"\\nTop organizations (non-OSM):\")\n",
    "for o, c in org_counter.most_common(30):\n",
    "    print(f\"  {o:<55} {c:>6,}\")\n",
    "\n",
    "print(\"\\nTop resource formats (non-OSM):\")\n",
    "for f, c in fmt_counter.most_common(30):\n",
    "    print(f\"  {f:<12} {c:>6,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363c93d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote sample review file: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\docs\\samples_for_mapping.csv\n",
      "Sample rows: 440\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Create review sample CSV\n",
    "# =========================\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# 1) Random sample\n",
    "sample = random.sample(records_for_sampling, k=min(SAMPLE_SIZE, len(records_for_sampling)))\n",
    "\n",
    "# 2) Ensure we include some examples for the top tags (calibration)\n",
    "top_tags = [t for t, _ in tag_counter.most_common(40)]\n",
    "tag_to_examples: Dict[str, int] = defaultdict(int)\n",
    "\n",
    "# Add up to one dataset per top tag if missing in the random sample\n",
    "present_ids = {r[\"dataset_id\"] for r in sample}\n",
    "for tag in top_tags:\n",
    "    for r in records_for_sampling:\n",
    "        if r[\"dataset_id\"] in present_ids:\n",
    "            continue\n",
    "        if tag in r[\"tags\"]:\n",
    "            sample.append(r)\n",
    "            present_ids.add(r[\"dataset_id\"])\n",
    "            tag_to_examples[tag] += 1\n",
    "            break\n",
    "\n",
    "out_sample_csv = DOCS_DIR / \"samples_for_mapping.csv\"\n",
    "with out_sample_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\n",
    "            \"dataset_id\", \"title\", \"organization\", \"dataset_source\", \"license_title\",\n",
    "            \"tags\", \"formats\", \"notes_snippet\"\n",
    "        ],\n",
    "    )\n",
    "    w.writeheader()\n",
    "    for r in sample:\n",
    "        w.writerow({\n",
    "            \"dataset_id\": r[\"dataset_id\"],\n",
    "            \"title\": r[\"title\"],\n",
    "            \"organization\": r[\"organization\"],\n",
    "            \"dataset_source\": r[\"dataset_source\"],\n",
    "            \"license_title\": r[\"license_title\"],\n",
    "            \"tags\": \";\".join(r[\"tags\"]),\n",
    "            \"formats\": \";\".join(sorted(set(r[\"formats\"]))),\n",
    "            \"notes_snippet\": short_text(r[\"notes\"], MAX_NOTES_CHARS),\n",
    "        })\n",
    "\n",
    "print(\"Wrote sample review file:\", out_sample_csv.resolve())\n",
    "print(\"Sample rows:\", len(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f421d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Minimal YAML writer (safe, limited)\n",
    "# =========================\n",
    "\n",
    "def yaml_escape(s: str) -> str:\n",
    "    # Quote strings that contain special characters or start with risky chars.\n",
    "    if s == \"\" or any(ch in s for ch in [\":\", \"#\", \"{\", \"}\", \"[\", \"]\", \",\", \"\\n\", \"\\r\", \"\\t\"]) or s.strip() != s:\n",
    "        return '\"' + s.replace('\"', '\\\\\"') + '\"'\n",
    "    # YAML booleans/null can be misread; quote them\n",
    "    if s.lower() in {\"true\", \"false\", \"null\", \"~\"}:\n",
    "        return '\"' + s + '\"'\n",
    "    return s\n",
    "\n",
    "def dump_yaml(obj: Any, indent: int = 0) -> str:\n",
    "    sp = \"  \" * indent\n",
    "    if isinstance(obj, dict):\n",
    "        lines = []\n",
    "        for k in sorted(obj.keys()):\n",
    "            v = obj[k]\n",
    "            key = yaml_escape(str(k))\n",
    "            if isinstance(v, (dict, list)):\n",
    "                lines.append(f\"{sp}{key}:\")\n",
    "                lines.append(dump_yaml(v, indent + 1))\n",
    "            else:\n",
    "                lines.append(f\"{sp}{key}: {yaml_escape(str(v))}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    if isinstance(obj, list):\n",
    "        lines = []\n",
    "        for item in obj:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                lines.append(f\"{sp}-\")\n",
    "                lines.append(dump_yaml(item, indent + 1))\n",
    "            else:\n",
    "                lines.append(f\"{sp}- {yaml_escape(str(item))}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    return f\"{sp}{yaml_escape(str(obj))}\"\n",
    "\n",
    "def write_yaml(path: Path, obj: Any, overwrite: bool = False) -> None:\n",
    "    if path.exists() and not overwrite:\n",
    "        print(f\"SKIP (exists): {path}\")\n",
    "        return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(dump_yaml(obj) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(\"Wrote:\", path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64bb1513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\\tag_to_rdls_component.yaml\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\\keyword_to_rdls_component.yaml\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\\org_hints.yaml\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Draft mapping configs (starter set)\n",
    "# =========================\n",
    "# These are intentionally minimal, meant to be refined by your team after reviewing docs/samples_for_mapping.csv.\n",
    "# Weights: 1 (weak) .. 5 (strong). You can use fractional weights if you want, but keep it simple at first.\n",
    "\n",
    "tag_to_rdls = {\n",
    "    \"hazard\": {\n",
    "        # classic hazard event tags\n",
    "        \"flooding\": 5,\n",
    "        \"drought\": 5,\n",
    "        \"cyclones-hurricanes-typhoons\": 5,\n",
    "        \"earthquake-tsunami\": 5,\n",
    "        \"climate hazards\": 4,\n",
    "        \"hydrology\": 3,  # often hazard-supporting\n",
    "        \"natural disasters\": 3,\n",
    "        \"hazards and risk\": 3,\n",
    "        \"forecasting\": 2,  # could be hazard services\n",
    "        \"topography\": 2,   # hazard-supporting\n",
    "    },\n",
    "    \"loss_impact\": {\n",
    "        \"damage assessment\": 5,\n",
    "        \"casualties\": 5,\n",
    "        \"fatalities\": 5,\n",
    "        \"mortality\": 4,\n",
    "        \"affected population\": 4,\n",
    "        \"affected area\": 4,\n",
    "        \"people in need-pin\": 3,\n",
    "        \"severity\": 3,\n",
    "    },\n",
    "    \"exposure\": {\n",
    "        \"facilities-infrastructure\": 5,\n",
    "        \"populated places-settlements\": 4,\n",
    "        \"population\": 4,\n",
    "        \"roads\": 4,\n",
    "        \"railways\": 3,\n",
    "        \"ports\": 3,\n",
    "        \"aviation\": 3,\n",
    "        \"points of interest-poi\": 3,\n",
    "        \"health facilities\": 4,\n",
    "        \"education facilities-schools\": 4,\n",
    "        \"energy\": 3,\n",
    "        \"gazetteer\": 2,\n",
    "        \"geodata\": 2,\n",
    "    },\n",
    "    \"vulnerability_proxy\": {\n",
    "        \"demographics\": 4,\n",
    "        \"poverty\": 4,\n",
    "        \"socioeconomics\": 4,\n",
    "        \"disability\": 3,\n",
    "        \"gender\": 3,\n",
    "        \"food security\": 3,\n",
    "        \"health\": 2,\n",
    "        \"education\": 2,\n",
    "        \"livelihoods\": 2,\n",
    "        \"nutrition\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Keyword patterns applied to (title + notes); these are regex snippets.\n",
    "keyword_to_rdls = {\n",
    "    \"hazard\": [\n",
    "        r\"\\bflood(s|ing)?\\b\",\n",
    "        r\"\\bdrought\\b\",\n",
    "        r\"\\bcyclone(s)?\\b\",\n",
    "        r\"\\bhurricane(s)?\\b\",\n",
    "        r\"\\btyphoon(s)?\\b\",\n",
    "        r\"\\bearthquake(s)?\\b\",\n",
    "        r\"\\btsunami\\b\",\n",
    "        r\"\\breturn period\\b\",\n",
    "        r\"\\bhazard\\b\",\n",
    "    ],\n",
    "    \"loss_impact\": [\n",
    "        r\"\\bdamage\\b\",\n",
    "        r\"\\bloss(es)?\\b\",\n",
    "        r\"\\bcost(s)?\\b\",\n",
    "        r\"\\bfatalit(y|ies)\\b\",\n",
    "        r\"\\bcasualt(y|ies)\\b\",\n",
    "        r\"\\baffected\\b\",\n",
    "    ],\n",
    "    \"exposure\": [\n",
    "        r\"\\bairport(s)?\\b\",\n",
    "        r\"\\broad(s)?\\b\",\n",
    "        r\"\\bbridge(s)?\\b\",\n",
    "        r\"\\bport(s)?\\b\",\n",
    "        r\"\\bhospital(s)?\\b\",\n",
    "        r\"\\bschool(s)?\\b\",\n",
    "        r\"\\bfacilit(y|ies)\\b\",\n",
    "        r\"\\binfrastructure\\b\",\n",
    "        r\"\\bbuildings?\\b\",\n",
    "        r\"\\bsettlement(s)?\\b\",\n",
    "    ],\n",
    "    \"vulnerability_proxy\": [\n",
    "        r\"\\bpoverty\\b\",\n",
    "        r\"\\bdisabilit(y|ies)\\b\",\n",
    "        r\"\\bmalnutrition\\b\",\n",
    "        r\"\\bfood security\\b\",\n",
    "        r\"\\bhealth indicator(s)?\\b\",\n",
    "        r\"\\bdemographic(s)?\\b\",\n",
    "        r\"\\bvulnerability\\b\",\n",
    "        r\"\\bhousehold(s)?\\b\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Organization hints (optional, but very useful for indicator packs)\n",
    "org_hints = {\n",
    "    # Examples to refine; add more after inspecting org_counter\n",
    "    \"World Bank Group\": {\"vulnerability_proxy\": 2},\n",
    "    \"The DHS Program\": {\"vulnerability_proxy\": 4},\n",
    "    \"Food and Agriculture Organization\": {\"vulnerability_proxy\": 3},\n",
    "    \"UNICEF\": {\"vulnerability_proxy\": 3},\n",
    "}\n",
    "\n",
    "# Where to write\n",
    "path_tag_yaml = CONFIG_DIR / \"tag_to_rdls_component.yaml\"\n",
    "path_kw_yaml = CONFIG_DIR / \"keyword_to_rdls_component.yaml\"\n",
    "path_org_yaml = CONFIG_DIR / \"org_hints.yaml\"\n",
    "\n",
    "write_yaml(path_tag_yaml, tag_to_rdls, overwrite=OVERWRITE_CONFIG)\n",
    "write_yaml(path_kw_yaml, keyword_to_rdls, overwrite=OVERWRITE_CONFIG)\n",
    "write_yaml(path_org_yaml, org_hints, overwrite=OVERWRITE_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "132418b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\docs\\mapping_rules.md\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Write mapping rules doc\n",
    "# =========================\n",
    "\n",
    "rules_md = DOCS_DIR / \"mapping_rules.md\"\n",
    "\n",
    "if (not rules_md.exists()) or OVERWRITE_CONFIG:\n",
    "    content = f\"\"\"# Mapping Rules (Draft)\n",
    "\n",
    "This document describes the *draft* mapping used to translate HDX dataset metadata into inclusive RDLS components.\n",
    "\n",
    "## Components\n",
    "- **hazard**: datasets describing hazard events/intensity/footprints or hazard model inputs (e.g., flood, drought).\n",
    "- **exposure**: datasets describing exposed elements (population, facilities, infrastructure).\n",
    "- **vulnerability_proxy**: indicator datasets used as proxies for vulnerability/sensitivity (health, poverty, food security).\n",
    "- **loss_impact**: datasets describing observed/estimated impacts (damage, fatalities, affected population).\n",
    "\n",
    "## Evidence sources (in priority order)\n",
    "1. **HDX tags** (weighted)\n",
    "2. **Keywords** in title/notes (regex patterns)\n",
    "3. **Organization hints** (helps with indicator series)\n",
    "\n",
    "## OSM policy\n",
    "OSM-derived datasets are excluded from downstream RDLS translation using the exclusion list in:\n",
    "`{OSM_EXCLUDED_IDS_TXT.as_posix()}`\n",
    "\n",
    "## Next steps\n",
    "1. Review `docs/samples_for_mapping.csv` to calibrate tag/keyword weights.\n",
    "2. Expand `config/tag_to_rdls_component.yaml` to cover common tags in your corpus.\n",
    "3. Add organization mappings for frequent publishers (World Bank, FAO, DHS, etc.).\n",
    "\"\"\"\n",
    "    rules_md.write_text(content, encoding=\"utf-8\")\n",
    "    print(\"Wrote:\", rules_md.resolve())\n",
    "else:\n",
    "    print(\"SKIP (exists):\", rules_md.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17295c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3 complete.\n",
      "Next notebooks:\n",
      "  - Step 4 will read YAML configs from: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\n",
      "  - Step 4 will use policy exclusion list from: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\policy\\osm_excluded_dataset_ids.txt\n",
      "\n",
      "Files created:\n",
      "  - C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\\tag_to_rdls_component.yaml\n",
      "  - C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\\keyword_to_rdls_component.yaml\n",
      "  - C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\\org_hints.yaml\n",
      "  - C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\docs\\mapping_rules.md\n",
      "  - C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\docs\\samples_for_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Next: where this plugs in\n",
    "# =========================\n",
    "\n",
    "print(\"\\nStep 3 complete.\")\n",
    "print(\"Next notebooks:\")\n",
    "print(\"  - Step 4 will read YAML configs from:\", CONFIG_DIR.resolve())\n",
    "print(\"  - Step 4 will use policy exclusion list from:\", OSM_EXCLUDED_IDS_TXT.resolve())\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  -\", (CONFIG_DIR / 'tag_to_rdls_component.yaml').resolve())\n",
    "print(\"  -\", (CONFIG_DIR / 'keyword_to_rdls_component.yaml').resolve())\n",
    "print(\"  -\", (CONFIG_DIR / 'org_hints.yaml').resolve())\n",
    "print(\"  -\", (DOCS_DIR / 'mapping_rules.md').resolve())\n",
    "print(\"  -\", (DOCS_DIR / 'samples_for_mapping.csv').resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8f630-c9d2-48fe-81b0-24274b6dcb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
