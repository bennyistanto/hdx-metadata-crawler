{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 03: Define RDLS Mapping Model\n",
    "\n",
    "**Purpose**: Create mapping configuration for classifying HDX datasets into RDLS components.\n",
    "\n",
    "**Process**:\n",
    "1. Scan corpus to collect tag/org/format statistics\n",
    "2. Generate weighted mapping configs (tags → RDLS components)\n",
    "3. Create keyword pattern rules for title/notes\n",
    "4. Produce review sample for calibration\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n1.2 Configure Paths\n\"\"\"\n\nNOTEBOOK_DIR = Path.cwd()\nBASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n\n# Input directories\nDUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\nDATASET_DIR = DUMP_DIR / 'dataset_metadata'\nPOLICY_DIR = DUMP_DIR / 'policy'\nOSM_EXCLUDED_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n\n# Output directories\nCONFIG_DIR = DUMP_DIR / 'config'\nREFERENCE_DIR = DUMP_DIR / 'reference'  # Pipeline reference materials (mapping docs, samples)\n\n# Configuration\nRANDOM_SEED = 42\nSAMPLE_SIZE = 400\nMAX_NOTES_CHARS = 350\nOVERWRITE_CONFIG = False\n\n# Create directories\nCONFIG_DIR.mkdir(parents=True, exist_ok=True)\nREFERENCE_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Dataset dir: {DATASET_DIR}\")\nprint(f\"Config dir: {CONFIG_DIR}\")\nprint(f\"Reference dir: {REFERENCE_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 JSON and Data Extraction Helpers\n",
    "\"\"\"\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in folder, sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "def read_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Read JSON with UTF-8 encoding.\"\"\"\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "\n",
    "def normalize_dataset_record(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Handle possible wrapper {'dataset': {...}}.\"\"\"\n",
    "    if isinstance(raw, dict) and \"id\" in raw:\n",
    "        return raw\n",
    "    if isinstance(raw, dict) and \"dataset\" in raw and isinstance(raw[\"dataset\"], dict):\n",
    "        return raw[\"dataset\"]\n",
    "    return raw\n",
    "\n",
    "\n",
    "def get_org_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract organization title.\"\"\"\n",
    "    org = ds.get(\"organization\")\n",
    "    if isinstance(org, dict):\n",
    "        return (org.get(\"title\") or org.get(\"name\") or \"\").strip()\n",
    "    return (org or \"\").strip()\n",
    "\n",
    "\n",
    "def get_tags(ds: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Extract tags as lowercase strings.\"\"\"\n",
    "    tags = ds.get(\"tags\") or []\n",
    "    out: List[str] = []\n",
    "    if isinstance(tags, list):\n",
    "        for t in tags:\n",
    "            if isinstance(t, dict):\n",
    "                name = t.get(\"name\") or \"\"\n",
    "                if name:\n",
    "                    out.append(name.strip().lower())\n",
    "            elif isinstance(t, str):\n",
    "                out.append(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_resource_formats(ds: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Extract resource formats.\"\"\"\n",
    "    formats: List[str] = []\n",
    "    for r in (ds.get(\"resources\") or []):\n",
    "        if isinstance(r, dict):\n",
    "            fmt = (r.get(\"format\") or \"\").strip().lower()\n",
    "            if fmt:\n",
    "                formats.append(fmt)\n",
    "    return formats\n",
    "\n",
    "\n",
    "def short_text(s: str, max_len: int) -> str:\n",
    "    \"\"\"Truncate text with ellipsis.\"\"\"\n",
    "    s = (s or \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s[:max_len] + (\"…\" if len(s) > max_len else \"\")\n",
    "\n",
    "\n",
    "def load_excluded_ids(path: Path) -> set:\n",
    "    \"\"\"Load OSM exclusion list.\"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: OSM exclusion list not found: {path}\")\n",
    "        return set()\n",
    "    ids = set()\n",
    "    for line in path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            ids.add(line)\n",
    "    return ids\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Scan Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Load Exclusion List and Scan Files\n",
    "\n",
    "Collect statistics on tags, organizations, and formats.\n",
    "\"\"\"\n",
    "\n",
    "excluded_ids = load_excluded_ids(OSM_EXCLUDED_IDS_TXT)\n",
    "print(f\"Loaded {len(excluded_ids):,} excluded OSM dataset IDs\")\n",
    "\n",
    "# Counters\n",
    "tag_counter = Counter()\n",
    "org_counter = Counter()\n",
    "fmt_counter = Counter()\n",
    "\n",
    "# Records for sampling\n",
    "records_for_sampling: List[Dict[str, Any]] = []\n",
    "\n",
    "# Scan files\n",
    "files = list(iter_json_files(DATASET_DIR))\n",
    "total = len(files)\n",
    "kept = 0\n",
    "skipped_osm = 0\n",
    "\n",
    "print(f\"Scanning {total:,} JSON files...\")\n",
    "\n",
    "iterator = tqdm(files, desc=\"Scanning corpus\") if HAS_TQDM else files\n",
    "\n",
    "for i, path in enumerate(iterator, start=1):\n",
    "    if not HAS_TQDM and i % 5000 == 0:\n",
    "        print(f\"  Processed {i:,}/{total:,}\")\n",
    "    \n",
    "    raw = read_json(path)\n",
    "    ds = normalize_dataset_record(raw)\n",
    "    \n",
    "    ds_id = (ds.get(\"id\") or \"\").strip()\n",
    "    if not ds_id:\n",
    "        continue\n",
    "    \n",
    "    if ds_id in excluded_ids:\n",
    "        skipped_osm += 1\n",
    "        continue\n",
    "    \n",
    "    kept += 1\n",
    "    \n",
    "    tags = get_tags(ds)\n",
    "    org = get_org_title(ds)\n",
    "    fmts = get_resource_formats(ds)\n",
    "    \n",
    "    tag_counter.update(tags)\n",
    "    if org:\n",
    "        org_counter.update([org])\n",
    "    fmt_counter.update(fmts)\n",
    "    \n",
    "    records_for_sampling.append({\n",
    "        \"dataset_id\": ds_id,\n",
    "        \"title\": ds.get(\"title\") or \"\",\n",
    "        \"name\": ds.get(\"name\") or \"\",\n",
    "        \"organization\": org,\n",
    "        \"dataset_source\": ds.get(\"dataset_source\") or \"\",\n",
    "        \"license_title\": ds.get(\"license_title\") or ds.get(\"license_id\") or \"\",\n",
    "        \"tags\": tags,\n",
    "        \"formats\": fmts,\n",
    "        \"notes\": ds.get(\"notes\") or \"\",\n",
    "    })\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CORPUS SUMMARY (non-OSM)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total files: {total:,}\")\n",
    "print(f\"Excluded (OSM): {skipped_osm:,}\")\n",
    "print(f\"Kept (non-OSM): {kept:,}\")\n",
    "print(f\"Unique tags: {len(tag_counter):,}\")\n",
    "print(f\"Unique orgs: {len(org_counter):,}\")\n",
    "print(f\"Unique formats: {len(fmt_counter):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.2 Display Top Statistics\n",
    "\"\"\"\n",
    "\n",
    "TOP_N = 40\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOP {TOP_N} TAGS\")\n",
    "print(f\"{'='*60}\")\n",
    "for t, c in tag_counter.most_common(TOP_N):\n",
    "    print(f\"  {t:<45} {c:>6,}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOP 20 ORGANIZATIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "for o, c in org_counter.most_common(20):\n",
    "    print(f\"  {o:<55} {c:>6,}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOP 20 FORMATS\")\n",
    "print(f\"{'='*60}\")\n",
    "for f, c in fmt_counter.most_common(20):\n",
    "    print(f\"  {f:<15} {c:>8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Create Review Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n4.1 Generate Review Sample CSV\n\nRandom sample plus examples for top tags.\n\"\"\"\n\nrandom.seed(RANDOM_SEED)\n\n# Random sample\nsample = random.sample(records_for_sampling, k=min(SAMPLE_SIZE, len(records_for_sampling)))\n\n# Ensure examples for top tags\ntop_tags = [t for t, _ in tag_counter.most_common(40)]\npresent_ids = {r[\"dataset_id\"] for r in sample}\n\nfor tag in top_tags:\n    for r in records_for_sampling:\n        if r[\"dataset_id\"] in present_ids:\n            continue\n        if tag in r[\"tags\"]:\n            sample.append(r)\n            present_ids.add(r[\"dataset_id\"])\n            break\n\n# Write sample CSV\nout_sample_csv = REFERENCE_DIR / \"samples_for_mapping.csv\"\n\nwith out_sample_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n    w = csv.DictWriter(\n        f,\n        fieldnames=[\n            \"dataset_id\", \"title\", \"organization\", \"dataset_source\", \n            \"license_title\", \"tags\", \"formats\", \"notes_snippet\"\n        ],\n    )\n    w.writeheader()\n    for r in sample:\n        w.writerow({\n            \"dataset_id\": r[\"dataset_id\"],\n            \"title\": r[\"title\"],\n            \"organization\": r[\"organization\"],\n            \"dataset_source\": r[\"dataset_source\"],\n            \"license_title\": r[\"license_title\"],\n            \"tags\": \";\".join(r[\"tags\"]),\n            \"formats\": \";\".join(sorted(set(r[\"formats\"]))),\n            \"notes_snippet\": short_text(r[\"notes\"], MAX_NOTES_CHARS),\n        })\n\nprint(f\"Wrote: {out_sample_csv}\")\nprint(f\"Sample rows: {len(sample)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Generate Mapping Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Minimal YAML Writer\n",
    "\n",
    "Safe YAML serializer without external dependencies.\n",
    "\"\"\"\n",
    "\n",
    "def yaml_escape(s: str) -> str:\n",
    "    \"\"\"Escape string for YAML if needed.\"\"\"\n",
    "    if s == \"\" or any(ch in s for ch in [\":\", \"#\", \"{\", \"}\", \"[\", \"]\", \",\", \"\\n\", \"\\r\", \"\\t\"]) or s.strip() != s:\n",
    "        return '\"' + s.replace('\"', '\\\\\"') + '\"'\n",
    "    if s.lower() in {\"true\", \"false\", \"null\", \"~\"}:\n",
    "        return '\"' + s + '\"'\n",
    "    return s\n",
    "\n",
    "\n",
    "def dump_yaml(obj: Any, indent: int = 0) -> str:\n",
    "    \"\"\"Convert object to YAML string.\"\"\"\n",
    "    sp = \"  \" * indent\n",
    "    if isinstance(obj, dict):\n",
    "        lines = []\n",
    "        for k in sorted(obj.keys()):\n",
    "            v = obj[k]\n",
    "            key = yaml_escape(str(k))\n",
    "            if isinstance(v, (dict, list)):\n",
    "                lines.append(f\"{sp}{key}:\")\n",
    "                lines.append(dump_yaml(v, indent + 1))\n",
    "            else:\n",
    "                lines.append(f\"{sp}{key}: {yaml_escape(str(v))}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    if isinstance(obj, list):\n",
    "        lines = []\n",
    "        for item in obj:\n",
    "            if isinstance(item, (dict, list)):\n",
    "                lines.append(f\"{sp}-\")\n",
    "                lines.append(dump_yaml(item, indent + 1))\n",
    "            else:\n",
    "                lines.append(f\"{sp}- {yaml_escape(str(item))}\")\n",
    "        return \"\\n\".join(lines)\n",
    "    return f\"{sp}{yaml_escape(str(obj))}\"\n",
    "\n",
    "\n",
    "def write_yaml(path: Path, obj: Any, overwrite: bool = False) -> None:\n",
    "    \"\"\"Write object to YAML file.\"\"\"\n",
    "    if path.exists() and not overwrite:\n",
    "        print(f\"SKIP (exists): {path}\")\n",
    "        return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(dump_yaml(obj) + \"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"Wrote: {path}\")\n",
    "\n",
    "\n",
    "print(\"YAML writer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Define Mapping Rules\n",
    "\n",
    "Tag weights, keyword patterns, and organization hints.\n",
    "\"\"\"\n",
    "\n",
    "# Tag to RDLS component mapping (weights: 1=weak, 5=strong)\n",
    "tag_to_rdls = {\n",
    "    \"hazard\": {\n",
    "        \"flooding\": 5,\n",
    "        \"drought\": 5,\n",
    "        \"cyclones-hurricanes-typhoons\": 5,\n",
    "        \"earthquake-tsunami\": 5,\n",
    "        \"climate hazards\": 4,\n",
    "        \"hydrology\": 3,\n",
    "        \"natural disasters\": 3,\n",
    "        \"hazards and risk\": 3,\n",
    "        \"forecasting\": 2,\n",
    "        \"topography\": 2,\n",
    "    },\n",
    "    \"loss_impact\": {\n",
    "        \"damage assessment\": 5,\n",
    "        \"casualties\": 5,\n",
    "        \"fatalities\": 5,\n",
    "        \"mortality\": 4,\n",
    "        \"affected population\": 4,\n",
    "        \"affected area\": 4,\n",
    "        \"people in need-pin\": 3,\n",
    "        \"severity\": 3,\n",
    "    },\n",
    "    \"exposure\": {\n",
    "        \"facilities-infrastructure\": 5,\n",
    "        \"populated places-settlements\": 4,\n",
    "        \"population\": 4,\n",
    "        \"roads\": 4,\n",
    "        \"railways\": 3,\n",
    "        \"ports\": 3,\n",
    "        \"aviation\": 3,\n",
    "        \"points of interest-poi\": 3,\n",
    "        \"health facilities\": 4,\n",
    "        \"education facilities-schools\": 4,\n",
    "        \"energy\": 3,\n",
    "        \"gazetteer\": 2,\n",
    "        \"geodata\": 2,\n",
    "    },\n",
    "    \"vulnerability_proxy\": {\n",
    "        \"demographics\": 4,\n",
    "        \"poverty\": 4,\n",
    "        \"socioeconomics\": 4,\n",
    "        \"disability\": 3,\n",
    "        \"gender\": 3,\n",
    "        \"food security\": 3,\n",
    "        \"health\": 2,\n",
    "        \"education\": 2,\n",
    "        \"livelihoods\": 2,\n",
    "        \"nutrition\": 2,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Keyword patterns for title/notes (regex)\n",
    "keyword_to_rdls = {\n",
    "    \"hazard\": [\n",
    "        r\"\\bflood(s|ing)?\\b\",\n",
    "        r\"\\bdrought\\b\",\n",
    "        r\"\\bcyclone(s)?\\b\",\n",
    "        r\"\\bhurricane(s)?\\b\",\n",
    "        r\"\\btyphoon(s)?\\b\",\n",
    "        r\"\\bearthquake(s)?\\b\",\n",
    "        r\"\\btsunami\\b\",\n",
    "        r\"\\breturn period\\b\",\n",
    "        r\"\\bhazard\\b\",\n",
    "    ],\n",
    "    \"loss_impact\": [\n",
    "        r\"\\bdamage\\b\",\n",
    "        r\"\\bloss(es)?\\b\",\n",
    "        r\"\\bcost(s)?\\b\",\n",
    "        r\"\\bfatalit(y|ies)\\b\",\n",
    "        r\"\\bcasualt(y|ies)\\b\",\n",
    "        r\"\\baffected\\b\",\n",
    "    ],\n",
    "    \"exposure\": [\n",
    "        r\"\\bairport(s)?\\b\",\n",
    "        r\"\\broad(s)?\\b\",\n",
    "        r\"\\bbridge(s)?\\b\",\n",
    "        r\"\\bport(s)?\\b\",\n",
    "        r\"\\bhospital(s)?\\b\",\n",
    "        r\"\\bschool(s)?\\b\",\n",
    "        r\"\\bfacilit(y|ies)\\b\",\n",
    "        r\"\\binfrastructure\\b\",\n",
    "        r\"\\bbuildings?\\b\",\n",
    "        r\"\\bsettlement(s)?\\b\",\n",
    "    ],\n",
    "    \"vulnerability_proxy\": [\n",
    "        r\"\\bpoverty\\b\",\n",
    "        r\"\\bdisabilit(y|ies)\\b\",\n",
    "        r\"\\bmalnutrition\\b\",\n",
    "        r\"\\bfood security\\b\",\n",
    "        r\"\\bhealth indicator(s)?\\b\",\n",
    "        r\"\\bdemographic(s)?\\b\",\n",
    "        r\"\\bvulnerability\\b\",\n",
    "        r\"\\bhousehold(s)?\\b\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Organization hints\n",
    "org_hints = {\n",
    "    \"World Bank Group\": {\"vulnerability_proxy\": 2},\n",
    "    \"The DHS Program\": {\"vulnerability_proxy\": 4},\n",
    "    \"Food and Agriculture Organization\": {\"vulnerability_proxy\": 3},\n",
    "    \"UNICEF\": {\"vulnerability_proxy\": 3},\n",
    "}\n",
    "\n",
    "print(\"Mapping rules defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.3 Write Mapping Config Files\n",
    "\"\"\"\n",
    "\n",
    "# Output paths\n",
    "path_tag_yaml = CONFIG_DIR / \"tag_to_rdls_component.yaml\"\n",
    "path_kw_yaml = CONFIG_DIR / \"keyword_to_rdls_component.yaml\"\n",
    "path_org_yaml = CONFIG_DIR / \"org_hints.yaml\"\n",
    "\n",
    "# Write configs\n",
    "write_yaml(path_tag_yaml, tag_to_rdls, overwrite=OVERWRITE_CONFIG)\n",
    "write_yaml(path_kw_yaml, keyword_to_rdls, overwrite=OVERWRITE_CONFIG)\n",
    "write_yaml(path_org_yaml, org_hints, overwrite=OVERWRITE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-4",
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\n5.4 Write Mapping Rules Documentation\n\"\"\"\n\nrules_md = REFERENCE_DIR / \"mapping_rules.md\"\n\nif (not rules_md.exists()) or OVERWRITE_CONFIG:\n    content = f\"\"\"# Mapping Rules (Draft)\n\nThis document describes the mapping used to translate HDX dataset metadata into RDLS components.\n\n## Components\n\n- **hazard**: Datasets describing hazard events/intensity/footprints or hazard model inputs\n- **exposure**: Datasets describing exposed elements (population, facilities, infrastructure)\n- **vulnerability_proxy**: Indicator datasets used as proxies for vulnerability/sensitivity\n- **loss_impact**: Datasets describing observed/estimated impacts (damage, fatalities)\n\n## Evidence Sources (Priority Order)\n\n1. **HDX tags** (weighted)\n2. **Keywords** in title/notes (regex patterns)\n3. **Organization hints** (helps with indicator series)\n\n## OSM Policy\n\nOSM-derived datasets are excluded using the exclusion list in:\n`{OSM_EXCLUDED_IDS_TXT.as_posix()}`\n\n## Configuration Files\n\n- `config/tag_to_rdls_component.yaml` - Tag weights\n- `config/keyword_to_rdls_component.yaml` - Keyword patterns\n- `config/org_hints.yaml` - Organization hints\n\n## Next Steps\n\n1. Review `reference/samples_for_mapping.csv` to calibrate weights\n2. Expand tag mappings for common tags in corpus\n3. Add organization mappings for frequent publishers\n\n---\n*Generated by HDX-RDLS Pipeline Notebook 03*\n\"\"\"\n    rules_md.write_text(content, encoding=\"utf-8\")\n    print(f\"Wrote: {rules_md}\")\nelse:\n    print(f\"SKIP (exists): {rules_md}\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Display Summary\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MAPPING CONFIG COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nCorpus statistics:\")\n",
    "print(f\"  Non-OSM datasets: {kept:,}\")\n",
    "print(f\"  Unique tags: {len(tag_counter):,}\")\n",
    "print(f\"  Unique organizations: {len(org_counter):,}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - {path_tag_yaml}\")\n",
    "print(f\"  - {path_kw_yaml}\")\n",
    "print(f\"  - {path_org_yaml}\")\n",
    "print(f\"  - {rules_md}\")\n",
    "print(f\"  - {out_sample_csv}\")\n",
    "\n",
    "print(f\"\\nNext: Run Notebook 04 to classify datasets using these configs.\")\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}