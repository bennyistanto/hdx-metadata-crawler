{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Step 6 - Translate HDX Metadata to RDLS v0.3 JSON\n",
    "\n",
    "**Purpose:** Transform HDX dataset-level metadata exports into RDLS v0.3 metadata records.\n",
    "\n",
    "**Process:**\n",
    "1. Load classification results and included dataset IDs from Step 5\n",
    "2. Build RDLS-compliant records with proper attributions, resources, and spatial info\n",
    "3. Apply component gating rules (V/L require H or E)\n",
    "4. Validate against JSON Schema and write outputs\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- Step 5 outputs:\n",
    "  - `hdx_dataset_metadata_dump/derived/classification_final.csv`\n",
    "  - `hdx_dataset_metadata_dump/derived/rdls_included_dataset_ids_final.txt`\n",
    "- HDX dataset metadata JSON corpus:\n",
    "  - `hdx_dataset_metadata_dump/dataset_metadata/*.json`\n",
    "- RDLS v0.3 assets:\n",
    "  - `rdls_schema_v0.3.json`\n",
    "  - `rdls_template_v03.json`\n",
    "\n",
    "## Outputs\n",
    "- `hdx_dataset_metadata_dump/rdls/records/*.json` — one RDLS record per included HDX dataset\n",
    "- `hdx_dataset_metadata_dump/rdls/index/rdls_index.jsonl` — index of written records\n",
    "- `hdx_dataset_metadata_dump/rdls/reports/translation_blocked.csv` — datasets blocked by policy/required-field gaps\n",
    "- `hdx_dataset_metadata_dump/rdls/reports/schema_validation.csv` — JSON Schema validation results\n",
    "\n",
    "## Strictness & Policy\n",
    "- **Schema-first:** required RDLS fields are always populated; optional fields omitted unless safely filled\n",
    "- **No extra fields:** output contains only fields defined in the RDLS schema\n",
    "- **No invented content:** missing values → absent optional fields (not empty strings)\n",
    "- **Open codelists:** values may be kept as-is if not in suggestions\n",
    "- **Component combination rule:**\n",
    "  - hazard-only and exposure-only are allowed\n",
    "  - vulnerability must accompany hazard or exposure\n",
    "  - loss must accompany hazard or exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm available: True\n",
      "Configuration:\n",
      "  DUMP_DIR: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump\n",
      "  DATASET_DIR: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/dataset_metadata\n",
      "  OUTPUT_MODE: in_place\n",
      "  RUN_ID: in_place\n",
      "  MAX_DATASETS: None\n",
      "  SKIP_EXISTING: False\n",
      "  CLEANUP_MODE: replace\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup: Import libraries and configure paths.\n",
    "\n",
    "Configuration Options:\n",
    "    MAX_DATASETS: Limit number of datasets to process (None for all, 50 for testing)\n",
    "    OUTPUT_MODE: 'in_place' or 'run_folder' for versioned outputs\n",
    "    SKIP_EXISTING: Resume-safe mode to skip already processed records\n",
    "    WRITE_PRETTY_JSON: Pretty-print JSON for readability\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- tqdm with graceful fallback ---\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "    def tqdm(iterable, **kwargs):\n",
    "        \"\"\"Fallback: return iterable unchanged if tqdm not installed.\"\"\"\n",
    "        return iterable\n",
    "\n",
    "print(f\"tqdm available: {TQDM_AVAILABLE}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranslationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for HDX to RDLS translation.\n",
    "    \n",
    "    Attributes:\n",
    "        dump_dir: Root directory for HDX metadata dump\n",
    "        max_datasets: Limit number of datasets (None for all)\n",
    "        output_mode: 'in_place' or 'run_folder'\n",
    "        clean_before_run: Clean existing outputs before writing (in_place only)\n",
    "        skip_existing: Skip already processed records\n",
    "        write_pretty_json: Pretty-print JSON output\n",
    "        auto_repair_components: Auto-add exposure for standalone V/L\n",
    "    \"\"\"\n",
    "    dump_dir: Path = field(default_factory=lambda: (Path(\"..\") / \"hdx_dataset_metadata_dump\").resolve())\n",
    "    max_datasets: Optional[int] = None  # Set to None for production\n",
    "    output_mode: str = \"in_place\"  # \"in_place\" | \"run_folder\"\n",
    "    clean_before_run: bool = True\n",
    "    skip_existing: bool = False\n",
    "    write_pretty_json: bool = True\n",
    "    auto_repair_components: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        if self.output_mode == \"run_folder\" and self.clean_before_run:\n",
    "            raise ValueError(\"clean_before_run cannot be True when output_mode='run_folder'\")\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "config = TranslationConfig()\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "# Controls what happens to old output files when this notebook is re-run.\n",
    "#   \"replace\" - Auto-delete old outputs and continue (default)\n",
    "#   \"prompt\"  - Show what will be deleted, ask user to confirm\n",
    "#   \"skip\"    - Keep old files, write new on top (may leave orphans)\n",
    "#   \"abort\"   - Stop if old outputs exist (for CI/automated runs)\n",
    "CLEANUP_MODE = \"replace\"\n",
    "\n",
    "# --- Resolve paths ---\n",
    "DUMP_DIR = config.dump_dir\n",
    "DATASET_DIR = (DUMP_DIR / \"dataset_metadata\").resolve()\n",
    "DERIVED_DIR = (DUMP_DIR / \"derived\").resolve()\n",
    "CLASSIFICATION_FINAL_CSV = (DERIVED_DIR / \"classification_final.csv\").resolve()\n",
    "INCLUDED_IDS_TXT = (DERIVED_DIR / \"rdls_included_dataset_ids_final.txt\").resolve()\n",
    "\n",
    "# RDLS assets\n",
    "RDLS_DIR = (DUMP_DIR / \"rdls\").resolve()\n",
    "RDLS_SCHEMA_PATH = (RDLS_DIR / \"schema\" / \"rdls_schema_v0.3.json\").resolve()\n",
    "RDLS_TEMPLATE_PATH = (RDLS_DIR / \"template\" / \"rdls_template_v03.json\").resolve()\n",
    "\n",
    "# Resolve run directory\n",
    "if config.output_mode == \"run_folder\":\n",
    "    RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    RDLS_RUN_DIR = (RDLS_DIR / \"runs\" / RUN_ID).resolve()\n",
    "else:\n",
    "    RUN_ID = \"in_place\"\n",
    "    RDLS_RUN_DIR = RDLS_DIR\n",
    "\n",
    "# Output directories\n",
    "OUT_RECORDS_DIR = RDLS_RUN_DIR / \"records\"\n",
    "OUT_INDEX_DIR = RDLS_RUN_DIR / \"index\"\n",
    "OUT_REPORTS_DIR = RDLS_RUN_DIR / \"reports\"\n",
    "\n",
    "OUT_INDEX_JSONL = OUT_INDEX_DIR / \"rdls_index.jsonl\"\n",
    "OUT_BLOCKED_CSV = OUT_REPORTS_DIR / \"translation_blocked.csv\"\n",
    "OUT_VALIDATION_CSV = OUT_REPORTS_DIR / \"schema_validation.csv\"\n",
    "OUT_QA_CSV = OUT_REPORTS_DIR / \"translation_qa.csv\"\n",
    "\n",
    "# Ensure output folders exist\n",
    "for p in [OUT_RECORDS_DIR, OUT_INDEX_DIR, OUT_REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  DUMP_DIR: {DUMP_DIR}\")\n",
    "print(f\"  DATASET_DIR: {DATASET_DIR}\")\n",
    "print(f\"  OUTPUT_MODE: {config.output_mode}\")\n",
    "print(f\"  RUN_ID: {RUN_ID}\")\n",
    "print(f\"  MAX_DATASETS: {config.max_datasets}\")\n",
    "print(f\"  SKIP_EXISTING: {config.skip_existing}\")\n",
    "print(f\"  CLEANUP_MODE: {CLEANUP_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Mapping Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2-mappings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format mappings loaded: 44 entries (+ 10 skip formats)\n",
      "Hazard keywords loaded: 22 entries\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mapping configurations for HDX to RDLS translation.\n",
    "\n",
    "Includes:\n",
    "    - Hazard filename aliases\n",
    "    - Resource format mapping (HDX -> RDLS)\n",
    "    - License mapping (HDX -> RDLS)\n",
    "    - Hazard keywords for inference\n",
    "\"\"\"\n",
    "\n",
    "# --- Hazard inference / filename alias ---\n",
    "HAZARD_FILENAME_ALIASES: Dict[str, str] = {\n",
    "    \"strong_wind\": \"windstorm\",\n",
    "    # Add more aliases as needed\n",
    "}\n",
    "\n",
    "# --- Resource format mapping (HDX -> RDLS data_format enum) ---\n",
    "# RDLS data_format is a CLOSED codelist with 21 values.\n",
    "# All HDX formats must map to one of these or be skipped.\n",
    "HDX_FORMAT_TO_RDLS: Dict[str, str] = {\n",
    "    # Direct matches\n",
    "    \"CSV\": \"CSV (csv)\",\n",
    "    \"XLS\": \"Excel (xlsx)\",\n",
    "    \"XLSX\": \"Excel (xlsx)\",\n",
    "    \"EXCEL\": \"Excel (xlsx)\",\n",
    "    \"JSON\": \"JSON (json)\",\n",
    "    \"GEOJSON\": \"GeoJSON (geojson)\",\n",
    "    \"SHP\": \"Shapefile (shp)\",\n",
    "    \"SHAPEFILE\": \"Shapefile (shp)\",\n",
    "    \"GPKG\": \"GeoPackage (gpkg)\",\n",
    "    \"GEOPACKAGE\": \"GeoPackage (gpkg)\",\n",
    "    \"KML\": \"KML (kml)\",\n",
    "    \"PDF\": \"PDF (pdf)\",\n",
    "    \"NC\": \"NetCDF (nc)\",\n",
    "    \"NETCDF\": \"NetCDF (nc)\",\n",
    "    \"TIF\": \"GeoTIFF (tif)\",\n",
    "    \"TIFF\": \"GeoTIFF (tif)\",\n",
    "    \"COG\": \"Cloud Optimized GeoTIFF (cog)\",\n",
    "    \"PARQUET\": \"Parquet (parquet)\",\n",
    "    \"APACHE PARQUET\": \"Parquet (parquet)\",\n",
    "    \"XML\": \"XML (xml)\",\n",
    "    # Extended mappings for previously-unmapped HDX formats\n",
    "    \"GEOTIFF\": \"GeoTIFF (tif)\",\n",
    "    \"GEODATABASE\": \"File Geodatabase (gdb)\",\n",
    "    \"GDB\": \"File Geodatabase (gdb)\",\n",
    "    \"TOPOJSON\": \"GeoJSON (geojson)\",          # TopoJSON is a GeoJSON extension\n",
    "    \"MBTILES\": \"GeoPackage (gpkg)\",            # Closest tile-based spatial format\n",
    "    \"KMZ\": \"KML (kml)\",                        # KMZ is compressed KML\n",
    "    \"TSV\": \"CSV (csv)\",                         # Tab-separated is CSV variant\n",
    "    \"TXT\": \"CSV (csv)\",                         # Text tabular data\n",
    "    \"GOOGLE SHEET\": \"Excel (xlsx)\",             # Spreadsheet equivalent\n",
    "    \"ESRI MAP PACKAGE\": \"File Geodatabase (gdb)\",  # Contains geodata\n",
    "    \"GDAL VIRTUAL FORMAT\": \"GeoTIFF (tif)\",    # Virtual raster reference\n",
    "    \"SQL\": \"CSV (csv)\",                         # Tabular data\n",
    "    \"DOCX\": \"PDF (pdf)\",                        # Document format\n",
    "    \"GRIB\": \"GRIB (grib)\",\n",
    "    \"HDF5\": \"HDF5 (hdf5)\",\n",
    "    \"HDF\": \"HDF5 (hdf5)\",\n",
    "    \"ZARR\": \"Zarr (zarr)\",\n",
    "    \"FGB\": \"FlatGeobuf (fgb)\",\n",
    "    \"FLATGEOBUF\": \"FlatGeobuf (fgb)\",\n",
    "    \"LAS\": \"LAS (las)\",\n",
    "    \"LAZ\": \"LAS (las)\",\n",
    "    \"COPC\": \"COPC (copc)\",\n",
    "    \"GRD\": \"GRID (grd)\",\n",
    "    \"GRID\": \"GRID (grd)\",\n",
    "}\n",
    "\n",
    "# Formats to skip entirely (not downloadable data files)\n",
    "HDX_FORMATS_SKIP: set = {\n",
    "    \"HTML\", \"PNG\", \"JPEG\", \"JPG\", \"GIF\", \"EMF\",   # Images/web pages\n",
    "    \"WEB APP\",                                       # Not a data resource\n",
    "    \"RAR\",                                           # Archive container\n",
    "    \"QGIS\", \"ESRI ARCMAP PROJECT FILE\",             # Project files\n",
    "}\n",
    "\n",
    "# Service format -> (data_format, access_modality)\n",
    "# Previously in HDX_FORMATS_SKIP, now handled properly\n",
    "HDX_SERVICE_FORMATS: Dict[str, Tuple[str, str]] = {\n",
    "    'GEOSERVICE': ('GeoJSON (geojson)', 'REST'),\n",
    "    'API':        ('JSON (json)', 'API'),\n",
    "}\n",
    "\n",
    "# --- License mapping (HDX -> RDLS) ---\n",
    "HDX_LICENSE_TO_RDLS: Dict[str, str] = {\n",
    "    \"public domain\": \"PDDL-1.0\",\n",
    "    \"odbl\": \"ODbL-1.0\",\n",
    "    \"cc-by-4.0\": \"CC-BY-4.0\",\n",
    "    \"cc by 4.0\": \"CC-BY-4.0\",\n",
    "    \"cc-by\": \"CC-BY-4.0\",\n",
    "    \"cc0\": \"CC0-1.0\",\n",
    "    \"cc0-1.0\": \"CC0-1.0\",\n",
    "    \"copyright\": \"Copyright\",\n",
    "}\n",
    "\n",
    "# --- Hazard keyword to type mapping ---\n",
    "HAZARD_KEYWORDS_TO_TYPE: Dict[str, str] = {\n",
    "    # Hydro\n",
    "    \"flood\": \"flood\", \"flooding\": \"flood\", \"river flood\": \"flood\",\n",
    "    \"flash flood\": \"flood\", \"inundation\": \"flood\",\n",
    "    # Drought\n",
    "    \"drought\": \"drought\", \"dry spell\": \"drought\", \"water scarcity\": \"drought\",\n",
    "    # Storms / wind\n",
    "    \"cyclone\": \"windstorm\", \"hurricane\": \"windstorm\", \"typhoon\": \"windstorm\",\n",
    "    \"windstorm\": \"windstorm\", \"storm\": \"windstorm\",\n",
    "    # Heat / wildfire\n",
    "    \"heatwave\": \"heat\", \"extreme heat\": \"heat\",\n",
    "    \"wildfire\": \"wildfire\", \"fire\": \"wildfire\",\n",
    "    # Seismic\n",
    "    \"earthquake\": \"earthquake\", \"tsunami\": \"tsunami\",\n",
    "    # Landslide\n",
    "    \"landslide\": \"landslide\", \"mudslide\": \"landslide\", \"avalanche\": \"landslide\",\n",
    "}\n",
    "\n",
    "# --- Risk components mapping (Step 5 -> RDLS enum) ---\n",
    "COMPONENT_MAP: Dict[str, str] = {\n",
    "    \"hazard\": \"hazard\",\n",
    "    \"exposure\": \"exposure\",\n",
    "    \"vulnerability_proxy\": \"vulnerability\",\n",
    "    \"loss_impact\": \"loss\",\n",
    "    \"vulnerability\": \"vulnerability\",\n",
    "    \"loss\": \"loss\",\n",
    "}\n",
    "\n",
    "print(f\"Format mappings loaded: {len(HDX_FORMAT_TO_RDLS)} entries (+ {len(HDX_FORMATS_SKIP)} skip formats)\")\n",
    "print(f\"Hazard keywords loaded: {len(HAZARD_KEYWORDS_TO_TYPE)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Load RDLS Schema and Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDLS required keys: ['id', 'title', 'risk_data_type', 'attributions', 'spatial', 'license', 'resources']\n",
      "RDLS allowed keys count: 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load RDLS schema and template for validation and record building.\n",
    "\n",
    "Raises:\n",
    "    FileNotFoundError: If schema or template files are missing.\n",
    "\"\"\"\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file with UTF-8 encoding.\"\"\"\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "# Validate schema/template existence\n",
    "if not RDLS_SCHEMA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"RDLS schema not found: {RDLS_SCHEMA_PATH}\")\n",
    "\n",
    "if not RDLS_TEMPLATE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"RDLS template not found: {RDLS_TEMPLATE_PATH}\")\n",
    "\n",
    "rdls_schema = safe_load_json(RDLS_SCHEMA_PATH)\n",
    "rdls_template = safe_load_json(RDLS_TEMPLATE_PATH)\n",
    "\n",
    "# RDLS dataset object allowed and required keys\n",
    "RDLS_ALLOWED_KEYS = set(rdls_schema[\"properties\"].keys())\n",
    "RDLS_REQUIRED_KEYS = list(rdls_schema.get(\"required\", []))\n",
    "\n",
    "print(f\"RDLS required keys: {RDLS_REQUIRED_KEYS}\")\n",
    "print(f\"RDLS allowed keys count: {len(RDLS_ALLOWED_KEYS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Load Step 5 Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4-load-inputs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rows: 26,246\n",
      "Included IDs: 13,152\n",
      "Included IDs in classification: 13,152\n",
      "\n",
      "Building dataset file index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c10844010b44606adf2b2febec697b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing files:   0%|          | 0/26246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files indexed: 26,246\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load classification results and build dataset index.\n",
    "\n",
    "Loads:\n",
    "    - classification_final.csv from Step 5\n",
    "    - included dataset IDs list\n",
    "    - Builds dataset_id -> file path index\n",
    "\"\"\"\n",
    "\n",
    "def read_ids_txt(path: Path) -> List[str]:\n",
    "    \"\"\"\n",
    "    Read dataset IDs from text file (one per line).\n",
    "    \n",
    "    Parameters:\n",
    "        path: Path to the IDs file\n",
    "        \n",
    "    Returns:\n",
    "        List of dataset IDs\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing IDs list: {path}\")\n",
    "    return [line.strip() for line in path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "\n",
    "\n",
    "# Validate inputs exist\n",
    "for path, name in [\n",
    "    (CLASSIFICATION_FINAL_CSV, \"Classification CSV\"),\n",
    "    (INCLUDED_IDS_TXT, \"Included IDs list\"),\n",
    "    (DATASET_DIR, \"Dataset metadata folder\"),\n",
    "]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing Step 5 output: {name} at {path}\")\n",
    "\n",
    "# Load classification data\n",
    "df = pd.read_csv(CLASSIFICATION_FINAL_CSV)\n",
    "included_ids = read_ids_txt(INCLUDED_IDS_TXT)\n",
    "\n",
    "# Fast lookups\n",
    "df = df.set_index(\"dataset_id\", drop=False)\n",
    "included_set = set(included_ids)\n",
    "\n",
    "print(f\"Classification rows: {len(df):,}\")\n",
    "print(f\"Included IDs: {len(included_ids):,}\")\n",
    "print(f\"Included IDs in classification: {sum(did in df.index for did in included_ids):,}\")\n",
    "\n",
    "# Build dataset_id -> file path mapping (avoid N glob calls)\n",
    "print(\"\\nBuilding dataset file index...\")\n",
    "dataset_file_index: Dict[str, Path] = {}\n",
    "for fp in tqdm(sorted(DATASET_DIR.glob(\"*.json\")), desc=\"Indexing files\"):\n",
    "    stem = fp.stem\n",
    "    dataset_uuid = stem.split(\"__\", 1)[0] if \"__\" in stem else stem\n",
    "    dataset_file_index[dataset_uuid] = fp\n",
    "\n",
    "print(f\"Dataset files indexed: {len(dataset_file_index):,}\")\n",
    "\n",
    "# Apply MAX_DATASETS limit for testing\n",
    "if config.max_datasets is not None:\n",
    "    included_ids = included_ids[:config.max_datasets]\n",
    "    included_set = set(included_ids)\n",
    "    print(f\"\\nTEST MODE: Limited to {len(included_ids)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5-helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully.\n",
      "  Country fixes: 52 entries\n",
      "  Regional mappings: 17 regions\n",
      "  Country ISO3 table: 436 entries\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper functions for parsing, slugifying, and mapping.\n",
    "\n",
    "Includes:\n",
    "    - Text parsing utilities\n",
    "    - ISO3 country inference (expanded for HDX group names)\n",
    "    - Regional group -> member countries mapping\n",
    "    - Hazard type inference\n",
    "    - License and format mapping\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    \"\"\"Clean text for RDLS JSON: fix encoding, strip HTML, normalize characters.\n",
    "\n",
    "    Handles: mojibake (double-encoded UTF-8), HTML tags/entities,\n",
    "    smart quotes, em/en dashes, non-breaking spaces, zero-width spaces,\n",
    "    control characters, and internal double quotes.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # 1. Fix mojibake (double-encoded UTF-8 via CP1252, ~240 UNESCO files)\n",
    "    try:\n",
    "        clean = text.encode('cp1252').decode('utf-8')\n",
    "    except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "        clean = text\n",
    "\n",
    "    # 2. Strip HTML tags\n",
    "    clean = re.sub(r'<[^>]+>', ' ', clean)\n",
    "\n",
    "    # 3. Decode HTML entities\n",
    "    clean = clean.replace('&nbsp;', ' ').replace('&amp;', '&')\n",
    "    clean = clean.replace('&lt;', '<').replace('&gt;', '>')\n",
    "    clean = clean.replace('&quot;', \"'\").replace('&#39;', \"'\")\n",
    "    clean = re.sub(r'&#(\\d+);', lambda m: chr(int(m.group(1))), clean)\n",
    "    clean = re.sub(r'&#x([0-9a-fA-F]+);', lambda m: chr(int(m.group(1), 16)), clean)\n",
    "\n",
    "    # 4. Normalize Unicode to ASCII-safe equivalents\n",
    "    clean = clean.replace('\\u2018', \"'\").replace('\\u2019', \"'\")\n",
    "    clean = clean.replace('\\u201C', \"'\").replace('\\u201D', \"'\")\n",
    "    clean = clean.replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    clean = clean.replace('\\u2026', '...').replace('\\u00A0', ' ')\n",
    "    clean = clean.replace('\\u200B', '').replace('\\u2022', '-')\n",
    "\n",
    "    # 5. Remove/replace control characters\n",
    "    clean = clean.replace('\\t', ' ').replace('\\r', ' ')\n",
    "\n",
    "    # 6. Replace internal double quotes with single quotes\n",
    "    clean = clean.replace('\"', \"'\")\n",
    "\n",
    "    # 7. Collapse whitespace to single space\n",
    "    clean = re.sub(r'\\s+', ' ', clean)\n",
    "\n",
    "    return clean.strip()\n",
    "\n",
    "\n",
    "def slugify_token(s: str, max_len: int = 32) -> str:\n",
    "    \"\"\"Convert string to URL-safe slug token.\"\"\"\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return (s[:max_len].strip(\"_\") or \"unknown\")\n",
    "\n",
    "\n",
    "def split_semicolon_list(s: Any) -> List[str]:\n",
    "    \"\"\"Split semicolon/comma separated string into list.\"\"\"\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return []\n",
    "    if isinstance(s, list):\n",
    "        return [str(x).strip() for x in s if str(x).strip()]\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [x.strip() for x in re.split(r\"[;,]\", s) if x.strip()]\n",
    "\n",
    "\n",
    "def looks_like_url(s: str) -> bool:\n",
    "    \"\"\"Check if string looks like a URL.\"\"\"\n",
    "    return bool(re.match(r\"^https?://\", (s or \"\").strip(), flags=re.I))\n",
    "\n",
    "\n",
    "# --- ISO3 inference ---\n",
    "def try_import_pycountry():\n",
    "    \"\"\"Try to import pycountry for country code lookups.\"\"\"\n",
    "    try:\n",
    "        import pycountry\n",
    "        return pycountry\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "_pycountry = try_import_pycountry()\n",
    "\n",
    "COMMON_COUNTRY_FIXES: Dict[str, str] = {\n",
    "    # Standard variants\n",
    "    \"cote d'ivoire\": \"CIV\", \"ivory coast\": \"CIV\",\n",
    "    \"democratic republic of the congo\": \"COD\", \"dr congo\": \"COD\",\n",
    "    \"republic of the congo\": \"COG\", \"congo, rep.\": \"COG\", \"congo, dem. rep.\": \"COD\",\n",
    "    \"lao pdr\": \"LAO\", \"viet nam\": \"VNM\",\n",
    "    \"korea, rep.\": \"KOR\", \"korea, dem. rep.\": \"PRK\",\n",
    "    \"syrian arab republic\": \"SYR\", \"iran, islamic republic of\": \"IRN\",\n",
    "    \"tanzania, united republic of\": \"TZA\", \"venezuela, bolivarian republic of\": \"VEN\",\n",
    "    \"bolivia, plurinational state of\": \"BOL\", \"moldova, republic of\": \"MDA\",\n",
    "    \"palestine\": \"PSE\", \"russia\": \"RUS\",\n",
    "    \"united states\": \"USA\", \"united kingdom\": \"GBR\",\n",
    "    # HDX-specific unresolved group names\n",
    "    \"state of palestine\": \"PSE\",\n",
    "    \"republic of korea\": \"KOR\", \"south korea\": \"KOR\",\n",
    "    \"holy see\": \"VAT\",\n",
    "    \"kosovo\": \"XKX\",  # Not in RDLS schema enum; accept validation warning\n",
    "    \"united states virgin islands\": \"VIR\",\n",
    "    \"sint maarten\": \"SXM\",\n",
    "    \"saint martin\": \"MAF\",\n",
    "    \"svalbard and jan mayen islands\": \"SJM\", \"svalbard and jan mayen\": \"SJM\",\n",
    "    \"bonaire, sint eustatius and saba\": \"BES\", \"bonaire; sint eustatius and saba\": \"BES\",\n",
    "    \"wallis and futuna islands\": \"WLF\", \"wallis and futuna\": \"WLF\",\n",
    "    \"french southern and antarctic territories\": \"ATF\", \"french southern territories\": \"ATF\",\n",
    "    \"saint helena\": \"SHN\",\n",
    "    \"eswatini\": \"SWZ\", \"kingdom of eswatini\": \"SWZ\", \"swaziland\": \"SWZ\",\n",
    "    \"north macedonia\": \"MKD\",\n",
    "    \"timor-leste\": \"TLS\", \"east timor\": \"TLS\",\n",
    "    \"cabo verde\": \"CPV\", \"cape verde\": \"CPV\",\n",
    "    \"micronesia\": \"FSM\", \"micronesia (federated states of)\": \"FSM\",\n",
    "    \"brunei\": \"BRN\", \"brunei darussalam\": \"BRN\",\n",
    "    \"turkiye\": \"TUR\",\n",
    "    \"czechia\": \"CZE\",\n",
    "}\n",
    "\n",
    "# HDX group names that are NOT countries (skip without warning)\n",
    "NON_COUNTRY_GROUPS: set = {\n",
    "    \"world\", \"global\",\n",
    "    \"nepal earthquake\",  # Event name, not a country\n",
    "}\n",
    "\n",
    "# Regional group name -> list of member ISO3 codes\n",
    "REGION_TO_COUNTRIES: Dict[str, List[str]] = {\n",
    "    \"africa\": [\n",
    "        \"DZA\", \"AGO\", \"BEN\", \"BWA\", \"BFA\", \"BDI\", \"CPV\", \"CMR\", \"CAF\", \"TCD\",\n",
    "        \"COM\", \"COG\", \"COD\", \"CIV\", \"DJI\", \"EGY\", \"GNQ\", \"ERI\", \"SWZ\", \"ETH\",\n",
    "        \"GAB\", \"GMB\", \"GHA\", \"GIN\", \"GNB\", \"KEN\", \"LSO\", \"LBR\", \"LBY\", \"MDG\",\n",
    "        \"MWI\", \"MLI\", \"MRT\", \"MUS\", \"MAR\", \"MOZ\", \"NAM\", \"NER\", \"NGA\", \"RWA\",\n",
    "        \"STP\", \"SEN\", \"SYC\", \"SLE\", \"SOM\", \"ZAF\", \"SSD\", \"SDN\", \"TZA\", \"TGO\",\n",
    "        \"TUN\", \"UGA\", \"ZMB\", \"ZWE\",\n",
    "    ],\n",
    "    \"east africa\": [\n",
    "        \"BDI\", \"COM\", \"DJI\", \"ERI\", \"ETH\", \"KEN\", \"MDG\", \"MWI\", \"MUS\", \"MOZ\",\n",
    "        \"RWA\", \"SYC\", \"SOM\", \"SSD\", \"TZA\", \"UGA\", \"ZMB\", \"ZWE\",\n",
    "    ],\n",
    "    \"west africa\": [\n",
    "        \"BEN\", \"BFA\", \"CPV\", \"CIV\", \"GMB\", \"GHA\", \"GIN\", \"GNB\", \"LBR\", \"MLI\",\n",
    "        \"MRT\", \"NER\", \"NGA\", \"SEN\", \"SLE\", \"TGO\",\n",
    "    ],\n",
    "    \"southern africa\": [\n",
    "        \"BWA\", \"SWZ\", \"LSO\", \"MOZ\", \"NAM\", \"ZAF\", \"ZMB\", \"ZWE\",\n",
    "    ],\n",
    "    \"asia\": [\n",
    "        \"AFG\", \"BGD\", \"BTN\", \"BRN\", \"KHM\", \"CHN\", \"IND\", \"IDN\", \"IRN\", \"IRQ\",\n",
    "        \"ISR\", \"JPN\", \"JOR\", \"KAZ\", \"KWT\", \"KGZ\", \"LAO\", \"LBN\", \"MYS\", \"MDV\",\n",
    "        \"MNG\", \"MMR\", \"NPL\", \"PRK\", \"OMN\", \"PAK\", \"PSE\", \"PHL\", \"QAT\", \"SAU\",\n",
    "        \"SGP\", \"KOR\", \"LKA\", \"SYR\", \"TWN\", \"TJK\", \"THA\", \"TLS\", \"TUR\", \"TKM\",\n",
    "        \"ARE\", \"UZB\", \"VNM\", \"YEM\",\n",
    "    ],\n",
    "    \"southeast asia\": [\n",
    "        \"BRN\", \"KHM\", \"IDN\", \"LAO\", \"MYS\", \"MMR\", \"PHL\", \"SGP\", \"THA\", \"TLS\", \"VNM\",\n",
    "    ],\n",
    "    \"south asia\": [\n",
    "        \"AFG\", \"BGD\", \"BTN\", \"IND\", \"MDV\", \"NPL\", \"PAK\", \"LKA\",\n",
    "    ],\n",
    "    \"central asia\": [\n",
    "        \"KAZ\", \"KGZ\", \"TJK\", \"TKM\", \"UZB\",\n",
    "    ],\n",
    "    \"europe\": [\n",
    "        \"ALB\", \"AND\", \"AUT\", \"BLR\", \"BEL\", \"BIH\", \"BGR\", \"HRV\", \"CYP\", \"CZE\",\n",
    "        \"DNK\", \"EST\", \"FIN\", \"FRA\", \"DEU\", \"GRC\", \"HUN\", \"ISL\", \"IRL\", \"ITA\",\n",
    "        \"LVA\", \"LIE\", \"LTU\", \"LUX\", \"MLT\", \"MDA\", \"MCO\", \"MNE\", \"NLD\", \"MKD\",\n",
    "        \"NOR\", \"POL\", \"PRT\", \"ROU\", \"RUS\", \"SMR\", \"SRB\", \"SVK\", \"SVN\", \"ESP\",\n",
    "        \"SWE\", \"CHE\", \"UKR\", \"GBR\", \"VAT\",\n",
    "    ],\n",
    "    \"middle east\": [\n",
    "        \"BHR\", \"EGY\", \"IRN\", \"IRQ\", \"ISR\", \"JOR\", \"KWT\", \"LBN\", \"OMN\", \"PSE\",\n",
    "        \"QAT\", \"SAU\", \"SYR\", \"TUR\", \"ARE\", \"YEM\",\n",
    "    ],\n",
    "    \"americas\": [\n",
    "        \"ATG\", \"ARG\", \"BHS\", \"BRB\", \"BLZ\", \"BOL\", \"BRA\", \"CAN\", \"CHL\", \"COL\",\n",
    "        \"CRI\", \"CUB\", \"DMA\", \"DOM\", \"ECU\", \"SLV\", \"GRD\", \"GTM\", \"GUY\", \"HTI\",\n",
    "        \"HND\", \"JAM\", \"MEX\", \"NIC\", \"PAN\", \"PRY\", \"PER\", \"KNA\", \"LCA\", \"VCT\",\n",
    "        \"SUR\", \"TTO\", \"URY\", \"USA\", \"VEN\",\n",
    "    ],\n",
    "    \"latin america\": [\n",
    "        \"ARG\", \"BOL\", \"BRA\", \"CHL\", \"COL\", \"CRI\", \"CUB\", \"DOM\", \"ECU\", \"SLV\",\n",
    "        \"GTM\", \"HTI\", \"HND\", \"MEX\", \"NIC\", \"PAN\", \"PRY\", \"PER\", \"URY\", \"VEN\",\n",
    "    ],\n",
    "    \"latin america and the caribbean\": [\n",
    "        \"ATG\", \"ARG\", \"BHS\", \"BRB\", \"BLZ\", \"BOL\", \"BRA\", \"CHL\", \"COL\", \"CRI\",\n",
    "        \"CUB\", \"DMA\", \"DOM\", \"ECU\", \"SLV\", \"GRD\", \"GTM\", \"GUY\", \"HTI\", \"HND\",\n",
    "        \"JAM\", \"MEX\", \"NIC\", \"PAN\", \"PRY\", \"PER\", \"KNA\", \"LCA\", \"VCT\", \"SUR\",\n",
    "        \"TTO\", \"URY\", \"VEN\",\n",
    "    ],\n",
    "    \"caribbean\": [\n",
    "        \"ATG\", \"BHS\", \"BRB\", \"CUB\", \"DMA\", \"DOM\", \"GRD\", \"HTI\", \"JAM\", \"KNA\",\n",
    "        \"LCA\", \"VCT\", \"TTO\",\n",
    "    ],\n",
    "    \"central america\": [\n",
    "        \"BLZ\", \"CRI\", \"SLV\", \"GTM\", \"HND\", \"NIC\", \"PAN\",\n",
    "    ],\n",
    "    \"oceania\": [\n",
    "        \"AUS\", \"FJI\", \"KIR\", \"MHL\", \"FSM\", \"NRU\", \"NZL\", \"PLW\", \"PNG\", \"WSM\",\n",
    "        \"SLB\", \"TON\", \"TUV\", \"VUT\",\n",
    "    ],\n",
    "    \"pacific\": [\n",
    "        \"FJI\", \"KIR\", \"MHL\", \"FSM\", \"NRU\", \"PLW\", \"PNG\", \"WSM\", \"SLB\", \"TON\",\n",
    "        \"TUV\", \"VUT\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Normalize region keys for lookup\n",
    "REGION_TO_COUNTRIES_NORM = {_norm_key.lower().strip(): v for _norm_key, v in REGION_TO_COUNTRIES.items()}\n",
    "\n",
    "\n",
    "def _norm_country_key(s: str) -> str:\n",
    "    \"\"\"Normalize country name for lookup.\"\"\"\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[\\(\\)\\[\\]\\{\\}\\.\\,\\;\\:]\", \" \", s)\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-']\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "COMMON_COUNTRY_FIXES_NORM = {_norm_country_key(k): v for k, v in COMMON_COUNTRY_FIXES.items()}\n",
    "\n",
    "# Load country ISO3 table if available\n",
    "COUNTRY_ISO3_CSV = (DUMP_DIR / \"config\" / \"country_name_to_iso3.csv\")\n",
    "\n",
    "def load_country_iso3_table(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load country name to ISO3 mapping from CSV.\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        df_iso = pd.read_csv(path)\n",
    "        cols = {c.lower(): c for c in df_iso.columns}\n",
    "        name_col = cols.get(\"name\") or cols.get(\"country\") or cols.get(\"country_name\")\n",
    "        iso3_col = cols.get(\"iso3\") or cols.get(\"alpha_3\") or cols.get(\"code\")\n",
    "        if not name_col or not iso3_col:\n",
    "            return {}\n",
    "        return {\n",
    "            _norm_country_key(str(r.get(name_col, \"\"))): str(r.get(iso3_col, \"\")).strip().upper()\n",
    "            for _, r in df_iso.iterrows()\n",
    "            if str(r.get(name_col, \"\")).strip() and len(str(r.get(iso3_col, \"\")).strip()) == 3\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "COUNTRY_ISO3_TABLE = load_country_iso3_table(COUNTRY_ISO3_CSV)\n",
    "\n",
    "\n",
    "def country_name_to_iso3(name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Convert country name to ISO3 code.\n",
    "    \n",
    "    Parameters:\n",
    "        name: Country name or ISO3 code\n",
    "        \n",
    "    Returns:\n",
    "        ISO3 code or None if not found\n",
    "    \"\"\"\n",
    "    n = (name or \"\").strip()\n",
    "    if not n:\n",
    "        return None\n",
    "    \n",
    "    # Already ISO3?\n",
    "    if len(n) == 3 and n.isalpha():\n",
    "        return n.upper()\n",
    "    \n",
    "    key = _norm_country_key(n)\n",
    "    \n",
    "    # Check fixes first\n",
    "    if key in COMMON_COUNTRY_FIXES_NORM:\n",
    "        return COMMON_COUNTRY_FIXES_NORM[key]\n",
    "    \n",
    "    # Check table\n",
    "    if key in COUNTRY_ISO3_TABLE:\n",
    "        return COUNTRY_ISO3_TABLE[key]\n",
    "    \n",
    "    # Try pycountry\n",
    "    if _pycountry is not None:\n",
    "        try:\n",
    "            c = _pycountry.countries.lookup(n)\n",
    "            return getattr(c, \"alpha_3\", None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_spatial(groups: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Infer RDLS spatial block from HDX country groups.\n",
    "    \n",
    "    Logic:\n",
    "      - \"World\"/\"global\" -> scale: \"global\", no countries\n",
    "      - Regional name (Africa, Europe, etc.) -> scale: \"regional\", countries: [members]\n",
    "      - Single country -> scale: \"national\", countries: [iso3]\n",
    "      - Multiple countries -> scale: \"regional\", countries: [iso3s]\n",
    "      - Non-country names (events) -> silently skip\n",
    "      - Completely unresolvable -> scale: \"global\"\n",
    "    \"\"\"\n",
    "    if not groups:\n",
    "        return {\"scale\": \"global\"}\n",
    "    \n",
    "    # Normalize group names for matching\n",
    "    norm_groups = [g.strip().lower() for g in groups]\n",
    "    \n",
    "    # Check for \"World\" / \"global\" -> global scale\n",
    "    if any(g in NON_COUNTRY_GROUPS for g in norm_groups):\n",
    "        # Filter out non-country groups, keep any actual countries\n",
    "        country_groups = [g for g, ng in zip(groups, norm_groups) if ng not in NON_COUNTRY_GROUPS]\n",
    "        if not country_groups:\n",
    "            return {\"scale\": \"global\"}\n",
    "        groups = country_groups\n",
    "        norm_groups = [g.strip().lower() for g in groups]\n",
    "    \n",
    "    # Check for regional group names\n",
    "    all_iso3s = []\n",
    "    remaining_groups = []\n",
    "    is_regional = False\n",
    "    \n",
    "    for g, ng in zip(groups, norm_groups):\n",
    "        if ng in REGION_TO_COUNTRIES_NORM:\n",
    "            all_iso3s.extend(REGION_TO_COUNTRIES_NORM[ng])\n",
    "            is_regional = True\n",
    "        else:\n",
    "            remaining_groups.append(g)\n",
    "    \n",
    "    # Resolve remaining groups as country names\n",
    "    for g in remaining_groups:\n",
    "        iso3 = country_name_to_iso3(g)\n",
    "        if iso3:\n",
    "            all_iso3s.append(iso3)\n",
    "    \n",
    "    # Deduplicate and sort\n",
    "    iso3s = sorted(set(all_iso3s))\n",
    "    \n",
    "    if is_regional and iso3s:\n",
    "        return {\"scale\": \"regional\", \"countries\": iso3s}\n",
    "    if len(iso3s) == 1:\n",
    "        return {\"scale\": \"national\", \"countries\": iso3s}\n",
    "    if len(iso3s) > 1:\n",
    "        return {\"scale\": \"regional\", \"countries\": iso3s}\n",
    "    \n",
    "    # Nothing resolved — default to global without noisy warning\n",
    "    return {\"scale\": \"global\"}\n",
    "\n",
    "\n",
    "def infer_hazard_types(tags: List[str], title: str = \"\", notes: str = \"\") -> List[str]:\n",
    "    \"\"\"Infer hazard types from tags and text content.\"\"\"\n",
    "    text = \" \".join([*tags, title or \"\", notes or \"\"]).lower()\n",
    "    hits = set()\n",
    "    for k, ht in HAZARD_KEYWORDS_TO_TYPE.items():\n",
    "        if k in text:\n",
    "            hits.add(ht)\n",
    "    return sorted(hits)\n",
    "\n",
    "\n",
    "def hazard_suffix_for_filename(hazard_types: List[str]) -> str:\n",
    "    \"\"\"Generate hazard suffix for filename.\"\"\"\n",
    "    if not hazard_types:\n",
    "        return \"\"\n",
    "    if len(hazard_types) > 1:\n",
    "        return \"_multihazard\"\n",
    "    ht = hazard_types[0]\n",
    "    return \"_\" + slugify_token(HAZARD_FILENAME_ALIASES.get(ht, ht), max_len=24)\n",
    "\n",
    "\n",
    "def parse_components(s: Any) -> List[str]:\n",
    "    \"\"\"Parse risk components from semicolon-separated string.\"\"\"\n",
    "    parts = split_semicolon_list(s)\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for p in parts:\n",
    "        mapped = COMPONENT_MAP.get(p.strip().lower())\n",
    "        if mapped and mapped not in seen:\n",
    "            result.append(mapped)\n",
    "            seen.add(mapped)\n",
    "    return result\n",
    "\n",
    "\n",
    "def choose_prefix(risk_data_type: List[str]) -> str:\n",
    "    \"\"\"Choose RDLS filename prefix based on risk data type.\"\"\"\n",
    "    rset = set(risk_data_type)\n",
    "    if \"loss\" in rset:\n",
    "        return \"rdls_lss-\"\n",
    "    if \"vulnerability\" in rset:\n",
    "        return \"rdls_vln-\"\n",
    "    if \"exposure\" in rset:\n",
    "        return \"rdls_exp-\"\n",
    "    return \"rdls_hzd-\"\n",
    "\n",
    "\n",
    "def map_license(hdx_license_title: str) -> str:\n",
    "    \"\"\"\n",
    "    Map HDX license strings to RDLS schema suggestions.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx_license_title: License title from HDX\n",
    "        \n",
    "    Returns:\n",
    "        RDLS-compatible license identifier\n",
    "    \"\"\"\n",
    "    raw = (hdx_license_title or \"\").strip()\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    \n",
    "    key = re.sub(r\"\\s+\", \" \", raw.lower().strip())\n",
    "    \n",
    "    # Pattern mappings\n",
    "    if re.search(r\"\\bcc0\\b\", key) or (\"public domain\" in key and \"cc0\" in key):\n",
    "        return \"CC0-1.0\"\n",
    "    if \"odbl\" in key or \"open database license\" in key:\n",
    "        return \"ODbL-1.0\"\n",
    "    if \"pddl\" in key or \"public domain dedication\" in key:\n",
    "        return \"PDDL-1.0\"\n",
    "    \n",
    "    # Creative Commons variants\n",
    "    k2 = key.replace(\"creative commons\", \"cc\")\n",
    "    \n",
    "    if re.search(r\"\\bcc\\s*by\\b\", k2) and \"sa\" not in k2 and \"nd\" not in k2 and \"nc\" not in k2:\n",
    "        return \"CC-BY-4.0\" if \"4.0\" in k2 or \"v4\" in k2 else (\"CC-BY-3.0\" if \"3.0\" in k2 else \"CC-BY-4.0\")\n",
    "    if \"by-sa\" in k2 or re.search(r\"\\bcc\\s*by\\s*sa\\b\", k2):\n",
    "        return \"CC-BY-SA-4.0\" if \"4.0\" in k2 else (\"CC-BY-SA-3.0\" if \"3.0\" in k2 else \"CC-BY-SA-4.0\")\n",
    "    if \"by-nc\" in k2 or re.search(r\"\\bcc\\s*by\\s*nc\\b\", k2):\n",
    "        return \"CC-BY-NC-4.0\" if \"4.0\" in k2 else (\"CC-BY-NC-3.0\" if \"3.0\" in k2 else \"CC-BY-NC-4.0\")\n",
    "    \n",
    "    return HDX_LICENSE_TO_RDLS.get(re.sub(r\"\\s+\", \" \", k2).strip(), raw)\n",
    "\n",
    "\n",
    "# Track unmapped HDX formats for reporting\n",
    "unmapped_hdx_formats: set = set()\n",
    "\n",
    "\n",
    "def _infer_format_from_name(name: str, url: str = \"\") -> Optional[str]:\n",
    "    \"\"\"Infer RDLS data_format from filename keywords when HDX format is ZIP or unknown.\"\"\"\n",
    "    text = f\"{name} {url}\".lower()\n",
    "    # Ordered: more specific patterns first to avoid false matches\n",
    "    HINTS = [\n",
    "        ('geotiff', 'GeoTIFF (tif)'),    ('geotif', 'GeoTIFF (tif)'),\n",
    "        ('.tif', 'GeoTIFF (tif)'),        ('shapefile', 'Shapefile (shp)'),\n",
    "        ('.shp', 'Shapefile (shp)'),      ('geopackage', 'GeoPackage (gpkg)'),\n",
    "        ('.gpkg', 'GeoPackage (gpkg)'),   ('geodatabase', 'File Geodatabase (gdb)'),\n",
    "        ('.gdb', 'File Geodatabase (gdb)'),\n",
    "        ('geojson', 'GeoJSON (geojson)'), ('.geojson', 'GeoJSON (geojson)'),\n",
    "        ('flatgeobuf', 'FlatGeobuf (fgb)'),\n",
    "        ('netcdf', 'NetCDF (nc)'),        ('.nc.', 'NetCDF (nc)'),\n",
    "        ('parquet', 'Parquet (parquet)'),\n",
    "        ('_csv', 'CSV (csv)'),            ('.csv', 'CSV (csv)'),\n",
    "        ('excel', 'Excel (xlsx)'),        ('_xlsx', 'Excel (xlsx)'),\n",
    "        ('.xlsx', 'Excel (xlsx)'),        ('.xls', 'Excel (xlsx)'),\n",
    "        ('_json', 'JSON (json)'),         ('.json', 'JSON (json)'),\n",
    "        ('.xml', 'XML (xml)'),            ('.kml', 'KML (kml)'),\n",
    "        ('.pdf', 'PDF (pdf)'),\n",
    "    ]\n",
    "    for hint, rdls_fmt in HINTS:\n",
    "        if hint in text:\n",
    "            return rdls_fmt\n",
    "    return None\n",
    "\n",
    "\n",
    "# Service URL patterns: (regex, data_format, access_modality)\n",
    "SERVICE_URL_PATTERNS = [\n",
    "    (r'arcgis\\.com/.*/rest/services/', 'GeoJSON (geojson)', 'REST'),\n",
    "    (r'/rest/services/.*(?:Feature|Map)Server', 'GeoJSON (geojson)', 'REST'),\n",
    "    (r'geoserver.*(?:/wms|/wfs|/ows)', 'GeoJSON (geojson)', 'WFS'),\n",
    "    (r'/wms\\b', 'XML (xml)', 'WMS'),\n",
    "    (r'/wfs\\b', 'GeoJSON (geojson)', 'WFS'),\n",
    "    (r'/wcs\\b', 'GeoTIFF (tif)', 'WCS'),\n",
    "]\n",
    "\n",
    "\n",
    "def detect_service_url(url: str) -> Optional[Tuple[str, str]]:\n",
    "    \"\"\"Detect service URLs and return (data_format, access_modality) or None.\"\"\"\n",
    "    if not url:\n",
    "        return None\n",
    "    for pattern, fmt, modality in SERVICE_URL_PATTERNS:\n",
    "        if re.search(pattern, url, re.IGNORECASE):\n",
    "            return (fmt, modality)\n",
    "    return None\n",
    "\n",
    "\n",
    "def map_data_format(hdx_fmt: str, url: str = \"\", name: str = \"\") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Map HDX format to RDLS data_format enum value.\n",
    "    \n",
    "    Returns None for formats that should be skipped (non-data formats).\n",
    "    Handles ZIP files by inferring format from filename.\n",
    "    \"\"\"\n",
    "    s = (hdx_fmt or \"\").strip().upper()\n",
    "    \n",
    "    # Check skip list first (non-data formats)\n",
    "    if s in HDX_FORMATS_SKIP:\n",
    "        return None\n",
    "    \n",
    "    # Direct dictionary lookup\n",
    "    if s in HDX_FORMAT_TO_RDLS:\n",
    "        return HDX_FORMAT_TO_RDLS[s]\n",
    "    \n",
    "    # ZIP/archive: infer format from filename or URL\n",
    "    if s in ('ZIP', '7Z', 'TAR', 'GZ', 'GZIP'):\n",
    "        return _infer_format_from_name(name, url)\n",
    "    \n",
    "    # Guess from URL extension\n",
    "    u = (url or \"\").lower().split(\"?\")[0]  # Strip query params\n",
    "    ext_map = [\n",
    "        (\".geojson\", \"GeoJSON (geojson)\"), (\".json\", \"JSON (json)\"),\n",
    "        (\".csv\", \"CSV (csv)\"), (\".xlsx\", \"Excel (xlsx)\"), (\".xls\", \"Excel (xlsx)\"),\n",
    "        (\".shp\", \"Shapefile (shp)\"),\n",
    "        (\".tif\", \"GeoTIFF (tif)\"), (\".tiff\", \"GeoTIFF (tif)\"),\n",
    "        (\".nc\", \"NetCDF (nc)\"), (\".pdf\", \"PDF (pdf)\"),\n",
    "        (\".parquet\", \"Parquet (parquet)\"), (\".gpkg\", \"GeoPackage (gpkg)\"),\n",
    "        (\".kml\", \"KML (kml)\"), (\".kmz\", \"KML (kml)\"),\n",
    "        (\".xml\", \"XML (xml)\"), (\".gdb\", \"File Geodatabase (gdb)\"),\n",
    "    ]\n",
    "    for ext, rdls in ext_map:\n",
    "        if u.endswith(ext):\n",
    "            return rdls\n",
    "    \n",
    "    # Last resort: try inferring from filename for unknown formats\n",
    "    inferred = _infer_format_from_name(name, url)\n",
    "    if inferred:\n",
    "        return inferred\n",
    "\n",
    "    # Truly unmapped — skip resource (return None)\n",
    "    if s:\n",
    "        unmapped_hdx_formats.add(s.lower())\n",
    "    return None\n",
    "\n",
    "\n",
    "print(f\"Helper functions loaded successfully.\")\n",
    "print(f\"  Country fixes: {len(COMMON_COUNTRY_FIXES)} entries\")\n",
    "print(f\"  Regional mappings: {len(REGION_TO_COUNTRIES)} regions\")\n",
    "print(f\"  Country ISO3 table: {len(COUNTRY_ISO3_TABLE)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Component Gating Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6-gating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component gating configured (auto_repair=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Component gating logic for RDLS risk_data_type validation.\n",
    "\n",
    "Rules:\n",
    "    - vulnerability must co-occur with hazard or exposure\n",
    "    - loss must co-occur with hazard or exposure\n",
    "    - risk_data_type must be non-empty\n",
    "\"\"\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ComponentGateResult:\n",
    "    \"\"\"\n",
    "    Result of component gating validation.\n",
    "    \n",
    "    Attributes:\n",
    "        ok: Whether validation passed\n",
    "        reasons: Tuple of reason codes\n",
    "        risk_data_type: Validated/repaired risk data type list\n",
    "    \"\"\"\n",
    "    ok: bool\n",
    "    reasons: Tuple[str, ...]\n",
    "    risk_data_type: List[str]\n",
    "\n",
    "\n",
    "def apply_component_gate(components: List[str]) -> ComponentGateResult:\n",
    "    \"\"\"\n",
    "    Enforce RDLS component combination rules.\n",
    "    \n",
    "    Parameters:\n",
    "        components: List of risk components\n",
    "        \n",
    "    Returns:\n",
    "        ComponentGateResult with validation status\n",
    "    \"\"\"\n",
    "    allowed = {\"hazard\", \"exposure\", \"vulnerability\", \"loss\"}\n",
    "    rset = {c for c in (components or []) if c in allowed}\n",
    "    \n",
    "    if not rset:\n",
    "        return ComponentGateResult(\n",
    "            ok=False,\n",
    "            reasons=(\"empty_or_unrecognized_components\",),\n",
    "            risk_data_type=[],\n",
    "        )\n",
    "    \n",
    "    reasons: List[str] = []\n",
    "    ok = True\n",
    "    \n",
    "    # Vulnerability requires hazard or exposure\n",
    "    if \"vulnerability\" in rset and not ({\"hazard\", \"exposure\"} & rset):\n",
    "        if config.auto_repair_components:\n",
    "            rset.add(\"exposure\")\n",
    "            reasons.append(\"auto_added_exposure_for_vulnerability\")\n",
    "        else:\n",
    "            ok = False\n",
    "            reasons.append(\"vulnerability_without_hazard_or_exposure\")\n",
    "    \n",
    "    # Loss requires hazard or exposure\n",
    "    if \"loss\" in rset and not ({\"hazard\", \"exposure\"} & rset):\n",
    "        if config.auto_repair_components:\n",
    "            rset.add(\"exposure\")\n",
    "            reasons.append(\"auto_added_exposure_for_loss\")\n",
    "        else:\n",
    "            ok = False\n",
    "            reasons.append(\"loss_without_hazard_or_exposure\")\n",
    "    \n",
    "    return ComponentGateResult(ok=ok, reasons=tuple(reasons), risk_data_type=sorted(rset))\n",
    "\n",
    "\n",
    "print(f\"Component gating configured (auto_repair={config.auto_repair_components})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. RDLS Record Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record builder functions loaded.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build RDLS dataset records from HDX metadata.\n",
    "\n",
    "Creates minimal, schema-safe records with:\n",
    "    - Required attributions (publisher, creator, contact)\n",
    "    - Resources with proper format mapping\n",
    "    - Spatial information inferred from groups\n",
    "\"\"\"\n",
    "\n",
    "def build_attributions(hdx: Dict[str, Any], dataset_id: str, dataset_page_url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS attributions from HDX metadata.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx: HDX dataset metadata\n",
    "        dataset_id: Dataset UUID\n",
    "        dataset_page_url: HDX dataset landing page URL\n",
    "        \n",
    "    Returns:\n",
    "        List of attribution objects (minItems=3)\n",
    "    \"\"\"\n",
    "    org = (hdx.get(\"organization\") or \"\").strip() or \"Not specified\"\n",
    "    src = (hdx.get(\"dataset_source\") or \"\").strip() or org\n",
    "    creator_url = src if looks_like_url(src) else dataset_page_url\n",
    "    \n",
    "    return [\n",
    "        {\"id\": \"attribution_publisher\", \"role\": \"publisher\", \"entity\": {\"name\": org, \"url\": dataset_page_url}},\n",
    "        {\"id\": \"attribution_creator\", \"role\": \"creator\", \"entity\": {\"name\": src, \"url\": creator_url}},\n",
    "        {\"id\": \"attribution_contact\", \"role\": \"contact_point\", \"entity\": {\"name\": org, \"url\": dataset_page_url}},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_resources(hdx: Dict[str, Any], dataset_id: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS resources from HDX resources.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx: HDX dataset metadata\n",
    "        dataset_id: Dataset UUID\n",
    "        \n",
    "    Returns:\n",
    "        List of resource objects (minItems=1)\n",
    "    \"\"\"\n",
    "    # Always include HDX metadata export\n",
    "    meta_url = f\"https://data.humdata.org/dataset/{dataset_id}/download_metadata?format=json\"\n",
    "    resources = [{\n",
    "        \"id\": \"hdx_dataset_metadata_json\",\n",
    "        \"title\": \"HDX dataset metadata (JSON)\",\n",
    "        \"description\": \"Dataset-level metadata exported from HDX.\",\n",
    "        \"data_format\": \"JSON (json)\",\n",
    "        \"access_modality\": \"file_download\",\n",
    "        \"download_url\": meta_url,\n",
    "    }]\n",
    "    \n",
    "    for r in hdx.get(\"resources\", []) or []:\n",
    "        rid = (r.get(\"id\") or \"\").strip()\n",
    "        rname = sanitize_text((r.get(\"name\") or \"\").strip()) or rid[:8] or \"resource\"\n",
    "        desc = sanitize_text((r.get(\"description\") or \"\").strip()) or f\"HDX resource: {rname}\"\n",
    "        dl = (r.get(\"download_url\") or \"\").strip()\n",
    "        hdx_format = (r.get(\"format\") or \"\").strip().upper()\n",
    "\n",
    "        # Determine data_format and access_modality\n",
    "        fmt = None\n",
    "        access_modality = \"file_download\"\n",
    "\n",
    "        # Check if it is a known service format (GEOSERVICE, API)\n",
    "        if hdx_format in HDX_SERVICE_FORMATS:\n",
    "            fmt, access_modality = HDX_SERVICE_FORMATS[hdx_format]\n",
    "            # Refine using URL patterns (e.g., ArcGIS REST)\n",
    "            svc = detect_service_url(dl)\n",
    "            if svc:\n",
    "                fmt, access_modality = svc\n",
    "        else:\n",
    "            fmt = map_data_format(hdx_format, dl, rname)\n",
    "            # Check if download URL is actually a service endpoint\n",
    "            svc = detect_service_url(dl)\n",
    "            if svc:\n",
    "                _, access_modality = svc\n",
    "\n",
    "        if not dl or not fmt:\n",
    "            continue\n",
    "\n",
    "        resources.append({\n",
    "            \"id\": f\"hdx_res_{rid[:8] or slugify_token(rname, 8)}\",\n",
    "            \"title\": rname,\n",
    "            \"description\": desc,\n",
    "            \"data_format\": fmt,\n",
    "            \"access_modality\": access_modality,\n",
    "            \"download_url\": dl,\n",
    "        })\n",
    "    \n",
    "    # Deduplicate by id\n",
    "    seen = set()\n",
    "    return [r for r in resources if not (r[\"id\"] in seen or seen.add(r[\"id\"]))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_details(hdx: Dict[str, Any]) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Build RDLS 'details' field from HDX metadata fields that have\n",
    "    no direct RDLS equivalent.\n",
    "\n",
    "    Composites: caveats, methodology, methodology_other, dataset_date,\n",
    "    data_update_frequency, last_modified.\n",
    "\n",
    "    Returns None if no meaningful content is available.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "\n",
    "    # Caveats (data quality / limitations)\n",
    "    caveats = sanitize_text((hdx.get('caveats') or '').strip())\n",
    "    if caveats:\n",
    "        parts.append(f\"Caveats: {caveats}\")\n",
    "\n",
    "    # Methodology\n",
    "    methodology = sanitize_text((hdx.get('methodology') or '').strip())\n",
    "    methodology_other = sanitize_text((hdx.get('methodology_other') or '').strip())\n",
    "    if methodology_other:\n",
    "        parts.append(f\"Methodology: {methodology_other}\")\n",
    "    elif methodology and methodology.lower() not in ('other', ''):\n",
    "        parts.append(f\"Methodology: {methodology}\")\n",
    "\n",
    "    # Temporal coverage\n",
    "    dataset_date = (hdx.get('dataset_date') or '').strip()\n",
    "    if dataset_date:\n",
    "        parts.append(f\"Temporal coverage: {dataset_date}\")\n",
    "\n",
    "    # Update frequency\n",
    "    frequency = (hdx.get('data_update_frequency') or '').strip()\n",
    "    if frequency:\n",
    "        parts.append(f\"Update frequency: {frequency}\")\n",
    "\n",
    "    # Last modified\n",
    "    last_modified = (hdx.get('last_modified') or hdx.get('metadata_modified') or '').strip()\n",
    "    if last_modified:\n",
    "        # Trim microseconds for readability: 2025-11-18T16:54:54.268514 -> 2025-11-18\n",
    "        date_part = last_modified[:10] if len(last_modified) >= 10 else last_modified\n",
    "        parts.append(f\"Last modified: {date_part}\")\n",
    "\n",
    "    if not parts:\n",
    "        return None\n",
    "\n",
    "    return ' | '.join(parts)\n",
    "\n",
    "\n",
    "def build_rdls_record(\n",
    "    hdx: Dict[str, Any],\n",
    "    class_row: pd.Series,\n",
    ") -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS record from HDX metadata and classification.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx: HDX dataset metadata\n",
    "        class_row: Classification row from Step 5\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (rdls_record or None if blocked, info dict)\n",
    "    \"\"\"\n",
    "    dataset_id = str(class_row[\"dataset_id\"])\n",
    "    title = sanitize_text((hdx.get(\"title\") or class_row.get(\"title\") or \"\").strip())\n",
    "    notes = sanitize_text((hdx.get(\"notes\") or \"\").strip())\n",
    "    details = build_details(hdx)\n",
    "    \n",
    "    # Parse and validate components\n",
    "    components = parse_components(class_row.get(\"rdls_components\"))\n",
    "    gate = apply_component_gate(components)\n",
    "    \n",
    "    if not gate.ok:\n",
    "        return None, {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"blocked\": True,\n",
    "            \"blocked_reasons\": \";\".join(gate.reasons),\n",
    "            \"risk_data_type\": \";\".join(gate.risk_data_type),\n",
    "        }\n",
    "    \n",
    "    # Infer spatial from groups\n",
    "    groups = split_semicolon_list(class_row.get(\"groups\"))\n",
    "    spatial = infer_spatial(groups)\n",
    "    \n",
    "    # Infer hazard types for naming\n",
    "    tags = split_semicolon_list(class_row.get(\"tags\"))\n",
    "    hazard_types = infer_hazard_types(tags, title=title, notes=notes)\n",
    "    \n",
    "    dataset_page_url = f\"https://data.humdata.org/dataset/{dataset_id}\"\n",
    "    \n",
    "    # Build entity token for naming\n",
    "    hdx_slug = slugify_token(str(hdx.get(\"name\") or \"\"), max_len=48)\n",
    "    title_slug = slugify_token(title, max_len=48)\n",
    "    dataset_slug = hdx_slug if hdx_slug != \"unknown\" else title_slug\n",
    "    \n",
    "    org_token = slugify_token(str(class_row.get(\"organization\") or hdx.get(\"organization\") or \"unknown\"), max_len=20)\n",
    "    iso3_tok = str(spatial[\"countries\"][0]).lower() if spatial.get(\"countries\") and len(spatial[\"countries\"]) == 1 else \"\"\n",
    "    \n",
    "    # Compose identifier\n",
    "    parts = [p for p in [org_token, iso3_tok, dataset_slug] if p]\n",
    "    entity_token = \"_\".join(parts)\n",
    "    \n",
    "    prefix = choose_prefix(gate.risk_data_type) + \"hdx_\"\n",
    "    hz_suffix = hazard_suffix_for_filename(hazard_types) if ({\"hazard\", \"loss\"} & set(gate.risk_data_type)) else \"\"\n",
    "    \n",
    "    stem_base = f\"{prefix}{entity_token}{hz_suffix}\"\n",
    "    stem = stem_base\n",
    "    \n",
    "    # Collision-proofing\n",
    "    out_path = OUT_RECORDS_DIR / f\"{stem}.json\"\n",
    "    if out_path.exists():\n",
    "        stem = f\"{stem_base}__{dataset_id[:8]}\"\n",
    "    \n",
    "    # Map license\n",
    "    license_raw = str(class_row.get(\"license_title\") or hdx.get(\"license_title\") or \"\").strip()\n",
    "    license_mapped = map_license(license_raw or \"Custom\")\n",
    "    \n",
    "    # Build RDLS dataset record\n",
    "    rdls_ds: Dict[str, Any] = {\n",
    "        \"id\": stem,\n",
    "        \"title\": title or hdx.get(\"name\", \"\") or dataset_id,\n",
    "        \"description\": notes or None,\n",
    "        \"risk_data_type\": gate.risk_data_type,\n",
    "        \"details\": details,\n",
    "        \"spatial\": spatial,\n",
    "        \"license\": license_mapped,\n",
    "        \"attributions\": build_attributions(hdx, dataset_id, dataset_page_url),\n",
    "        \"resources\": build_resources(hdx, dataset_id),\n",
    "        \"links\": [\n",
    "            {\n",
    "                \"href\": \"https://docs.riskdatalibrary.org/en/0__3__0/rdls_schema.json\",\n",
    "                \"rel\": \"describedby\",\n",
    "            },\n",
    "            {\n",
    "                \"href\": dataset_page_url,\n",
    "                \"rel\": \"source\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Remove None values and filter to allowed keys\n",
    "    rdls_ds = {k: v for k, v in rdls_ds.items() if v is not None and k in RDLS_ALLOWED_KEYS}\n",
    "    \n",
    "    # Wrap in top-level structure\n",
    "    rdls_record = {\"datasets\": [rdls_ds]}\n",
    "    \n",
    "    info = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"rdls_id\": stem,\n",
    "        \"filename\": f\"{stem}.json\",\n",
    "        \"risk_data_type\": \";\".join(gate.risk_data_type),\n",
    "        \"spatial_scale\": spatial.get(\"scale\", \"\"),\n",
    "        \"countries_count\": len(spatial.get(\"countries\", []) or []),\n",
    "        \"license_raw\": license_raw,\n",
    "        \"orgtoken\": org_token,\n",
    "        \"hazard_suffix\": hz_suffix.lstrip(\"_\"),\n",
    "        \"organization_token\": org_token,\n",
    "        \"iso3\": iso3_tok,\n",
    "        \"hazard_types\": \";\".join(hazard_types),\n",
    "        \"blocked\": False,\n",
    "        \"blocked_reasons\": \"\",\n",
    "    }\n",
    "    \n",
    "    return rdls_record, info\n",
    "\n",
    "\n",
    "print(\"Record builder functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Clean Previous Outputs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-8-clean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 06 Records]:\n",
      "  rdls_*.json                             : 13,152 files\n",
      "  Cleaned 13,152 files. Ready for fresh output.\n",
      "\n",
      "Output cleanup [NB 06 Index]:\n",
      "  rdls_index.jsonl                        : 1 files\n",
      "  Cleaned 1 files. Ready for fresh output.\n",
      "\n",
      "Output cleanup [NB 06 Reports]: Directory is clean.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deleted': 0, 'skipped': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.1 Clean Previous Outputs\n",
    "\n",
    "Removes stale output files before writing new ones.\n",
    "Controlled by CLEANUP_MODE in cell 1 above.\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice, defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "\n",
    "# ── Run cleanup for NB 06 outputs ─────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    OUT_RECORDS_DIR,\n",
    "    patterns=[\"rdls_*.json\"],\n",
    "    label=\"NB 06 Records\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n",
    "\n",
    "clean_previous_outputs(\n",
    "    OUT_INDEX_DIR,\n",
    "    patterns=[\"rdls_index.jsonl\"],\n",
    "    label=\"NB 06 Index\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n",
    "\n",
    "clean_previous_outputs(\n",
    "    OUT_REPORTS_DIR,\n",
    "    patterns=[\n",
    "        \"translation_blocked.csv\",\n",
    "        \"schema_validation.csv\",\n",
    "        \"translation_qa.csv\",\n",
    "    ],\n",
    "    label=\"NB 06 Reports\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## 9. Validate and Write Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-9-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonschema validation enabled (Draft2020-12)\n",
      "\n",
      "Processing 13,152 datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0500a014004873854772cb46ec01c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating to RDLS:   0%|          | 0/13152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRANSLATION COMPLETE\n",
      "==================================================\n",
      "Written: 13,152\n",
      "Skipped (existing): 0\n",
      "Blocked: 0\n",
      "Schema valid: 13,143 of 13,152\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Process all datasets: build records, validate, and write outputs.\n",
    "\n",
    "Outputs:\n",
    "    - Individual RDLS JSON records\n",
    "    - Index JSONL file\n",
    "    - Blocked datasets report\n",
    "    - Validation results report\n",
    "    - QA summary report\n",
    "\"\"\"\n",
    "\n",
    "# --- JSON Schema validation setup ---\n",
    "def try_import_jsonschema():\n",
    "    try:\n",
    "        import jsonschema\n",
    "        return jsonschema\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "_jsonschema = try_import_jsonschema()\n",
    "validator = None\n",
    "\n",
    "if _jsonschema is not None:\n",
    "    try:\n",
    "        validator = _jsonschema.Draft202012Validator(rdls_schema)\n",
    "        print(\"jsonschema validation enabled (Draft2020-12)\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: jsonschema init failed: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: jsonschema not installed; validation will be skipped\")\n",
    "\n",
    "\n",
    "def validate_record(rec: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    \"\"\"Validate RDLS record against schema.\"\"\"\n",
    "    if validator is None:\n",
    "        return True, \"\"\n",
    "    errors = sorted(validator.iter_errors(rec[\"datasets\"][0]), key=lambda e: e.path)\n",
    "    if not errors:\n",
    "        return True, \"\"\n",
    "    msgs = [f\"{'.'.join(str(p) for p in e.path)}: {e.message}\" for e in errors[:10]]\n",
    "    return False, \" | \".join(msgs)\n",
    "\n",
    "\n",
    "def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    \"\"\"Append object to JSONL file.\"\"\"\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# Initialize output file\n",
    "OUT_INDEX_JSONL.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "# Tracking lists\n",
    "blocked_rows: List[Dict[str, Any]] = []\n",
    "validation_rows: List[Dict[str, Any]] = []\n",
    "qa_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "# Counters\n",
    "written = 0\n",
    "skipped_existing = 0\n",
    "blocked = 0\n",
    "validated_ok = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(included_ids):,} datasets...\")\n",
    "\n",
    "for dataset_id in tqdm(included_ids, desc=\"Translating to RDLS\"):\n",
    "    # Find dataset file\n",
    "    fp = dataset_file_index.get(dataset_id)\n",
    "    \n",
    "    if fp is None or not fp.exists():\n",
    "        blocked += 1\n",
    "        blocked_rows.append({\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"status\": \"blocked_missing_hdx_dataset_json\",\n",
    "            \"reason\": \"missing_hdx_dataset_json\",\n",
    "            \"risk_data_type\": \"\",\n",
    "        })\n",
    "        qa_rows.append({\n",
    "            \"dataset_id\": dataset_id, \"output_id\": \"\", \"filename\": \"\",\n",
    "            \"risk_data_type\": \"\", \"spatial_scale\": \"\", \"countries_count\": 0,\n",
    "            \"license_raw\": \"\", \"orgtoken\": \"\", \"hazard_suffix\": \"\",\n",
    "            \"status\": \"blocked_missing_hdx_dataset_json\", \"reason\": \"missing_hdx_dataset_json\",\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Load HDX metadata\n",
    "    hdx = safe_load_json(fp)\n",
    "    row = df.loc[dataset_id]\n",
    "    \n",
    "    # Build RDLS record\n",
    "    rdls_rec, info = build_rdls_record(hdx, row)\n",
    "    \n",
    "    if rdls_rec is None:\n",
    "        blocked += 1\n",
    "        reason = info.get(\"blocked_reasons\") or \"blocked_by_policy\"\n",
    "        rdt = info.get(\"risk_data_type\") or \"\"\n",
    "        blocked_rows.append({\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"status\": \"blocked_by_policy\",\n",
    "            \"reason\": reason,\n",
    "            \"risk_data_type\": rdt,\n",
    "        })\n",
    "        qa_rows.append({\n",
    "            \"dataset_id\": dataset_id, \"output_id\": \"\", \"filename\": \"\",\n",
    "            \"risk_data_type\": rdt, \"spatial_scale\": \"\", \"countries_count\": 0,\n",
    "            \"license_raw\": \"\", \"orgtoken\": \"\", \"hazard_suffix\": \"\",\n",
    "            \"status\": \"blocked_by_policy\", \"reason\": reason,\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    out_path = OUT_RECORDS_DIR / info[\"filename\"]\n",
    "    \n",
    "    # Skip if exists and configured\n",
    "    if config.skip_existing and out_path.exists():\n",
    "        skipped_existing += 1\n",
    "        qa_rows.append({\n",
    "            \"dataset_id\": dataset_id, \"output_id\": info.get(\"rdls_id\", \"\"),\n",
    "            \"filename\": info.get(\"filename\", \"\"), \"risk_data_type\": info.get(\"risk_data_type\", \"\"),\n",
    "            \"spatial_scale\": info.get(\"spatial_scale\", \"\"), \"countries_count\": info.get(\"countries_count\", 0),\n",
    "            \"license_raw\": info.get(\"license_raw\", \"\"), \"orgtoken\": info.get(\"orgtoken\", \"\"),\n",
    "            \"hazard_suffix\": info.get(\"hazard_suffix\", \"\"),\n",
    "            \"status\": \"skipped_existing\", \"reason\": \"\",\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Validate\n",
    "    ok, msg = validate_record(rdls_rec)\n",
    "    validation_rows.append({\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"rdls_id\": info[\"rdls_id\"],\n",
    "        \"filename\": info[\"filename\"],\n",
    "        \"valid\": ok,\n",
    "        \"message\": msg,\n",
    "    })\n",
    "    if ok:\n",
    "        validated_ok += 1\n",
    "    \n",
    "    # Write JSON\n",
    "    if config.write_pretty_json:\n",
    "        out_path.write_text(json.dumps(rdls_rec, indent=2, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        out_path.write_text(json.dumps(rdls_rec, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "    \n",
    "    append_jsonl(OUT_INDEX_JSONL, info)\n",
    "    written += 1\n",
    "    \n",
    "    qa_rows.append({\n",
    "        \"dataset_id\": dataset_id, \"output_id\": info.get(\"rdls_id\", \"\"),\n",
    "        \"filename\": info.get(\"filename\", \"\"), \"risk_data_type\": info.get(\"risk_data_type\", \"\"),\n",
    "        \"spatial_scale\": info.get(\"spatial_scale\", \"\"), \"countries_count\": info.get(\"countries_count\", 0),\n",
    "        \"license_raw\": info.get(\"license_raw\", \"\"), \"orgtoken\": info.get(\"orgtoken\", \"\"),\n",
    "        \"hazard_suffix\": info.get(\"hazard_suffix\", \"\"),\n",
    "        \"status\": \"written\", \"reason\": \"\",\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"TRANSLATION COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Written: {written:,}\")\n",
    "print(f\"Skipped (existing): {skipped_existing:,}\")\n",
    "print(f\"Blocked: {blocked:,}\")\n",
    "print(f\"Schema valid: {validated_ok:,} of {len(validation_rows):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10-header",
   "metadata": {},
   "source": [
    "## 10. Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-10-reports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/translation_blocked.csv\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/schema_validation.csv\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/translation_qa.csv\n",
      "\n",
      "Index file: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/index/rdls_index.jsonl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save translation reports to CSV files.\n",
    "\"\"\"\n",
    "\n",
    "# Save blocked datasets report\n",
    "blocked_df = pd.DataFrame(blocked_rows, columns=[\"dataset_id\", \"status\", \"reason\", \"risk_data_type\"])\n",
    "blocked_df.to_csv(OUT_BLOCKED_CSV, index=False)\n",
    "print(f\"Wrote: {OUT_BLOCKED_CSV}\")\n",
    "\n",
    "# Save validation report\n",
    "val_df = pd.DataFrame(validation_rows, columns=[\"dataset_id\", \"rdls_id\", \"filename\", \"valid\", \"message\"])\n",
    "val_df.to_csv(OUT_VALIDATION_CSV, index=False)\n",
    "print(f\"Wrote: {OUT_VALIDATION_CSV}\")\n",
    "\n",
    "# Save QA report\n",
    "qa_df = pd.DataFrame(qa_rows, columns=[\n",
    "    \"dataset_id\", \"output_id\", \"filename\", \"risk_data_type\",\n",
    "    \"spatial_scale\", \"countries_count\", \"license_raw\", \"orgtoken\",\n",
    "    \"hazard_suffix\", \"status\", \"reason\",\n",
    "])\n",
    "qa_df.to_csv(OUT_QA_CSV, index=False)\n",
    "print(f\"Wrote: {OUT_QA_CSV}\")\n",
    "\n",
    "print(f\"\\nIndex file: {OUT_INDEX_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11-header",
   "metadata": {},
   "source": [
    "## 11. QA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-11-qa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "QA SUMMARY\n",
      "==================================================\n",
      "Index lines: 13,152\n",
      "Records on disk: 13,152\n",
      "\n",
      "Blocked datasets: 0\n",
      "\n",
      "Validation failures: 9\n",
      "Top validation errors:\n",
      "  - spatial.countries.0: 'XKX' is not one of ['AFG', 'ALB', 'DZA...: 5\n",
      "  - spatial.countries.137: 'XKX' is not one of ['AFG', 'ALB', 'D...: 1\n",
      "  - spatial.countries.176: 'XKX' is not one of ['AFG', 'ALB', 'D...: 1\n",
      "  - spatial.countries.87: 'XKX' is not one of ['AFG', 'ALB', 'DZ...: 1\n",
      "  - spatial.countries.203: 'XKX' is not one of ['AFG', 'ALB', 'D...: 1\n",
      "\n",
      "QA Status Distribution:\n",
      "  - written: 13,152\n",
      "\n",
      "Nested Required Fields Check:\n",
      "----------------------------------------\n",
      "  All records have complete attributions (3 roles) and resource sub-fields.\n",
      "\n",
      "Unmapped HDX Formats (6):\n",
      "  - arc/info grid\n",
      "  - doc\n",
      "  - erdas image\n",
      "  - stata data file\n",
      "  - zipped jpeg\n",
      "  - zipped tif\n",
      "\n",
      "==================================================\n",
      "Step 6 complete. Proceed to Step 7 for validation and packaging.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick QA summary of translation results.\n",
    "\"\"\"\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV safely, returning empty DataFrame if file is empty.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except EmptyDataError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QA SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Index stats\n",
    "idx_lines = OUT_INDEX_JSONL.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "print(f\"Index lines: {len(idx_lines):,}\")\n",
    "print(f\"Records on disk: {len(list(OUT_RECORDS_DIR.glob('*.json'))):,}\")\n",
    "\n",
    "# Blocked stats\n",
    "if OUT_BLOCKED_CSV.exists():\n",
    "    blocked_df = safe_read_csv(OUT_BLOCKED_CSV)\n",
    "    print(f\"\\nBlocked datasets: {len(blocked_df):,}\")\n",
    "    if not blocked_df.empty and \"reason\" in blocked_df.columns:\n",
    "        print(\"Top blocked reasons:\")\n",
    "        for reason, count in blocked_df[\"reason\"].value_counts().head(5).items():\n",
    "            print(f\"  - {reason}: {count:,}\")\n",
    "\n",
    "# Validation stats\n",
    "if OUT_VALIDATION_CSV.exists():\n",
    "    val_df = safe_read_csv(OUT_VALIDATION_CSV)\n",
    "    if not val_df.empty:\n",
    "        failures = val_df.loc[val_df[\"valid\"] == False, \"message\"]\n",
    "        print(f\"\\nValidation failures: {len(failures):,}\")\n",
    "        if len(failures) > 0:\n",
    "            print(\"Top validation errors:\")\n",
    "            for msg, count in failures.value_counts().head(5).items():\n",
    "                print(f\"  - {msg[:60]}...: {count:,}\")\n",
    "\n",
    "# QA status\n",
    "if OUT_QA_CSV.exists():\n",
    "    qa_df = safe_read_csv(OUT_QA_CSV)\n",
    "    if not qa_df.empty and \"status\" in qa_df.columns:\n",
    "        print(f\"\\nQA Status Distribution:\")\n",
    "        for status, count in qa_df[\"status\"].value_counts().items():\n",
    "            print(f\"  - {status}: {count:,}\")\n",
    "\n",
    "# --- Nested required fields validation (M8) ---\n",
    "print(\"\\nNested Required Fields Check:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "REQUIRED_ATTRIBUTION_ROLES = {\"publisher\", \"creator\", \"contact_point\"}\n",
    "REQUIRED_RESOURCE_FIELDS = {\"id\", \"title\", \"data_format\", \"access_modality\"}\n",
    "\n",
    "nested_warnings = []\n",
    "\n",
    "for rec_path in sorted(OUT_RECORDS_DIR.glob(\"*.json\")):\n",
    "    try:\n",
    "        rec = json.loads(rec_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        continue\n",
    "    ds = rec.get(\"datasets\", [{}])[0] if isinstance(rec.get(\"datasets\"), list) and rec.get(\"datasets\") else rec\n",
    "    rdls_id = ds.get(\"id\", rec_path.stem)\n",
    "\n",
    "    # Check attributions: must have all 3 required roles\n",
    "    attributions = ds.get(\"attributions\", [])\n",
    "    attr_roles = {a.get(\"role\") for a in attributions if isinstance(a, dict)}\n",
    "    missing_roles = REQUIRED_ATTRIBUTION_ROLES - attr_roles\n",
    "    if missing_roles:\n",
    "        nested_warnings.append({\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"check\": \"attribution_roles\",\n",
    "            \"detail\": f\"Missing roles: {sorted(missing_roles)}\",\n",
    "        })\n",
    "\n",
    "    # Check resources: each must have required sub-fields\n",
    "    resources = ds.get(\"resources\", [])\n",
    "    for i, r in enumerate(resources):\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        missing_res = {k for k in REQUIRED_RESOURCE_FIELDS if not r.get(k)}\n",
    "        if missing_res:\n",
    "            nested_warnings.append({\n",
    "                \"rdls_id\": rdls_id,\n",
    "                \"check\": f\"resource[{i}]_fields\",\n",
    "                \"detail\": f\"Missing fields: {sorted(missing_res)}\",\n",
    "            })\n",
    "\n",
    "if nested_warnings:\n",
    "    print(f\"  Nested field warnings: {len(nested_warnings)}\")\n",
    "    for w in nested_warnings[:10]:\n",
    "        print(f\"    [{w['rdls_id']}] {w['check']}: {w['detail']}\")\n",
    "    if len(nested_warnings) > 10:\n",
    "        print(f\"    ... and {len(nested_warnings) - 10} more\")\n",
    "else:\n",
    "    print(\"  All records have complete attributions (3 roles) and resource sub-fields.\")\n",
    "\n",
    "# Report unmapped formats if any were tracked\n",
    "if unmapped_hdx_formats:\n",
    "    print(f\"\\nUnmapped HDX Formats ({len(unmapped_hdx_formats)}):\")\n",
    "    for fmt in sorted(unmapped_hdx_formats):\n",
    "        print(f\"  - {fmt}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Step 6 complete. Proceed to Step 7 for validation and packaging.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd480840-b809-4d6a-8ea9-ba37f1f113d2",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
