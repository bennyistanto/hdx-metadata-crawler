{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Step 6 - Translate HDX Metadata to RDLS v0.3 JSON\n",
    "\n",
    "**Purpose:** Transform HDX dataset-level metadata exports into RDLS v0.3 metadata records.\n",
    "\n",
    "**Process:**\n",
    "1. Load classification results and included dataset IDs from Step 5\n",
    "2. Build RDLS-compliant records with proper attributions, resources, and spatial info\n",
    "3. Apply component gating rules (V/L require H or E)\n",
    "4. Validate against JSON Schema and write outputs\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- Step 5 outputs:\n",
    "  - `hdx_dataset_metadata_dump/derived/classification_final.csv`\n",
    "  - `hdx_dataset_metadata_dump/derived/rdls_included_dataset_ids_final.txt`\n",
    "- HDX dataset metadata JSON corpus:\n",
    "  - `hdx_dataset_metadata_dump/dataset_metadata/*.json`\n",
    "- RDLS v0.3 assets:\n",
    "  - `rdls_schema_v0.3.json`\n",
    "  - `rdls_template_v03.json`\n",
    "\n",
    "## Outputs\n",
    "- `hdx_dataset_metadata_dump/rdls/records/*.json` — one RDLS record per included HDX dataset\n",
    "- `hdx_dataset_metadata_dump/rdls/index/rdls_index.jsonl` — index of written records\n",
    "- `hdx_dataset_metadata_dump/rdls/reports/translation_blocked.csv` — datasets blocked by policy/required-field gaps\n",
    "- `hdx_dataset_metadata_dump/rdls/reports/schema_validation.csv` — JSON Schema validation results\n",
    "\n",
    "## Strictness & Policy\n",
    "- **Schema-first:** required RDLS fields are always populated; optional fields omitted unless safely filled\n",
    "- **No extra fields:** output contains only fields defined in the RDLS schema\n",
    "- **No invented content:** missing values → absent optional fields (not empty strings)\n",
    "- **Open codelists:** values may be kept as-is if not in suggestions\n",
    "- **Component combination rule:**\n",
    "  - hazard-only and exposure-only are allowed\n",
    "  - vulnerability must accompany hazard or exposure\n",
    "  - loss must accompany hazard or exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup: Import libraries and configure paths.\n",
    "\n",
    "Configuration Options:\n",
    "    MAX_DATASETS: Limit number of datasets to process (None for all, 50 for testing)\n",
    "    OUTPUT_MODE: 'in_place' or 'run_folder' for versioned outputs\n",
    "    SKIP_EXISTING: Resume-safe mode to skip already processed records\n",
    "    WRITE_PRETTY_JSON: Pretty-print JSON for readability\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- tqdm with graceful fallback ---\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "    def tqdm(iterable, **kwargs):\n",
    "        \"\"\"Fallback: return iterable unchanged if tqdm not installed.\"\"\"\n",
    "        return iterable\n",
    "\n",
    "print(f\"tqdm available: {TQDM_AVAILABLE}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranslationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for HDX to RDLS translation.\n",
    "    \n",
    "    Attributes:\n",
    "        dump_dir: Root directory for HDX metadata dump\n",
    "        max_datasets: Limit number of datasets (None for all)\n",
    "        output_mode: 'in_place' or 'run_folder'\n",
    "        clean_before_run: Clean existing outputs before writing (in_place only)\n",
    "        skip_existing: Skip already processed records\n",
    "        write_pretty_json: Pretty-print JSON output\n",
    "        auto_repair_components: Auto-add exposure for standalone V/L\n",
    "    \"\"\"\n",
    "    dump_dir: Path = field(default_factory=lambda: (Path(\"..\") / \"hdx_dataset_metadata_dump\").resolve())\n",
    "    max_datasets: Optional[int] = 50  # Set to None for production\n",
    "    output_mode: str = \"in_place\"  # \"in_place\" | \"run_folder\"\n",
    "    clean_before_run: bool = True\n",
    "    skip_existing: bool = False\n",
    "    write_pretty_json: bool = True\n",
    "    auto_repair_components: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        if self.output_mode == \"run_folder\" and self.clean_before_run:\n",
    "            raise ValueError(\"clean_before_run cannot be True when output_mode='run_folder'\")\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "config = TranslationConfig()\n",
    "\n",
    "# --- Resolve paths ---\n",
    "DUMP_DIR = config.dump_dir\n",
    "DATASET_DIR = (DUMP_DIR / \"dataset_metadata\").resolve()\n",
    "DERIVED_DIR = (DUMP_DIR / \"derived\").resolve()\n",
    "CLASSIFICATION_FINAL_CSV = (DERIVED_DIR / \"classification_final.csv\").resolve()\n",
    "INCLUDED_IDS_TXT = (DERIVED_DIR / \"rdls_included_dataset_ids_final.txt\").resolve()\n",
    "\n",
    "# RDLS assets\n",
    "RDLS_DIR = (DUMP_DIR / \"rdls\").resolve()\n",
    "RDLS_SCHEMA_PATH = (RDLS_DIR / \"schema\" / \"rdls_schema_v0.3.json\").resolve()\n",
    "RDLS_TEMPLATE_PATH = (RDLS_DIR / \"template\" / \"rdls_template_v03.json\").resolve()\n",
    "\n",
    "# Resolve run directory\n",
    "if config.output_mode == \"run_folder\":\n",
    "    RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    RDLS_RUN_DIR = (RDLS_DIR / \"runs\" / RUN_ID).resolve()\n",
    "else:\n",
    "    RUN_ID = \"in_place\"\n",
    "    RDLS_RUN_DIR = RDLS_DIR\n",
    "\n",
    "# Output directories\n",
    "OUT_RECORDS_DIR = RDLS_RUN_DIR / \"records\"\n",
    "OUT_INDEX_DIR = RDLS_RUN_DIR / \"index\"\n",
    "OUT_REPORTS_DIR = RDLS_RUN_DIR / \"reports\"\n",
    "\n",
    "OUT_INDEX_JSONL = OUT_INDEX_DIR / \"rdls_index.jsonl\"\n",
    "OUT_BLOCKED_CSV = OUT_REPORTS_DIR / \"translation_blocked.csv\"\n",
    "OUT_VALIDATION_CSV = OUT_REPORTS_DIR / \"schema_validation.csv\"\n",
    "OUT_QA_CSV = OUT_REPORTS_DIR / \"translation_qa.csv\"\n",
    "\n",
    "# Ensure output folders exist\n",
    "for p in [OUT_RECORDS_DIR, OUT_INDEX_DIR, OUT_REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  DUMP_DIR: {DUMP_DIR}\")\n",
    "print(f\"  DATASET_DIR: {DATASET_DIR}\")\n",
    "print(f\"  OUTPUT_MODE: {config.output_mode}\")\n",
    "print(f\"  RUN_ID: {RUN_ID}\")\n",
    "print(f\"  MAX_DATASETS: {config.max_datasets}\")\n",
    "print(f\"  SKIP_EXISTING: {config.skip_existing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Mapping Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-mappings",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping configurations for HDX to RDLS translation.\n",
    "\n",
    "Includes:\n",
    "    - Hazard filename aliases\n",
    "    - Resource format mapping (HDX -> RDLS)\n",
    "    - License mapping (HDX -> RDLS)\n",
    "    - Hazard keywords for inference\n",
    "\"\"\"\n",
    "\n",
    "# --- Hazard inference / filename alias ---\n",
    "HAZARD_FILENAME_ALIASES: Dict[str, str] = {\n",
    "    \"strong_wind\": \"windstorm\",\n",
    "    # Add more aliases as needed\n",
    "}\n",
    "\n",
    "# --- Resource format mapping (HDX -> RDLS data_format enum) ---\n",
    "HDX_FORMAT_TO_RDLS: Dict[str, str] = {\n",
    "    \"CSV\": \"CSV (csv)\",\n",
    "    \"XLS\": \"Excel (xlsx)\",\n",
    "    \"XLSX\": \"Excel (xlsx)\",\n",
    "    \"EXCEL\": \"Excel (xlsx)\",\n",
    "    \"JSON\": \"JSON (json)\",\n",
    "    \"GEOJSON\": \"GeoJSON (geojson)\",\n",
    "    \"SHP\": \"Shapefile (shp)\",\n",
    "    \"SHAPEFILE\": \"Shapefile (shp)\",\n",
    "    \"GPKG\": \"GeoPackage (gpkg)\",\n",
    "    \"GEOPACKAGE\": \"GeoPackage (gpkg)\",\n",
    "    \"KML\": \"KML (kml)\",\n",
    "    \"PDF\": \"PDF (pdf)\",\n",
    "    \"NC\": \"NetCDF (nc)\",\n",
    "    \"NETCDF\": \"NetCDF (nc)\",\n",
    "    \"TIF\": \"GeoTIFF (tif)\",\n",
    "    \"TIFF\": \"GeoTIFF (tif)\",\n",
    "    \"COG\": \"Cloud Optimized GeoTIFF (cog)\",\n",
    "    \"PARQUET\": \"Parquet (parquet)\",\n",
    "    \"XML\": \"XML (xml)\",\n",
    "}\n",
    "\n",
    "# --- License mapping (HDX -> RDLS) ---\n",
    "HDX_LICENSE_TO_RDLS: Dict[str, str] = {\n",
    "    \"public domain\": \"PDDL-1.0\",\n",
    "    \"odbl\": \"ODbL-1.0\",\n",
    "    \"cc-by-4.0\": \"CC-BY-4.0\",\n",
    "    \"cc by 4.0\": \"CC-BY-4.0\",\n",
    "    \"cc-by\": \"CC-BY-4.0\",\n",
    "    \"cc0\": \"CC0-1.0\",\n",
    "    \"cc0-1.0\": \"CC0-1.0\",\n",
    "    \"copyright\": \"Copyright\",\n",
    "}\n",
    "\n",
    "# --- Hazard keyword to type mapping ---\n",
    "HAZARD_KEYWORDS_TO_TYPE: Dict[str, str] = {\n",
    "    # Hydro\n",
    "    \"flood\": \"flood\", \"flooding\": \"flood\", \"river flood\": \"flood\",\n",
    "    \"flash flood\": \"flood\", \"inundation\": \"flood\",\n",
    "    # Drought\n",
    "    \"drought\": \"drought\", \"dry spell\": \"drought\", \"water scarcity\": \"drought\",\n",
    "    # Storms / wind\n",
    "    \"cyclone\": \"windstorm\", \"hurricane\": \"windstorm\", \"typhoon\": \"windstorm\",\n",
    "    \"windstorm\": \"windstorm\", \"storm\": \"windstorm\",\n",
    "    # Heat / wildfire\n",
    "    \"heatwave\": \"heat\", \"extreme heat\": \"heat\",\n",
    "    \"wildfire\": \"wildfire\", \"fire\": \"wildfire\",\n",
    "    # Seismic\n",
    "    \"earthquake\": \"earthquake\", \"tsunami\": \"tsunami\",\n",
    "    # Landslide\n",
    "    \"landslide\": \"landslide\", \"mudslide\": \"landslide\", \"avalanche\": \"landslide\",\n",
    "}\n",
    "\n",
    "# --- Risk components mapping (Step 5 -> RDLS enum) ---\n",
    "COMPONENT_MAP: Dict[str, str] = {\n",
    "    \"hazard\": \"hazard\",\n",
    "    \"exposure\": \"exposure\",\n",
    "    \"vulnerability_proxy\": \"vulnerability\",\n",
    "    \"loss_impact\": \"loss\",\n",
    "    \"vulnerability\": \"vulnerability\",\n",
    "    \"loss\": \"loss\",\n",
    "}\n",
    "\n",
    "print(f\"Format mappings loaded: {len(HDX_FORMAT_TO_RDLS)} entries\")\n",
    "print(f\"Hazard keywords loaded: {len(HAZARD_KEYWORDS_TO_TYPE)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Load RDLS Schema and Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load RDLS schema and template for validation and record building.\n",
    "\n",
    "Raises:\n",
    "    FileNotFoundError: If schema or template files are missing.\n",
    "\"\"\"\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file with UTF-8 encoding.\"\"\"\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "# Validate schema/template existence\n",
    "if not RDLS_SCHEMA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"RDLS schema not found: {RDLS_SCHEMA_PATH}\")\n",
    "\n",
    "if not RDLS_TEMPLATE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"RDLS template not found: {RDLS_TEMPLATE_PATH}\")\n",
    "\n",
    "rdls_schema = safe_load_json(RDLS_SCHEMA_PATH)\n",
    "rdls_template = safe_load_json(RDLS_TEMPLATE_PATH)\n",
    "\n",
    "# RDLS dataset object allowed and required keys\n",
    "RDLS_ALLOWED_KEYS = set(rdls_schema[\"properties\"].keys())\n",
    "RDLS_REQUIRED_KEYS = list(rdls_schema.get(\"required\", []))\n",
    "\n",
    "print(f\"RDLS required keys: {RDLS_REQUIRED_KEYS}\")\n",
    "print(f\"RDLS allowed keys count: {len(RDLS_ALLOWED_KEYS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Load Step 5 Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-load-inputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load classification results and build dataset index.\n",
    "\n",
    "Loads:\n",
    "    - classification_final.csv from Step 5\n",
    "    - included dataset IDs list\n",
    "    - Builds dataset_id -> file path index\n",
    "\"\"\"\n",
    "\n",
    "def read_ids_txt(path: Path) -> List[str]:\n",
    "    \"\"\"\n",
    "    Read dataset IDs from text file (one per line).\n",
    "    \n",
    "    Parameters:\n",
    "        path: Path to the IDs file\n",
    "        \n",
    "    Returns:\n",
    "        List of dataset IDs\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing IDs list: {path}\")\n",
    "    return [line.strip() for line in path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
    "\n",
    "\n",
    "# Validate inputs exist\n",
    "for path, name in [\n",
    "    (CLASSIFICATION_FINAL_CSV, \"Classification CSV\"),\n",
    "    (INCLUDED_IDS_TXT, \"Included IDs list\"),\n",
    "    (DATASET_DIR, \"Dataset metadata folder\"),\n",
    "]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing Step 5 output: {name} at {path}\")\n",
    "\n",
    "# Load classification data\n",
    "df = pd.read_csv(CLASSIFICATION_FINAL_CSV)\n",
    "included_ids = read_ids_txt(INCLUDED_IDS_TXT)\n",
    "\n",
    "# Fast lookups\n",
    "df = df.set_index(\"dataset_id\", drop=False)\n",
    "included_set = set(included_ids)\n",
    "\n",
    "print(f\"Classification rows: {len(df):,}\")\n",
    "print(f\"Included IDs: {len(included_ids):,}\")\n",
    "print(f\"Included IDs in classification: {sum(did in df.index for did in included_ids):,}\")\n",
    "\n",
    "# Build dataset_id -> file path mapping (avoid N glob calls)\n",
    "print(\"\\nBuilding dataset file index...\")\n",
    "dataset_file_index: Dict[str, Path] = {}\n",
    "for fp in tqdm(sorted(DATASET_DIR.glob(\"*.json\")), desc=\"Indexing files\"):\n",
    "    stem = fp.stem\n",
    "    dataset_uuid = stem.split(\"__\", 1)[0] if \"__\" in stem else stem\n",
    "    dataset_file_index[dataset_uuid] = fp\n",
    "\n",
    "print(f\"Dataset files indexed: {len(dataset_file_index):,}\")\n",
    "\n",
    "# Apply MAX_DATASETS limit for testing\n",
    "if config.max_datasets is not None:\n",
    "    included_ids = included_ids[:config.max_datasets]\n",
    "    included_set = set(included_ids)\n",
    "    print(f\"\\nTEST MODE: Limited to {len(included_ids)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for parsing, slugifying, and mapping.\n",
    "\n",
    "Includes:\n",
    "    - Text parsing utilities\n",
    "    - ISO3 country inference\n",
    "    - Hazard type inference\n",
    "    - License and format mapping\n",
    "\"\"\"\n",
    "\n",
    "def slugify_token(s: str, max_len: int = 32) -> str:\n",
    "    \"\"\"Convert string to URL-safe slug token.\"\"\"\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return (s[:max_len].strip(\"_\") or \"unknown\")\n",
    "\n",
    "\n",
    "def split_semicolon_list(s: Any) -> List[str]:\n",
    "    \"\"\"Split semicolon/comma separated string into list.\"\"\"\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return []\n",
    "    if isinstance(s, list):\n",
    "        return [str(x).strip() for x in s if str(x).strip()]\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [x.strip() for x in re.split(r\"[;,]\", s) if x.strip()]\n",
    "\n",
    "\n",
    "def looks_like_url(s: str) -> bool:\n",
    "    \"\"\"Check if string looks like a URL.\"\"\"\n",
    "    return bool(re.match(r\"^https?://\", (s or \"\").strip(), flags=re.I))\n",
    "\n",
    "\n",
    "# --- ISO3 inference ---\n",
    "def try_import_pycountry():\n",
    "    \"\"\"Try to import pycountry for country code lookups.\"\"\"\n",
    "    try:\n",
    "        import pycountry\n",
    "        return pycountry\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "_pycountry = try_import_pycountry()\n",
    "\n",
    "COMMON_COUNTRY_FIXES: Dict[str, str] = {\n",
    "    \"cote d'ivoire\": \"CIV\", \"ivory coast\": \"CIV\",\n",
    "    \"democratic republic of the congo\": \"COD\", \"dr congo\": \"COD\",\n",
    "    \"republic of the congo\": \"COG\", \"congo, rep.\": \"COG\", \"congo, dem. rep.\": \"COD\",\n",
    "    \"lao pdr\": \"LAO\", \"viet nam\": \"VNM\",\n",
    "    \"korea, rep.\": \"KOR\", \"korea, dem. rep.\": \"PRK\",\n",
    "    \"syrian arab republic\": \"SYR\", \"iran, islamic republic of\": \"IRN\",\n",
    "    \"tanzania, united republic of\": \"TZA\", \"venezuela, bolivarian republic of\": \"VEN\",\n",
    "    \"bolivia, plurinational state of\": \"BOL\", \"moldova, republic of\": \"MDA\",\n",
    "    \"palestine\": \"PSE\", \"russia\": \"RUS\",\n",
    "    \"united states\": \"USA\", \"united kingdom\": \"GBR\",\n",
    "}\n",
    "\n",
    "def _norm_country_key(s: str) -> str:\n",
    "    \"\"\"Normalize country name for lookup.\"\"\"\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[\\(\\)\\[\\]\\{\\}\\.\\,\\;\\:]\", \" \", s)\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-']\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "COMMON_COUNTRY_FIXES_NORM = {_norm_country_key(k): v for k, v in COMMON_COUNTRY_FIXES.items()}\n",
    "\n",
    "# Load country ISO3 table if available\n",
    "COUNTRY_ISO3_CSV = (DUMP_DIR / \"config\" / \"country_name_to_iso3.csv\")\n",
    "\n",
    "def load_country_iso3_table(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load country name to ISO3 mapping from CSV.\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        df_iso = pd.read_csv(path)\n",
    "        cols = {c.lower(): c for c in df_iso.columns}\n",
    "        name_col = cols.get(\"name\") or cols.get(\"country\") or cols.get(\"country_name\")\n",
    "        iso3_col = cols.get(\"iso3\") or cols.get(\"alpha_3\") or cols.get(\"code\")\n",
    "        if not name_col or not iso3_col:\n",
    "            return {}\n",
    "        return {\n",
    "            _norm_country_key(str(r.get(name_col, \"\"))): str(r.get(iso3_col, \"\")).strip().upper()\n",
    "            for _, r in df_iso.iterrows()\n",
    "            if str(r.get(name_col, \"\")).strip() and len(str(r.get(iso3_col, \"\")).strip()) == 3\n",
    "        }\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "COUNTRY_ISO3_TABLE = load_country_iso3_table(COUNTRY_ISO3_CSV)\n",
    "\n",
    "\n",
    "def country_name_to_iso3(name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Convert country name to ISO3 code.\n",
    "    \n",
    "    Parameters:\n",
    "        name: Country name or ISO3 code\n",
    "        \n",
    "    Returns:\n",
    "        ISO3 code or None if not found\n",
    "    \"\"\"\n",
    "    n = (name or \"\").strip()\n",
    "    if not n:\n",
    "        return None\n",
    "    \n",
    "    # Already ISO3?\n",
    "    if len(n) == 3 and n.isalpha():\n",
    "        return n.upper()\n",
    "    \n",
    "    key = _norm_country_key(n)\n",
    "    \n",
    "    # Check fixes first\n",
    "    if key in COMMON_COUNTRY_FIXES_NORM:\n",
    "        return COMMON_COUNTRY_FIXES_NORM[key]\n",
    "    \n",
    "    # Check table\n",
    "    if key in COUNTRY_ISO3_TABLE:\n",
    "        return COUNTRY_ISO3_TABLE[key]\n",
    "    \n",
    "    # Try pycountry\n",
    "    if _pycountry is not None:\n",
    "        try:\n",
    "            c = _pycountry.countries.lookup(n)\n",
    "            return getattr(c, \"alpha_3\", None)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_spatial(groups: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Infer RDLS spatial block from HDX country groups.\"\"\"\n",
    "    iso3s = sorted(set(filter(None, [country_name_to_iso3(g) for g in groups])))\n",
    "    \n",
    "    if len(iso3s) == 1:\n",
    "        return {\"scale\": \"national\", \"countries\": iso3s}\n",
    "    if len(iso3s) > 1:\n",
    "        return {\"scale\": \"regional\", \"countries\": iso3s}\n",
    "    return {\"scale\": \"global\"}\n",
    "\n",
    "\n",
    "def infer_hazard_types(tags: List[str], title: str = \"\", notes: str = \"\") -> List[str]:\n",
    "    \"\"\"Infer hazard types from tags and text content.\"\"\"\n",
    "    text = \" \".join([*tags, title or \"\", notes or \"\"]).lower()\n",
    "    hits = set()\n",
    "    for k, ht in HAZARD_KEYWORDS_TO_TYPE.items():\n",
    "        if k in text:\n",
    "            hits.add(ht)\n",
    "    return sorted(hits)\n",
    "\n",
    "\n",
    "def hazard_suffix_for_filename(hazard_types: List[str]) -> str:\n",
    "    \"\"\"Generate hazard suffix for filename.\"\"\"\n",
    "    if not hazard_types:\n",
    "        return \"\"\n",
    "    if len(hazard_types) > 1:\n",
    "        return \"_multihazard\"\n",
    "    ht = hazard_types[0]\n",
    "    return \"_\" + slugify_token(HAZARD_FILENAME_ALIASES.get(ht, ht), max_len=24)\n",
    "\n",
    "\n",
    "def parse_components(s: Any) -> List[str]:\n",
    "    \"\"\"Parse risk components from semicolon-separated string.\"\"\"\n",
    "    parts = split_semicolon_list(s)\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for p in parts:\n",
    "        mapped = COMPONENT_MAP.get(p.strip().lower())\n",
    "        if mapped and mapped not in seen:\n",
    "            result.append(mapped)\n",
    "            seen.add(mapped)\n",
    "    return result\n",
    "\n",
    "\n",
    "def choose_prefix(risk_data_type: List[str]) -> str:\n",
    "    \"\"\"Choose RDLS filename prefix based on risk data type.\"\"\"\n",
    "    rset = set(risk_data_type)\n",
    "    if \"loss\" in rset:\n",
    "        return \"rdls_lss-\"\n",
    "    if \"vulnerability\" in rset:\n",
    "        return \"rdls_vln-\"\n",
    "    if \"exposure\" in rset:\n",
    "        return \"rdls_exp-\"\n",
    "    return \"rdls_hzd-\"\n",
    "\n",
    "\n",
    "def map_license(hdx_license_title: str) -> str:\n",
    "    \"\"\"\n",
    "    Map HDX license strings to RDLS schema suggestions.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx_license_title: License title from HDX\n",
    "        \n",
    "    Returns:\n",
    "        RDLS-compatible license identifier\n",
    "    \"\"\"\n",
    "    raw = (hdx_license_title or \"\").strip()\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    \n",
    "    key = re.sub(r\"\\s+\", \" \", raw.lower().strip())\n",
    "    \n",
    "    # Pattern mappings\n",
    "    if re.search(r\"\\bcc0\\b\", key) or (\"public domain\" in key and \"cc0\" in key):\n",
    "        return \"CC0-1.0\"\n",
    "    if \"odbl\" in key or \"open database license\" in key:\n",
    "        return \"ODbL-1.0\"\n",
    "    if \"pddl\" in key or \"public domain dedication\" in key:\n",
    "        return \"PDDL-1.0\"\n",
    "    \n",
    "    # Creative Commons variants\n",
    "    k2 = key.replace(\"creative commons\", \"cc\")\n",
    "    \n",
    "    if re.search(r\"\\bcc\\s*by\\b\", k2) and \"sa\" not in k2 and \"nd\" not in k2 and \"nc\" not in k2:\n",
    "        return \"CC-BY-4.0\" if \"4.0\" in k2 or \"v4\" in k2 else (\"CC-BY-3.0\" if \"3.0\" in k2 else \"CC-BY-4.0\")\n",
    "    if \"by-sa\" in k2 or re.search(r\"\\bcc\\s*by\\s*sa\\b\", k2):\n",
    "        return \"CC-BY-SA-4.0\" if \"4.0\" in k2 else (\"CC-BY-SA-3.0\" if \"3.0\" in k2 else \"CC-BY-SA-4.0\")\n",
    "    if \"by-nc\" in k2 or re.search(r\"\\bcc\\s*by\\s*nc\\b\", k2):\n",
    "        return \"CC-BY-NC-4.0\" if \"4.0\" in k2 else (\"CC-BY-NC-3.0\" if \"3.0\" in k2 else \"CC-BY-NC-4.0\")\n",
    "    \n",
    "    return HDX_LICENSE_TO_RDLS.get(re.sub(r\"\\s+\", \" \", k2).strip(), raw)\n",
    "\n",
    "\n",
    "def map_data_format(hdx_fmt: str, url: str = \"\") -> Optional[str]:\n",
    "    \"\"\"Map HDX format to RDLS data_format enum.\"\"\"\n",
    "    s = (hdx_fmt or \"\").strip().upper()\n",
    "    if s in HDX_FORMAT_TO_RDLS:\n",
    "        return HDX_FORMAT_TO_RDLS[s]\n",
    "    \n",
    "    # Guess from URL extension\n",
    "    u = (url or \"\").lower()\n",
    "    ext_map = [\n",
    "        (\".geojson\", \"GeoJSON (geojson)\"), (\".json\", \"JSON (json)\"),\n",
    "        (\".csv\", \"CSV (csv)\"), (\".xlsx\", \"Excel (xlsx)\"), (\".xls\", \"Excel (xlsx)\"),\n",
    "        (\".shp\", \"Shapefile (shp)\"), (\".zip\", \"Shapefile (shp)\"),\n",
    "        (\".tif\", \"GeoTIFF (tif)\"), (\".tiff\", \"GeoTIFF (tif)\"),\n",
    "        (\".nc\", \"NetCDF (nc)\"), (\".pdf\", \"PDF (pdf)\"),\n",
    "        (\".parquet\", \"Parquet (parquet)\"), (\".gpkg\", \"GeoPackage (gpkg)\"),\n",
    "    ]\n",
    "    for ext, rdls in ext_map:\n",
    "        if u.endswith(ext):\n",
    "            return rdls\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Component Gating Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-gating",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Component gating logic for RDLS risk_data_type validation.\n",
    "\n",
    "Rules:\n",
    "    - vulnerability must co-occur with hazard or exposure\n",
    "    - loss must co-occur with hazard or exposure\n",
    "    - risk_data_type must be non-empty\n",
    "\"\"\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ComponentGateResult:\n",
    "    \"\"\"\n",
    "    Result of component gating validation.\n",
    "    \n",
    "    Attributes:\n",
    "        ok: Whether validation passed\n",
    "        reasons: Tuple of reason codes\n",
    "        risk_data_type: Validated/repaired risk data type list\n",
    "    \"\"\"\n",
    "    ok: bool\n",
    "    reasons: Tuple[str, ...]\n",
    "    risk_data_type: List[str]\n",
    "\n",
    "\n",
    "def apply_component_gate(components: List[str]) -> ComponentGateResult:\n",
    "    \"\"\"\n",
    "    Enforce RDLS component combination rules.\n",
    "    \n",
    "    Parameters:\n",
    "        components: List of risk components\n",
    "        \n",
    "    Returns:\n",
    "        ComponentGateResult with validation status\n",
    "    \"\"\"\n",
    "    allowed = {\"hazard\", \"exposure\", \"vulnerability\", \"loss\"}\n",
    "    rset = {c for c in (components or []) if c in allowed}\n",
    "    \n",
    "    if not rset:\n",
    "        return ComponentGateResult(\n",
    "            ok=False,\n",
    "            reasons=(\"empty_or_unrecognized_components\",),\n",
    "            risk_data_type=[],\n",
    "        )\n",
    "    \n",
    "    reasons: List[str] = []\n",
    "    ok = True\n",
    "    \n",
    "    # Vulnerability requires hazard or exposure\n",
    "    if \"vulnerability\" in rset and not ({\"hazard\", \"exposure\"} & rset):\n",
    "        if config.auto_repair_components:\n",
    "            rset.add(\"exposure\")\n",
    "            reasons.append(\"auto_added_exposure_for_vulnerability\")\n",
    "        else:\n",
    "            ok = False\n",
    "            reasons.append(\"vulnerability_without_hazard_or_exposure\")\n",
    "    \n",
    "    # Loss requires hazard or exposure\n",
    "    if \"loss\" in rset and not ({\"hazard\", \"exposure\"} & rset):\n",
    "        if config.auto_repair_components:\n",
    "            rset.add(\"exposure\")\n",
    "            reasons.append(\"auto_added_exposure_for_loss\")\n",
    "        else:\n",
    "            ok = False\n",
    "            reasons.append(\"loss_without_hazard_or_exposure\")\n",
    "    \n",
    "    return ComponentGateResult(ok=ok, reasons=tuple(reasons), risk_data_type=sorted(rset))\n",
    "\n",
    "\n",
    "print(f\"Component gating configured (auto_repair={config.auto_repair_components})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. RDLS Record Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build RDLS dataset records from HDX metadata.\n",
    "\n",
    "Creates minimal, schema-safe records with:\n",
    "    - Required attributions (publisher, creator, contact)\n",
    "    - Resources with proper format mapping\n",
    "    - Spatial information inferred from groups\n",
    "\"\"\"\n",
    "\n",
    "def build_attributions(hdx: Dict[str, Any], dataset_id: str, dataset_page_url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS attributions from HDX metadata.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx: HDX dataset metadata\n",
    "        dataset_id: Dataset UUID\n",
    "        dataset_page_url: HDX dataset landing page URL\n",
    "        \n",
    "    Returns:\n",
    "        List of attribution objects (minItems=3)\n",
    "    \"\"\"\n",
    "    org = (hdx.get(\"organization\") or \"\").strip() or \"Unknown publisher\"\n",
    "    src = (hdx.get(\"dataset_source\") or \"\").strip() or org\n",
    "    creator_url = src if looks_like_url(src) else dataset_page_url\n",
    "    \n",
    "    return [\n",
    "        {\"id\": \"attribution_publisher\", \"role\": \"publisher\", \"entity\": {\"name\": org, \"url\": dataset_page_url}},\n",
    "        {\"id\": \"attribution_creator\", \"role\": \"creator\", \"entity\": {\"name\": src, \"url\": creator_url}},\n",
    "        {\"id\": \"attribution_contact\", \"role\": \"contact_point\", \"entity\": {\"name\": org, \"url\": dataset_page_url}},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_resources(hdx: Dict[str, Any], dataset_id: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS resources from HDX resources.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx: HDX dataset metadata\n",
    "        dataset_id: Dataset UUID\n",
    "        \n",
    "    Returns:\n",
    "        List of resource objects (minItems=1)\n",
    "    \"\"\"\n",
    "    # Always include HDX metadata export\n",
    "    meta_url = f\"https://data.humdata.org/dataset/{dataset_id}/download_metadata?format=json\"\n",
    "    resources = [{\n",
    "        \"id\": \"hdx_dataset_metadata_json\",\n",
    "        \"title\": \"HDX dataset metadata (JSON)\",\n",
    "        \"description\": \"Dataset-level metadata exported from HDX.\",\n",
    "        \"data_format\": \"JSON (json)\",\n",
    "        \"access_modality\": \"file_download\",\n",
    "        \"download_url\": meta_url,\n",
    "    }]\n",
    "    \n",
    "    for r in hdx.get(\"resources\", []) or []:\n",
    "        rid = (r.get(\"id\") or \"\").strip()\n",
    "        rname = (r.get(\"name\") or \"\").strip() or rid[:8] or \"resource\"\n",
    "        desc = (r.get(\"description\") or \"\").strip() or f\"HDX resource: {rname}\"\n",
    "        dl = (r.get(\"download_url\") or \"\").strip()\n",
    "        fmt = map_data_format(r.get(\"format\") or \"\", dl)\n",
    "        \n",
    "        if not dl or not fmt:\n",
    "            continue\n",
    "        \n",
    "        resources.append({\n",
    "            \"id\": f\"hdx_res_{rid[:8] or slugify_token(rname, 8)}\",\n",
    "            \"title\": rname,\n",
    "            \"description\": desc,\n",
    "            \"data_format\": fmt,\n",
    "            \"access_modality\": \"file_download\",\n",
    "            \"download_url\": dl,\n",
    "        })\n",
    "    \n",
    "    # Deduplicate by id\n",
    "    seen = set()\n",
    "    return [r for r in resources if not (r[\"id\"] in seen or seen.add(r[\"id\"]))]\n",
    "\n",
    "\n",
    "def build_rdls_record(\n",
    "    hdx: Dict[str, Any],\n",
    "    class_row: pd.Series,\n",
    ") -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS record from HDX metadata and classification.\n",
    "    \n",
    "    Parameters:\n",
    "        hdx: HDX dataset metadata\n",
    "        class_row: Classification row from Step 5\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (rdls_record or None if blocked, info dict)\n",
    "    \"\"\"\n",
    "    dataset_id = str(class_row[\"dataset_id\"])\n",
    "    title = (hdx.get(\"title\") or class_row.get(\"title\") or \"\").strip()\n",
    "    notes = (hdx.get(\"notes\") or \"\").strip()\n",
    "    \n",
    "    # Parse and validate components\n",
    "    components = parse_components(class_row.get(\"rdls_components\"))\n",
    "    gate = apply_component_gate(components)\n",
    "    \n",
    "    if not gate.ok:\n",
    "        return None, {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"blocked\": True,\n",
    "            \"blocked_reasons\": \";\".join(gate.reasons),\n",
    "            \"risk_data_type\": \";\".join(gate.risk_data_type),\n",
    "        }\n",
    "    \n",
    "    # Infer spatial from groups\n",
    "    groups = split_semicolon_list(class_row.get(\"groups\"))\n",
    "    spatial = infer_spatial(groups)\n",
    "    \n",
    "    # Infer hazard types for naming\n",
    "    tags = split_semicolon_list(class_row.get(\"tags\"))\n",
    "    hazard_types = infer_hazard_types(tags, title=title, notes=notes)\n",
    "    \n",
    "    dataset_page_url = f\"https://data.humdata.org/dataset/{dataset_id}\"\n",
    "    \n",
    "    # Build entity token for naming\n",
    "    hdx_slug = slugify_token(str(hdx.get(\"name\") or \"\"), max_len=48)\n",
    "    title_slug = slugify_token(title, max_len=48)\n",
    "    dataset_slug = hdx_slug if hdx_slug != \"unknown\" else title_slug\n",
    "    \n",
    "    org_token = slugify_token(str(class_row.get(\"organization\") or hdx.get(\"organization\") or \"unknown\"), max_len=20)\n",
    "    iso3_tok = str(spatial[\"countries\"][0]).lower() if spatial.get(\"countries\") and len(spatial[\"countries\"]) == 1 else \"\"\n",
    "    \n",
    "    # Compose identifier\n",
    "    parts = [p for p in [org_token, iso3_tok, dataset_slug] if p]\n",
    "    entity_token = \"_\".join(parts)\n",
    "    \n",
    "    prefix = choose_prefix(gate.risk_data_type) + \"hdx_\"\n",
    "    hz_suffix = hazard_suffix_for_filename(hazard_types) if ({\"hazard\", \"loss\"} & set(gate.risk_data_type)) else \"\"\n",
    "    \n",
    "    stem_base = f\"{prefix}{entity_token}{hz_suffix}\"\n",
    "    stem = stem_base\n",
    "    \n",
    "    # Collision-proofing\n",
    "    out_path = OUT_RECORDS_DIR / f\"{stem}.json\"\n",
    "    if out_path.exists():\n",
    "        stem = f\"{stem_base}__{dataset_id[:8]}\"\n",
    "    \n",
    "    # Map license\n",
    "    license_raw = str(class_row.get(\"license_title\") or hdx.get(\"license_title\") or \"\").strip()\n",
    "    license_mapped = map_license(license_raw or \"Custom\")\n",
    "    \n",
    "    # Build RDLS dataset record\n",
    "    rdls_ds: Dict[str, Any] = {\n",
    "        \"id\": stem,\n",
    "        \"title\": title or f\"HDX dataset {dataset_id}\",\n",
    "        \"description\": notes or None,\n",
    "        \"risk_data_type\": gate.risk_data_type,\n",
    "        \"spatial\": spatial,\n",
    "        \"license\": license_mapped,\n",
    "        \"attributions\": build_attributions(hdx, dataset_id, dataset_page_url),\n",
    "        \"resources\": build_resources(hdx, dataset_id),\n",
    "        \"links\": [{\n",
    "            \"href\": rdls_schema.get(\"$id\") or \"https://docs.riskdatalibrary.org/en/latest/reference/rdls_schema/\",\n",
    "            \"rel\": \"describedby\",\n",
    "        }],\n",
    "    }\n",
    "    \n",
    "    # Remove None values and filter to allowed keys\n",
    "    rdls_ds = {k: v for k, v in rdls_ds.items() if v is not None and k in RDLS_ALLOWED_KEYS}\n",
    "    \n",
    "    # Wrap in top-level structure\n",
    "    rdls_record = {\"datasets\": [rdls_ds]}\n",
    "    \n",
    "    info = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"rdls_id\": stem,\n",
    "        \"filename\": f\"{stem}.json\",\n",
    "        \"risk_data_type\": \";\".join(gate.risk_data_type),\n",
    "        \"spatial_scale\": spatial.get(\"scale\", \"\"),\n",
    "        \"countries_count\": len(spatial.get(\"countries\", []) or []),\n",
    "        \"license_raw\": license_raw,\n",
    "        \"orgtoken\": org_token,\n",
    "        \"hazard_suffix\": hz_suffix.lstrip(\"_\"),\n",
    "        \"organization_token\": org_token,\n",
    "        \"iso3\": iso3_tok,\n",
    "        \"hazard_types\": \";\".join(hazard_types),\n",
    "        \"blocked\": False,\n",
    "        \"blocked_reasons\": \"\",\n",
    "    }\n",
    "    \n",
    "    return rdls_record, info\n",
    "\n",
    "\n",
    "print(\"Record builder functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Clean Previous Outputs (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean previous outputs if configured.\n",
    "\n",
    "Only cleans when OUTPUT_MODE='in_place' and CLEAN_BEFORE_RUN=True.\n",
    "Includes safety guardrails to prevent accidental deletion.\n",
    "\"\"\"\n",
    "\n",
    "def _safe_clean_folder(folder: Path, pattern: str) -> int:\n",
    "    \"\"\"\n",
    "    Safely delete files matching pattern in folder.\n",
    "    \n",
    "    Parameters:\n",
    "        folder: Directory to clean\n",
    "        pattern: Glob pattern for files to delete\n",
    "        \n",
    "    Returns:\n",
    "        Number of files deleted\n",
    "    \"\"\"\n",
    "    folder = folder.resolve()\n",
    "    \n",
    "    # Safety: only clean inside RDLS_DIR\n",
    "    if not str(folder).startswith(str(RDLS_DIR)):\n",
    "        raise ValueError(f\"Refusing to clean outside rdls/: {folder}\")\n",
    "    \n",
    "    # Safety: only allow expected folder names\n",
    "    if folder.name not in {\"records\", \"index\", \"reports\"}:\n",
    "        raise ValueError(f\"Unexpected folder name for cleaning: {folder.name}\")\n",
    "    \n",
    "    n = 0\n",
    "    for f in folder.glob(pattern):\n",
    "        try:\n",
    "            f.unlink()\n",
    "            n += 1\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: failed to delete {f}: {e}\")\n",
    "    return n\n",
    "\n",
    "\n",
    "if config.output_mode == \"in_place\" and config.clean_before_run:\n",
    "    print(\"Cleaning previous outputs...\")\n",
    "    removed_records = _safe_clean_folder(OUT_RECORDS_DIR, \"*.json\")\n",
    "    removed_index = _safe_clean_folder(OUT_INDEX_DIR, \"*.jsonl\")\n",
    "    removed_reports = _safe_clean_folder(OUT_REPORTS_DIR, \"*.csv\")\n",
    "    print(f\"Cleaned: records={removed_records}, index={removed_index}, reports={removed_reports}\")\n",
    "else:\n",
    "    print(\"No cleaning performed (clean_before_run=False or output_mode=run_folder)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9-header",
   "metadata": {},
   "source": [
    "## 9. Validate and Write Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process all datasets: build records, validate, and write outputs.\n",
    "\n",
    "Outputs:\n",
    "    - Individual RDLS JSON records\n",
    "    - Index JSONL file\n",
    "    - Blocked datasets report\n",
    "    - Validation results report\n",
    "    - QA summary report\n",
    "\"\"\"\n",
    "\n",
    "# --- JSON Schema validation setup ---\n",
    "def try_import_jsonschema():\n",
    "    try:\n",
    "        import jsonschema\n",
    "        return jsonschema\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "_jsonschema = try_import_jsonschema()\n",
    "validator = None\n",
    "\n",
    "if _jsonschema is not None:\n",
    "    try:\n",
    "        validator = _jsonschema.Draft202012Validator(rdls_schema)\n",
    "        print(\"jsonschema validation enabled (Draft2020-12)\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: jsonschema init failed: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: jsonschema not installed; validation will be skipped\")\n",
    "\n",
    "\n",
    "def validate_record(rec: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    \"\"\"Validate RDLS record against schema.\"\"\"\n",
    "    if validator is None:\n",
    "        return True, \"\"\n",
    "    errors = sorted(validator.iter_errors(rec[\"datasets\"][0]), key=lambda e: e.path)\n",
    "    if not errors:\n",
    "        return True, \"\"\n",
    "    msgs = [f\"{'.'.join(str(p) for p in e.path)}: {e.message}\" for e in errors[:10]]\n",
    "    return False, \" | \".join(msgs)\n",
    "\n",
    "\n",
    "def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    \"\"\"Append object to JSONL file.\"\"\"\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# Initialize output file\n",
    "OUT_INDEX_JSONL.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "# Tracking lists\n",
    "blocked_rows: List[Dict[str, Any]] = []\n",
    "validation_rows: List[Dict[str, Any]] = []\n",
    "qa_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "# Counters\n",
    "written = 0\n",
    "skipped_existing = 0\n",
    "blocked = 0\n",
    "validated_ok = 0\n",
    "\n",
    "print(f\"\\nProcessing {len(included_ids):,} datasets...\")\n",
    "\n",
    "for dataset_id in tqdm(included_ids, desc=\"Translating to RDLS\"):\n",
    "    # Find dataset file\n",
    "    fp = dataset_file_index.get(dataset_id)\n",
    "    \n",
    "    if fp is None or not fp.exists():\n",
    "        blocked += 1\n",
    "        blocked_rows.append({\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"status\": \"blocked_missing_hdx_dataset_json\",\n",
    "            \"reason\": \"missing_hdx_dataset_json\",\n",
    "            \"risk_data_type\": \"\",\n",
    "        })\n",
    "        qa_rows.append({\n",
    "            \"dataset_id\": dataset_id, \"output_id\": \"\", \"filename\": \"\",\n",
    "            \"risk_data_type\": \"\", \"spatial_scale\": \"\", \"countries_count\": 0,\n",
    "            \"license_raw\": \"\", \"orgtoken\": \"\", \"hazard_suffix\": \"\",\n",
    "            \"status\": \"blocked_missing_hdx_dataset_json\", \"reason\": \"missing_hdx_dataset_json\",\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Load HDX metadata\n",
    "    hdx = safe_load_json(fp)\n",
    "    row = df.loc[dataset_id]\n",
    "    \n",
    "    # Build RDLS record\n",
    "    rdls_rec, info = build_rdls_record(hdx, row)\n",
    "    \n",
    "    if rdls_rec is None:\n",
    "        blocked += 1\n",
    "        reason = info.get(\"blocked_reasons\") or \"blocked_by_policy\"\n",
    "        rdt = info.get(\"risk_data_type\") or \"\"\n",
    "        blocked_rows.append({\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"status\": \"blocked_by_policy\",\n",
    "            \"reason\": reason,\n",
    "            \"risk_data_type\": rdt,\n",
    "        })\n",
    "        qa_rows.append({\n",
    "            \"dataset_id\": dataset_id, \"output_id\": \"\", \"filename\": \"\",\n",
    "            \"risk_data_type\": rdt, \"spatial_scale\": \"\", \"countries_count\": 0,\n",
    "            \"license_raw\": \"\", \"orgtoken\": \"\", \"hazard_suffix\": \"\",\n",
    "            \"status\": \"blocked_by_policy\", \"reason\": reason,\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    out_path = OUT_RECORDS_DIR / info[\"filename\"]\n",
    "    \n",
    "    # Skip if exists and configured\n",
    "    if config.skip_existing and out_path.exists():\n",
    "        skipped_existing += 1\n",
    "        qa_rows.append({\n",
    "            \"dataset_id\": dataset_id, \"output_id\": info.get(\"rdls_id\", \"\"),\n",
    "            \"filename\": info.get(\"filename\", \"\"), \"risk_data_type\": info.get(\"risk_data_type\", \"\"),\n",
    "            \"spatial_scale\": info.get(\"spatial_scale\", \"\"), \"countries_count\": info.get(\"countries_count\", 0),\n",
    "            \"license_raw\": info.get(\"license_raw\", \"\"), \"orgtoken\": info.get(\"orgtoken\", \"\"),\n",
    "            \"hazard_suffix\": info.get(\"hazard_suffix\", \"\"),\n",
    "            \"status\": \"skipped_existing\", \"reason\": \"\",\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Validate\n",
    "    ok, msg = validate_record(rdls_rec)\n",
    "    validation_rows.append({\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"rdls_id\": info[\"rdls_id\"],\n",
    "        \"filename\": info[\"filename\"],\n",
    "        \"valid\": ok,\n",
    "        \"message\": msg,\n",
    "    })\n",
    "    if ok:\n",
    "        validated_ok += 1\n",
    "    \n",
    "    # Write JSON\n",
    "    if config.write_pretty_json:\n",
    "        out_path.write_text(json.dumps(rdls_rec, indent=2, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        out_path.write_text(json.dumps(rdls_rec, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "    \n",
    "    append_jsonl(OUT_INDEX_JSONL, info)\n",
    "    written += 1\n",
    "    \n",
    "    qa_rows.append({\n",
    "        \"dataset_id\": dataset_id, \"output_id\": info.get(\"rdls_id\", \"\"),\n",
    "        \"filename\": info.get(\"filename\", \"\"), \"risk_data_type\": info.get(\"risk_data_type\", \"\"),\n",
    "        \"spatial_scale\": info.get(\"spatial_scale\", \"\"), \"countries_count\": info.get(\"countries_count\", 0),\n",
    "        \"license_raw\": info.get(\"license_raw\", \"\"), \"orgtoken\": info.get(\"orgtoken\", \"\"),\n",
    "        \"hazard_suffix\": info.get(\"hazard_suffix\", \"\"),\n",
    "        \"status\": \"written\", \"reason\": \"\",\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"TRANSLATION COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Written: {written:,}\")\n",
    "print(f\"Skipped (existing): {skipped_existing:,}\")\n",
    "print(f\"Blocked: {blocked:,}\")\n",
    "print(f\"Schema valid: {validated_ok:,} of {len(validation_rows):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10-header",
   "metadata": {},
   "source": [
    "## 10. Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-reports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save translation reports to CSV files.\n",
    "\"\"\"\n",
    "\n",
    "# Save blocked datasets report\n",
    "blocked_df = pd.DataFrame(blocked_rows, columns=[\"dataset_id\", \"status\", \"reason\", \"risk_data_type\"])\n",
    "blocked_df.to_csv(OUT_BLOCKED_CSV, index=False)\n",
    "print(f\"Wrote: {OUT_BLOCKED_CSV}\")\n",
    "\n",
    "# Save validation report\n",
    "val_df = pd.DataFrame(validation_rows, columns=[\"dataset_id\", \"rdls_id\", \"filename\", \"valid\", \"message\"])\n",
    "val_df.to_csv(OUT_VALIDATION_CSV, index=False)\n",
    "print(f\"Wrote: {OUT_VALIDATION_CSV}\")\n",
    "\n",
    "# Save QA report\n",
    "qa_df = pd.DataFrame(qa_rows, columns=[\n",
    "    \"dataset_id\", \"output_id\", \"filename\", \"risk_data_type\",\n",
    "    \"spatial_scale\", \"countries_count\", \"license_raw\", \"orgtoken\",\n",
    "    \"hazard_suffix\", \"status\", \"reason\",\n",
    "])\n",
    "qa_df.to_csv(OUT_QA_CSV, index=False)\n",
    "print(f\"Wrote: {OUT_QA_CSV}\")\n",
    "\n",
    "print(f\"\\nIndex file: {OUT_INDEX_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11-header",
   "metadata": {},
   "source": [
    "## 11. QA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11-qa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quick QA summary of translation results.\n",
    "\"\"\"\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV safely, returning empty DataFrame if file is empty.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except EmptyDataError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"QA SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Index stats\n",
    "idx_lines = OUT_INDEX_JSONL.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "print(f\"Index lines: {len(idx_lines):,}\")\n",
    "print(f\"Records on disk: {len(list(OUT_RECORDS_DIR.glob('*.json'))):,}\")\n",
    "\n",
    "# Blocked stats\n",
    "if OUT_BLOCKED_CSV.exists():\n",
    "    blocked_df = safe_read_csv(OUT_BLOCKED_CSV)\n",
    "    print(f\"\\nBlocked datasets: {len(blocked_df):,}\")\n",
    "    if not blocked_df.empty and \"reason\" in blocked_df.columns:\n",
    "        print(\"Top blocked reasons:\")\n",
    "        for reason, count in blocked_df[\"reason\"].value_counts().head(5).items():\n",
    "            print(f\"  - {reason}: {count:,}\")\n",
    "\n",
    "# Validation stats\n",
    "if OUT_VALIDATION_CSV.exists():\n",
    "    val_df = safe_read_csv(OUT_VALIDATION_CSV)\n",
    "    if not val_df.empty:\n",
    "        failures = val_df.loc[val_df[\"valid\"] == False, \"message\"]\n",
    "        print(f\"\\nValidation failures: {len(failures):,}\")\n",
    "        if len(failures) > 0:\n",
    "            print(\"Top validation errors:\")\n",
    "            for msg, count in failures.value_counts().head(5).items():\n",
    "                print(f\"  - {msg[:60]}...: {count:,}\")\n",
    "\n",
    "# QA status\n",
    "if OUT_QA_CSV.exists():\n",
    "    qa_df = safe_read_csv(OUT_QA_CSV)\n",
    "    if not qa_df.empty and \"status\" in qa_df.columns:\n",
    "        print(f\"\\nQA Status Distribution:\")\n",
    "        for status, count in qa_df[\"status\"].value_counts().items():\n",
    "            print(f\"  - {status}: {count:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Step 6 complete. Proceed to Step 7 for validation and packaging.\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
