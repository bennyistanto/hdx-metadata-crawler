{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2004834e",
   "metadata": {},
   "source": [
    "# Step 6 - Translate HDX metadata to RDLS v0.3 JSON\n",
    "\n",
    "This notebook translates **HDX dataset-level metadata exports** into **RDLS v0.3** metadata records.\n",
    "\n",
    "## Inputs\n",
    "- Step 5 outputs:\n",
    "  - `hdx_dataset_metadata_dump/derived/classification_final.csv`\n",
    "  - `hdx_dataset_metadata_dump/derived/rdls_included_dataset_ids_final.txt`\n",
    "- HDX dataset metadata JSON corpus:\n",
    "  - `hdx_dataset_metadata_dump/dataset_metadata/*.json`\n",
    "- RDLS v0.3 assets (provide local paths below):\n",
    "  - `rdls_schema_v0.3.json`\n",
    "  - `rdls_template_v03.json`\n",
    "\n",
    "## Outputs\n",
    "- `hdx_dataset_metadata_dump/rdls/records/*.json` — one RDLS record per included HDX dataset\n",
    "- `hdx_dataset_metadata_dump/rdls/index/rdls_index.jsonl` — index of written records\n",
    "- `hdx_dataset_metadata_dump/rdls/reports/translation_blocked.csv` — datasets blocked by policy/required-field gaps\n",
    "- `hdx_dataset_metadata_dump/rdls/reports/schema_validation.csv` — JSON Schema validation results (pass/fail)\n",
    "\n",
    "## Strictness & policy\n",
    "- **Schema-first:** required RDLS fields are always populated; optional fields are omitted unless we can fill them safely.\n",
    "- **No extra fields:** the output contains **only fields defined in the RDLS schema**.\n",
    "- **Do not invent content:** if a value cannot be mapped from HDX, we **leave the RDLS optional field absent** (not an empty string), to avoid violating schema `minLength` constraints.\n",
    "- **Open codelists:** for schema fields marked `openCodelist: true`, values may be kept as-is if not in suggestions.\n",
    "- **Component combination rule (your team policy):**\n",
    "  - hazard-only and exposure-only are allowed\n",
    "  - vulnerability must accompany hazard or exposure\n",
    "  - loss must accompany hazard or exposure\n",
    "  - if this is violated, the dataset is **blocked** (until resolved via overrides in Step 5)\n",
    "\n",
    "## Naming convention\n",
    "The RDLS `datasets[0].id` equals the output filename stem, following:\n",
    "\n",
    "`{prefix}{entity_token}{optional_hazard_suffix}.json`\n",
    "\n",
    "Prefix precedence:\n",
    "- loss → `rdls_lss-`\n",
    "- vulnerability → `rdls_vln-`\n",
    "- exposure → `rdls_exp-`\n",
    "- hazard → `rdls_hzd-`\n",
    "\n",
    "Collision-proofing:\n",
    "- if a filename already exists, append `__hdx_{dataset_uuid[:8]}`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90cea5-6f6a-4c33-b07e-175eede83037",
   "metadata": {},
   "source": [
    "### Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b039f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned in-place: records=50, index=1, reports=3\n",
      "DUMP_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\n",
      "DATASET_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\dataset_metadata\n",
      "RDLS_SCHEMA_PATH: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\schema\\rdls_schema_v0.3.json\n",
      "RDLS_TEMPLATE_PATH: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\template\\rdls_template_v03.json\n",
      "OUTPUT_MODE: in_place\n",
      "RUN_ID: in_place\n",
      "RDLS_RUN_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\n",
      "OUT_RECORDS_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\records\n",
      "OUT_INDEX_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\index\n",
      "OUT_REPORTS_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Configuration\n",
    "# ======================\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Paths (assumes you run this notebook from the `notebook/` folder) ---\n",
    "DUMP_DIR = (Path(\"..\") / \"hdx_dataset_metadata_dump\").resolve()\n",
    "DATASET_DIR = (DUMP_DIR / \"dataset_metadata\").resolve()\n",
    "\n",
    "DERIVED_DIR = (DUMP_DIR / \"derived\").resolve()\n",
    "CLASSIFICATION_FINAL_CSV = (DERIVED_DIR / \"classification_final.csv\").resolve()\n",
    "INCLUDED_IDS_TXT = (DERIVED_DIR / \"rdls_included_dataset_ids_final.txt\").resolve()\n",
    "\n",
    "# --- RDLS assets (edit if you store them elsewhere) ---\n",
    "# Recommended folder layout:\n",
    "#   hdx_dataset_metadata_dump/rdls/schema/rdls_schema_v0.3.json\n",
    "#   hdx_dataset_metadata_dump/rdls/template/rdls_template_v03.json\n",
    "RDLS_DIR = (DUMP_DIR / \"rdls\").resolve()\n",
    "RDLS_SCHEMA_PATH = (RDLS_DIR / \"schema\" / \"rdls_schema_v0.3.json\").resolve()\n",
    "RDLS_TEMPLATE_PATH = (RDLS_DIR / \"template\" / \"rdls_template_v03.json\").resolve()\n",
    "\n",
    "# =========================\n",
    "# Output mode configuration\n",
    "# =========================\n",
    "# Choose ONE:\n",
    "# - \"run_folder\": write to rdls/runs/<RUN_ID>/{records,index,reports}\n",
    "# - \"in_place\"  : write to rdls/{records,index,reports}\n",
    "OUTPUT_MODE = \"in_place\"  # \"run_folder\" | \"in_place\"\n",
    "\n",
    "# If OUTPUT_MODE == \"in_place\", optionally clean existing outputs before writing\n",
    "CLEAN_IN_PLACE_BEFORE_RUN = True  # True | False\n",
    "\n",
    "# Safety: never allow cleaning when using run_folder mode\n",
    "if OUTPUT_MODE == \"run_folder\" and CLEAN_IN_PLACE_BEFORE_RUN:\n",
    "    raise ValueError(\"Invalid config: CLEAN_IN_PLACE_BEFORE_RUN cannot be True when OUTPUT_MODE='run_folder'\")\n",
    "\n",
    "# Resolve run root (either rdls/ or rdls/runs/<RUN_ID>/)\n",
    "if OUTPUT_MODE == \"run_folder\":\n",
    "    RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    RDLS_RUN_DIR = (RDLS_DIR / \"runs\" / RUN_ID).resolve()\n",
    "else:\n",
    "    RUN_ID = \"in_place\"\n",
    "    RDLS_RUN_DIR = RDLS_DIR\n",
    "\n",
    "# --- Outputs (under the chosen run directory) ---\n",
    "OUT_RECORDS_DIR = RDLS_RUN_DIR / \"records\"\n",
    "OUT_INDEX_DIR = RDLS_RUN_DIR / \"index\"\n",
    "OUT_REPORTS_DIR = RDLS_RUN_DIR / \"reports\"\n",
    "\n",
    "OUT_INDEX_JSONL = OUT_INDEX_DIR / \"rdls_index.jsonl\"\n",
    "OUT_BLOCKED_CSV = OUT_REPORTS_DIR / \"translation_blocked.csv\"\n",
    "OUT_VALIDATION_CSV = OUT_REPORTS_DIR / \"schema_validation.csv\"\n",
    "OUT_QA_CSV = OUT_REPORTS_DIR / \"translation_qa.csv\"\n",
    "\n",
    "# --- Runtime controls ---\n",
    "# MAX_DATASETS: Optional[int] = None     # set e.g. 200 for a test run\n",
    "MAX_DATASETS = 50\n",
    "SKIP_EXISTING: bool = False             # True (Default) resume-safe\n",
    "WRITE_PRETTY_JSON: bool = True          # pretty-print for readability (slower, larger)\n",
    "\n",
    "# --- Hazard inference / filename alias ---\n",
    "# RDLS hazard types are fixed enums (e.g., \"strong_wind\"). Filenames can use friendlier aliases if you want.\n",
    "HAZARD_FILENAME_ALIASES = {\n",
    "    \"strong_wind\": \"windstorm\",  # your example preference\n",
    "    # Keep others as-is by default; add aliases if you want:\n",
    "    # \"extreme_temperature\": \"extreme_temperature\",\n",
    "    # \"coastal_flood\": \"coastal_flood\",\n",
    "}\n",
    "\n",
    "# --- Resource format mapping (HDX -> RDLS data_format enum label) ---\n",
    "HDX_FORMAT_TO_RDLS = {\n",
    "    \"CSV\": \"CSV (csv)\",\n",
    "    \"XLS\": \"Excel (xlsx)\",\n",
    "    \"XLSX\": \"Excel (xlsx)\",\n",
    "    \"EXCEL\": \"Excel (xlsx)\",\n",
    "    \"JSON\": \"JSON (json)\",\n",
    "    \"GEOJSON\": \"GeoJSON (geojson)\",\n",
    "    \"SHP\": \"Shapefile (shp)\",\n",
    "    \"SHAPEFILE\": \"Shapefile (shp)\",\n",
    "    \"GPKG\": \"GeoPackage (gpkg)\",\n",
    "    \"GEOPACKAGE\": \"GeoPackage (gpkg)\",\n",
    "    \"KML\": \"KML (kml)\",\n",
    "    \"PDF\": \"PDF (pdf)\",\n",
    "    \"NC\": \"NetCDF (nc)\",\n",
    "    \"NETCDF\": \"NetCDF (nc)\",\n",
    "    \"TIF\": \"GeoTIFF (tif)\",\n",
    "    \"TIFF\": \"GeoTIFF (tif)\",\n",
    "    \"COG\": \"Cloud Optimized GeoTIFF (cog)\",\n",
    "    \"PARQUET\": \"Parquet (parquet)\",\n",
    "    \"XML\": \"XML (xml)\",\n",
    "}\n",
    "\n",
    "# --- License mapping (HDX license_title -> preferred RDLS suggestions when possible) ---\n",
    "HDX_LICENSE_TO_RDLS = {\n",
    "    \"public domain\": \"PDDL-1.0\",\n",
    "    \"odbl\": \"ODbL-1.0\",\n",
    "    \"cc-by-4.0\": \"CC-BY-4.0\",\n",
    "    \"cc by 4.0\": \"CC-BY-4.0\",\n",
    "    \"cc-by\": \"CC-BY-4.0\",\n",
    "    \"cc0\": \"CC0-1.0\",\n",
    "    \"cc0-1.0\": \"CC0-1.0\",\n",
    "    \"copyright\": \"Copyright\",\n",
    "}\n",
    "\n",
    "# Ensure output folders exist\n",
    "for p in [OUT_RECORDS_DIR, OUT_INDEX_DIR, OUT_REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _safe_clean_folder(folder: Path, pattern: str) -> int:\n",
    "    \"\"\"\n",
    "    Delete files matching pattern inside an expected RDLS subfolder.\n",
    "    Guardrails:\n",
    "      - only cleans inside RDLS_DIR\n",
    "      - only allows folder names: records/index/reports\n",
    "    \"\"\"\n",
    "    folder = folder.resolve()\n",
    "    if not str(folder).startswith(str(RDLS_DIR)):\n",
    "        raise ValueError(f\"Refusing to clean outside rdls/: {folder}\")\n",
    "    if folder.name not in {\"records\", \"index\", \"reports\"}:\n",
    "        raise ValueError(f\"Unexpected folder name for cleaning: {folder.name}\")\n",
    "    n = 0\n",
    "    for f in folder.glob(pattern):\n",
    "        try:\n",
    "            f.unlink()\n",
    "            n += 1\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: failed to delete {f}: {e}\")\n",
    "    return n\n",
    "\n",
    "if OUTPUT_MODE == \"in_place\" and CLEAN_IN_PLACE_BEFORE_RUN:\n",
    "    removed_records = _safe_clean_folder(OUT_RECORDS_DIR, \"*.json\")\n",
    "    removed_index = _safe_clean_folder(OUT_INDEX_DIR, \"*.jsonl\")\n",
    "    removed_reports = _safe_clean_folder(OUT_REPORTS_DIR, \"*.csv\")\n",
    "    print(f\"Cleaned in-place: records={removed_records}, index={removed_index}, reports={removed_reports}\")\n",
    "else:\n",
    "    print(\"No cleaning performed.\")\n",
    "\n",
    "print(\"DUMP_DIR:\", DUMP_DIR)\n",
    "print(\"DATASET_DIR:\", DATASET_DIR)\n",
    "print(\"RDLS_SCHEMA_PATH:\", RDLS_SCHEMA_PATH)\n",
    "print(\"RDLS_TEMPLATE_PATH:\", RDLS_TEMPLATE_PATH)\n",
    "print(\"OUTPUT_MODE:\", OUTPUT_MODE)\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"RDLS_RUN_DIR:\", RDLS_RUN_DIR)\n",
    "print(\"OUT_RECORDS_DIR:\", OUT_RECORDS_DIR)\n",
    "print(\"OUT_INDEX_DIR:\", OUT_INDEX_DIR)\n",
    "print(\"OUT_REPORTS_DIR:\", OUT_REPORTS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b330b08-4d81-482e-b1d5-02cf164bf3eb",
   "metadata": {},
   "source": [
    "### Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a879319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDLS required keys: ['id', 'title', 'risk_data_type', 'attributions', 'spatial', 'license', 'resources']\n",
      "RDLS allowed keys count: 20\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Load RDLS schema + template (and required field list)\n",
    "# ======================\n",
    "from copy import deepcopy\n",
    "\n",
    "if not RDLS_SCHEMA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"RDLS schema not found: {RDLS_SCHEMA_PATH}\")\n",
    "\n",
    "if not RDLS_TEMPLATE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"RDLS template not found: {RDLS_TEMPLATE_PATH}\")\n",
    "\n",
    "rdls_schema = json.loads(RDLS_SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "rdls_template = json.loads(RDLS_TEMPLATE_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# RDLS dataset object allowed keys (schema properties)\n",
    "RDLS_ALLOWED_KEYS = set(rdls_schema[\"properties\"].keys())\n",
    "\n",
    "# Required keys for a dataset record (from schema)\n",
    "RDLS_REQUIRED_KEYS = list(rdls_schema.get(\"required\", []))\n",
    "\n",
    "print(\"RDLS required keys:\", RDLS_REQUIRED_KEYS)\n",
    "print(\"RDLS allowed keys count:\", len(RDLS_ALLOWED_KEYS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f6d28-f4e9-47c5-a44b-eca64e906f10",
   "metadata": {},
   "source": [
    "### Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a722c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_final rows: 26246\n",
      "included ids: 10759\n",
      "included ids present in classification: 10759\n",
      "dataset files indexed: 26246\n",
      "TEST RUN: MAX_DATASETS = 50 -> using 50 ids\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Load Step 5 outputs + build dataset_id -> JSON path index\n",
    "# ======================\n",
    "def read_ids_txt(path: Path) -> List[str]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing IDs list: {path}\")\n",
    "    out: List[str] = []\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        s = line.strip()\n",
    "        if s:\n",
    "            out.append(s)\n",
    "    return out\n",
    "\n",
    "if not CLASSIFICATION_FINAL_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing Step 5 output: {CLASSIFICATION_FINAL_CSV}\")\n",
    "if not INCLUDED_IDS_TXT.exists():\n",
    "    raise FileNotFoundError(f\"Missing Step 5 output: {INCLUDED_IDS_TXT}\")\n",
    "if not DATASET_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing HDX dataset metadata folder: {DATASET_DIR}\")\n",
    "\n",
    "df = pd.read_csv(CLASSIFICATION_FINAL_CSV)\n",
    "included_ids = read_ids_txt(INCLUDED_IDS_TXT)\n",
    "\n",
    "# Fast lookups\n",
    "df = df.set_index(\"dataset_id\", drop=False)\n",
    "included_set = set(included_ids)\n",
    "\n",
    "print(\"classification_final rows:\", len(df))\n",
    "print(\"included ids:\", len(included_ids))\n",
    "print(\"included ids present in classification:\", sum(did in df.index for did in included_ids))\n",
    "\n",
    "# Build a dataset_id -> file path mapping once (avoid N=10k glob calls)\n",
    "dataset_file_index: Dict[str, Path] = {}\n",
    "for fp in sorted(DATASET_DIR.glob(\"*.json\")):\n",
    "    # expected filename pattern: {dataset_uuid}__{slug}.json\n",
    "    stem = fp.stem\n",
    "    if \"__\" in stem:\n",
    "        dataset_uuid = stem.split(\"__\", 1)[0]\n",
    "    else:\n",
    "        dataset_uuid = stem\n",
    "    dataset_file_index[dataset_uuid] = fp\n",
    "\n",
    "print(\"dataset files indexed:\", len(dataset_file_index))\n",
    "\n",
    "# Optional: limit for testing\n",
    "if MAX_DATASETS is not None:\n",
    "    included_ids = included_ids[:MAX_DATASETS]\n",
    "    included_set = set(included_ids)\n",
    "    print(\"TEST RUN: MAX_DATASETS =\", MAX_DATASETS, \"-> using\", len(included_ids), \"ids\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb7108-df15-4206-b41f-92932cab82e3",
   "metadata": {},
   "source": [
    "### Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8694c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Helpers: parsing, slugifying, hazard inference, mappings\n",
    "# ======================\n",
    "def slugify_token(s: str, max_len: int = 32) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    if not s:\n",
    "        return \"unknown\"\n",
    "    return s[:max_len].strip(\"_\") or \"unknown\"\n",
    "\n",
    "def split_semicolon_list(s: Any) -> List[str]:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return []\n",
    "    if isinstance(s, list):\n",
    "        return [str(x).strip() for x in s if str(x).strip()]\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    return [x.strip() for x in re.split(r\"[;,]\", s) if x.strip()]\n",
    "\n",
    "def looks_like_url(s: str) -> bool:\n",
    "    return bool(re.match(r\"^https?://\", (s or \"\").strip(), flags=re.I))\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# --- ISO3 inference (best-effort, no external dependencies required) ---\n",
    "def try_import_pycountry():\n",
    "    try:\n",
    "        import pycountry  # type: ignore\n",
    "        return pycountry\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_pycountry = try_import_pycountry()\n",
    "\n",
    "COMMON_COUNTRY_FIXES = {\n",
    "    \"cote d'ivoire\": \"CIV\",\n",
    "    \"ivory coast\": \"CIV\",\n",
    "    \"democratic republic of the congo\": \"COD\",\n",
    "    \"dr congo\": \"COD\",\n",
    "    \"republic of the congo\": \"COG\",\n",
    "    \"congo, rep.\": \"COG\",\n",
    "    \"congo, dem. rep.\": \"COD\",\n",
    "    \"lao pdr\": \"LAO\",\n",
    "    \"viet nam\": \"VNM\",\n",
    "    \"korea, rep.\": \"KOR\",\n",
    "    \"korea, dem. rep.\": \"PRK\",\n",
    "    \"syrian arab republic\": \"SYR\",\n",
    "    \"iran, islamic republic of\": \"IRN\",\n",
    "    \"tanzania, united republic of\": \"TZA\",\n",
    "    \"venezuela, bolivarian republic of\": \"VEN\",\n",
    "    \"bolivia, plurinational state of\": \"BOL\",\n",
    "    \"moldova, republic of\": \"MDA\",\n",
    "    \"palestine\": \"PSE\",\n",
    "    \"russia\": \"RUS\",\n",
    "    \"united states\": \"USA\",\n",
    "    \"united kingdom\": \"GBR\",\n",
    "}\n",
    "\n",
    "\n",
    "# Optional: a lightweight country name -> ISO3 table persisted to disk.\n",
    "# This avoids forcing `pycountry` at runtime while still enabling country inference at scale.\n",
    "COUNTRY_ISO3_CSV = (DUMP_DIR / \"config\" / \"country_name_to_iso3.csv\")\n",
    "\n",
    "def _norm_country_key(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r\"[\\(\\)\\[\\]\\{\\}\\.\\,\\;\\:]\", \" \", s)\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    s = re.sub(r\"[^a-z0-9\\s\\-']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "COMMON_COUNTRY_FIXES_NORM: Dict[str, str] = {_norm_country_key(k): v for k, v in COMMON_COUNTRY_FIXES.items()}\n",
    "\n",
    "def load_country_iso3_table(path: Path) -> Dict[str, str]:\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    try:\n",
    "        df_iso = pd.read_csv(path)\n",
    "    except Exception:\n",
    "        return {}\n",
    "    cols = {c.lower(): c for c in df_iso.columns}\n",
    "    name_col = cols.get(\"name\") or cols.get(\"country\") or cols.get(\"country_name\")\n",
    "    iso3_col = cols.get(\"iso3\") or cols.get(\"alpha_3\") or cols.get(\"code\")\n",
    "    if not name_col or not iso3_col:\n",
    "        return {}\n",
    "    out: Dict[str, str] = {}\n",
    "    for _, r in df_iso.iterrows():\n",
    "        name = str(r.get(name_col, \"\")).strip()\n",
    "        iso3 = str(r.get(iso3_col, \"\")).strip().upper()\n",
    "        if name and iso3 and len(iso3) == 3:\n",
    "            out[_norm_country_key(name)] = iso3\n",
    "    return out\n",
    "\n",
    "COUNTRY_ISO3_TABLE: Dict[str, str] = load_country_iso3_table(COUNTRY_ISO3_CSV)\n",
    "\n",
    "def maybe_generate_country_iso3_table(path: Path) -> None:\n",
    "    \"\"\"Generate a country name -> ISO3 table if pycountry is available and file is absent.\"\"\"\n",
    "    if path.exists():\n",
    "        return\n",
    "    if _pycountry is None:\n",
    "        print(\n",
    "            \"NOTE: pycountry not installed; country inference may default to 'global'. \"\n",
    "            \"To enable ISO3 mapping, install pycountry or provide: \" + str(path)\n",
    "        )\n",
    "        return\n",
    "    rows = []\n",
    "    seen = set()\n",
    "    for c in list(_pycountry.countries):  # type: ignore\n",
    "        iso3 = getattr(c, \"alpha_3\", None)\n",
    "        if not iso3:\n",
    "            continue\n",
    "        for attr in [\"name\", \"official_name\", \"common_name\"]:\n",
    "            nm = getattr(c, attr, None)\n",
    "            if nm:\n",
    "                k = _norm_country_key(str(nm))\n",
    "                if k and k not in seen:\n",
    "                    seen.add(k)\n",
    "                    rows.append({\"name\": str(nm), \"iso3\": str(iso3)})\n",
    "    # Add common fixes/synonyms\n",
    "    for k, iso3 in COMMON_COUNTRY_FIXES.items():\n",
    "        k2 = _norm_country_key(k)\n",
    "        if k2 and k2 not in seen:\n",
    "            seen.add(k2)\n",
    "            rows.append({\"name\": k, \"iso3\": iso3})\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(rows).to_csv(path, index=False)\n",
    "    print(\"Generated:\", path)\n",
    "\n",
    "maybe_generate_country_iso3_table(COUNTRY_ISO3_CSV)\n",
    "# Reload after generation\n",
    "if not COUNTRY_ISO3_TABLE and COUNTRY_ISO3_CSV.exists():\n",
    "    COUNTRY_ISO3_TABLE = load_country_iso3_table(COUNTRY_ISO3_CSV)\n",
    "\n",
    "def country_name_to_iso3(name: str) -> Optional[str]:\n",
    "    n = (name or \"\").strip()\n",
    "    if not n:\n",
    "        return None\n",
    "\n",
    "    # If the group already looks like an ISO3, accept it\n",
    "    if len(n) == 3 and n.isalpha():\n",
    "        return n.upper()\n",
    "\n",
    "    key = _norm_country_key(n)\n",
    "\n",
    "    # First: explicit fixes\n",
    "    if key in COMMON_COUNTRY_FIXES_NORM:\n",
    "        return COMMON_COUNTRY_FIXES_NORM[key]\n",
    "\n",
    "    # Second: persisted table (if available)\n",
    "    iso3 = COUNTRY_ISO3_TABLE.get(key)\n",
    "    if iso3:\n",
    "        return iso3\n",
    "\n",
    "    # Third: pycountry lookup (optional dependency)\n",
    "    if _pycountry is not None:\n",
    "        try:\n",
    "            c = _pycountry.countries.lookup(n)  # type: ignore\n",
    "            return getattr(c, \"alpha_3\", None)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def infer_spatial(groups: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Infer RDLS spatial block from HDX country groups (best-effort).\"\"\"\n",
    "    iso3s: List[str] = []\n",
    "    for g in groups:\n",
    "        iso3 = country_name_to_iso3(g)\n",
    "        if iso3:\n",
    "            iso3s.append(iso3)\n",
    "\n",
    "    iso3s = sorted(set(iso3s))\n",
    "    if len(iso3s) == 1:\n",
    "        return {\"scale\": \"national\", \"countries\": iso3s}\n",
    "    if len(iso3s) > 1:\n",
    "        return {\"scale\": \"regional\", \"countries\": iso3s}\n",
    "    return {\"scale\": \"global\"}\n",
    "\n",
    "# ============================\n",
    "# Hazard keyword → hazard_type\n",
    "# ============================\n",
    "# Used only for filename suffix inference (optional).\n",
    "# Keep conservative; default to empty if no matches.\n",
    "\n",
    "HAZARD_KEYWORDS_TO_TYPE = {\n",
    "    # hydro\n",
    "    \"flood\": \"flood\",\n",
    "    \"flooding\": \"flood\",\n",
    "    \"river flood\": \"flood\",\n",
    "    \"flash flood\": \"flood\",\n",
    "    \"inundation\": \"flood\",\n",
    "\n",
    "    # drought\n",
    "    \"drought\": \"drought\",\n",
    "    \"dry spell\": \"drought\",\n",
    "    \"water scarcity\": \"drought\",\n",
    "\n",
    "    # storms / wind\n",
    "    \"cyclone\": \"windstorm\",\n",
    "    \"hurricane\": \"windstorm\",\n",
    "    \"typhoon\": \"windstorm\",\n",
    "    \"windstorm\": \"windstorm\",\n",
    "    \"storm\": \"windstorm\",\n",
    "\n",
    "    # heat / wildfire\n",
    "    \"heatwave\": \"heat\",\n",
    "    \"extreme heat\": \"heat\",\n",
    "    \"wildfire\": \"wildfire\",\n",
    "    \"fire\": \"wildfire\",\n",
    "\n",
    "    # seismic\n",
    "    \"earthquake\": \"earthquake\",\n",
    "    \"tsunami\": \"tsunami\",\n",
    "\n",
    "    # landslide\n",
    "    \"landslide\": \"landslide\",\n",
    "    \"mudslide\": \"landslide\",\n",
    "    \"avalanche\": \"landslide\",\n",
    "}\n",
    "\n",
    "def infer_hazard_types(tags: List[str], title: str = \"\", notes: str = \"\") -> List[str]:\n",
    "    text = \" \".join([*tags, title or \"\", notes or \"\"]).lower()\n",
    "    hits = set()\n",
    "    for k, ht in HAZARD_KEYWORDS_TO_TYPE.items():\n",
    "        if k in text:\n",
    "            hits.add(ht)\n",
    "    return sorted(hits)\n",
    "\n",
    "def hazard_suffix_for_filename(hazard_types: List[str]) -> str:\n",
    "    if not hazard_types:\n",
    "        return \"\"\n",
    "    if len(hazard_types) > 1:\n",
    "        return \"_multihazard\"\n",
    "    ht = hazard_types[0]\n",
    "    ht_alias = HAZARD_FILENAME_ALIASES.get(ht, ht)\n",
    "    return \"_\" + slugify_token(ht_alias, max_len=24)\n",
    "\n",
    "# --- Risk components mapping (Step 5 -> RDLS enum) ---\n",
    "COMPONENT_MAP = {\n",
    "    \"hazard\": \"hazard\",\n",
    "    \"exposure\": \"exposure\",\n",
    "    \"vulnerability_proxy\": \"vulnerability\",\n",
    "    \"loss_impact\": \"loss\",\n",
    "    \"vulnerability\": \"vulnerability\",\n",
    "    \"loss\": \"loss\",\n",
    "}\n",
    "\n",
    "def parse_components(s: Any) -> List[str]:\n",
    "    parts = split_semicolon_list(s)\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        p2 = COMPONENT_MAP.get(p.strip().lower(), None)\n",
    "        if p2:\n",
    "            out.append(p2)\n",
    "    # unique + preserve precedence order (hazard/exposure/vulnerability/loss not required here)\n",
    "    seen = set()\n",
    "    final = []\n",
    "    for x in out:\n",
    "        if x not in seen:\n",
    "            final.append(x)\n",
    "            seen.add(x)\n",
    "    return final\n",
    "\n",
    "# --- Naming prefix precedence ---\n",
    "def choose_prefix(risk_data_type: List[str]) -> str:\n",
    "    rset = set(risk_data_type)\n",
    "    if \"loss\" in rset:\n",
    "        return \"rdls_lss-\"\n",
    "    if \"vulnerability\" in rset:\n",
    "        return \"rdls_vln-\"\n",
    "    if \"exposure\" in rset:\n",
    "        return \"rdls_exp-\"\n",
    "    return \"rdls_hzd-\"\n",
    "\n",
    "def map_license(hdx_license_title: str) -> str:\n",
    "    \"\"\"Map HDX license strings into RDLS schema license suggestions when possible.\n",
    "\n",
    "    Policy:\n",
    "    - If we can confidently map to a schema-aligned identifier (e.g., CC-BY-4.0, ODbL-1.0), return that.\n",
    "    - Otherwise, keep the original HDX string (openCodelist allows new values).\n",
    "    \"\"\"\n",
    "    raw = (hdx_license_title or \"\").strip()\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "\n",
    "    key = raw.lower().strip()\n",
    "    key = re.sub(r\"\\s+\", \" \", key)\n",
    "\n",
    "    # High-confidence pattern mappings\n",
    "    # CC0\n",
    "    if re.search(r\"\\bcc0\\b\", key) or \"public domain\" in key and \"cc0\" in key:\n",
    "        return \"CC0-1.0\"\n",
    "\n",
    "    # ODbL\n",
    "    if \"odbl\" in key or \"open database license\" in key:\n",
    "        return \"ODbL-1.0\"\n",
    "\n",
    "    # ODC / PDDL\n",
    "    if \"pddl\" in key or \"public domain dedication\" in key:\n",
    "        return \"PDDL-1.0\"\n",
    "    if \"odc-by\" in key or \"odc by\" in key:\n",
    "        return \"ODC-By-1.0\"\n",
    "\n",
    "    # Creative Commons variants\n",
    "    # Normalize \"creative commons\" prefix\n",
    "    k2 = key.replace(\"creative commons\", \"cc\")\n",
    "\n",
    "    # CC-BY\n",
    "    if re.search(r\"\\bcc\\s*by\\b\", k2) and \"sa\" not in k2 and \"nd\" not in k2 and \"nc\" not in k2:\n",
    "        # Prefer explicit version 4.0 if present\n",
    "        if \"4.0\" in k2 or \"v4\" in k2:\n",
    "            return \"CC-BY-4.0\"\n",
    "        if \"3.0\" in k2:\n",
    "            return \"CC-BY-3.0\"\n",
    "        return \"CC-BY-4.0\" if \"by\" in k2 else raw\n",
    "\n",
    "    # CC-BY-SA\n",
    "    if (\"by-sa\" in k2) or re.search(r\"\\bcc\\s*by\\s*sa\\b\", k2):\n",
    "        if \"4.0\" in k2:\n",
    "            return \"CC-BY-SA-4.0\"\n",
    "        if \"3.0\" in k2:\n",
    "            return \"CC-BY-SA-3.0\"\n",
    "        return \"CC-BY-SA-4.0\"\n",
    "\n",
    "    # CC-BY-NC\n",
    "    if (\"by-nc\" in k2) or re.search(r\"\\bcc\\s*by\\s*nc\\b\", k2):\n",
    "        if \"4.0\" in k2:\n",
    "            return \"CC-BY-NC-4.0\"\n",
    "        if \"3.0\" in k2:\n",
    "            return \"CC-BY-NC-3.0\"\n",
    "        return \"CC-BY-NC-4.0\"\n",
    "\n",
    "    # CC-BY-ND\n",
    "    if (\"by-nd\" in k2) or re.search(r\"\\bcc\\s*by\\s*nd\\b\", k2):\n",
    "        if \"4.0\" in k2:\n",
    "            return \"CC BY-ND 4.0\"\n",
    "        if \"3.0\" in k2:\n",
    "            return \"CC BY-ND 3.0\"\n",
    "        return \"CC BY-ND 4.0\"\n",
    "\n",
    "    # CC-BY-NC-SA\n",
    "    if \"by-nc-sa\" in k2 or (\"nc\" in k2 and \"sa\" in k2 and \"by\" in k2):\n",
    "        if \"4.0\" in k2:\n",
    "            return \"CC-BY-NC-SA-4.0\"\n",
    "        if \"3.0\" in k2:\n",
    "            return \"CC-BY-NC-SA-3.0\"\n",
    "        return \"CC-BY-NC-SA-4.0\"\n",
    "\n",
    "    # Fallback to explicit mapping dict, if provided\n",
    "    k3 = re.sub(r\"\\s+\", \" \", k2).strip()\n",
    "    return HDX_LICENSE_TO_RDLS.get(k3, raw)\n",
    "\n",
    "\n",
    "def map_data_format(hdx_fmt: str, url: str = \"\") -> Optional[str]:\n",
    "    s = (hdx_fmt or \"\").strip().upper()\n",
    "    if s in HDX_FORMAT_TO_RDLS:\n",
    "        return HDX_FORMAT_TO_RDLS[s]\n",
    "    # Guess from URL extension if needed\n",
    "    u = (url or \"\").lower()\n",
    "    for ext, rdls in [\n",
    "        (\".geojson\", \"GeoJSON (geojson)\"),\n",
    "        (\".json\", \"JSON (json)\"),\n",
    "        (\".csv\", \"CSV (csv)\"),\n",
    "        (\".xlsx\", \"Excel (xlsx)\"),\n",
    "        (\".xls\", \"Excel (xlsx)\"),\n",
    "        (\".shp\", \"Shapefile (shp)\"),\n",
    "        (\".zip\", \"Shapefile (shp)\"),\n",
    "        (\".tif\", \"GeoTIFF (tif)\"),\n",
    "        (\".tiff\", \"GeoTIFF (tif)\"),\n",
    "        (\".nc\", \"NetCDF (nc)\"),\n",
    "        (\".pdf\", \"PDF (pdf)\"),\n",
    "        (\".parquet\", \"Parquet (parquet)\"),\n",
    "        (\".gpkg\", \"GeoPackage (gpkg)\"),\n",
    "    ]:\n",
    "        if u.endswith(ext):\n",
    "            return rdls\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c513036-2dd3-4003-8dc0-1de8ee9c9be8",
   "metadata": {},
   "source": [
    "### Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc35c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# Component gating / repair\n",
    "# ==========================\n",
    "\n",
    "AUTO_REPAIR_COMPONENTS = True  # set False if you want strict blocking\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ComponentGateResult:\n",
    "    ok: bool\n",
    "    reasons: Tuple[str, ...]\n",
    "    risk_data_type: List[str]  # must be subset of: hazard, exposure, vulnerability, loss\n",
    "\n",
    "def apply_component_gate(components: List[str]) -> ComponentGateResult:\n",
    "    \"\"\"\n",
    "    Enforce RDLS component combination rules for risk_data_type.\n",
    "\n",
    "    Rules:\n",
    "    - vulnerability must co-occur with hazard or exposure\n",
    "    - loss must co-occur with hazard or exposure\n",
    "    - risk_data_type must be non-empty and only include allowed values\n",
    "\n",
    "    If AUTO_REPAIR_COMPONENTS=True:\n",
    "      - vulnerability-only -> add exposure\n",
    "      - loss-only -> add exposure\n",
    "    \"\"\"\n",
    "    allowed = {\"hazard\", \"exposure\", \"vulnerability\", \"loss\"}\n",
    "    rset = {c for c in (components or []) if c in allowed}\n",
    "\n",
    "    if not rset:\n",
    "        return ComponentGateResult(\n",
    "            ok=False,\n",
    "            reasons=(\"empty_or_unrecognized_components\",),\n",
    "            risk_data_type=[],\n",
    "        )\n",
    "\n",
    "    reasons: List[str] = []\n",
    "    ok = True\n",
    "\n",
    "    if \"vulnerability\" in rset and not ({\"hazard\", \"exposure\"} & rset):\n",
    "        if AUTO_REPAIR_COMPONENTS:\n",
    "            rset.add(\"exposure\")\n",
    "            reasons.append(\"auto_added_exposure_for_vulnerability\")\n",
    "        else:\n",
    "            ok = False\n",
    "            reasons.append(\"vulnerability_without_hazard_or_exposure\")\n",
    "\n",
    "    if \"loss\" in rset and not ({\"hazard\", \"exposure\"} & rset):\n",
    "        if AUTO_REPAIR_COMPONENTS:\n",
    "            rset.add(\"exposure\")\n",
    "            reasons.append(\"auto_added_exposure_for_loss\")\n",
    "        else:\n",
    "            ok = False\n",
    "            reasons.append(\"loss_without_hazard_or_exposure\")\n",
    "\n",
    "    return ComponentGateResult(ok=ok, reasons=tuple(reasons), risk_data_type=sorted(rset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e43d0c-9b98-4791-9bc3-4d57790d1ec3",
   "metadata": {},
   "source": [
    "### Cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43930597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Build RDLS dataset record (minimal, schema-safe)\n",
    "# ======================\n",
    "def build_attributions(hdx: Dict[str, Any], dataset_id: str, dataset_page_url: str) -> List[Dict[str, Any]]:\n",
    "    org = (hdx.get(\"organization\") or \"\").strip() or \"Unknown publisher\"\n",
    "    src = (hdx.get(\"dataset_source\") or \"\").strip() or org\n",
    "\n",
    "    # Prefer dataset_source URL if it is a URL; otherwise use the dataset landing page\n",
    "    creator_url = src if looks_like_url(src) else dataset_page_url\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"id\": \"attribution_publisher\",\n",
    "            \"role\": \"publisher\",\n",
    "            \"entity\": {\"name\": org, \"url\": dataset_page_url},\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"attribution_creator\",\n",
    "            \"role\": \"creator\",\n",
    "            \"entity\": {\"name\": src, \"url\": creator_url},\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"attribution_contact\",\n",
    "            \"role\": \"contact_point\",\n",
    "            \"entity\": {\"name\": org, \"url\": dataset_page_url},\n",
    "        },\n",
    "    ]\n",
    "\n",
    "def build_resources(hdx: Dict[str, Any], dataset_id: str) -> List[Dict[str, Any]]:\n",
    "    # Always include at least one safe resource: HDX metadata export JSON\n",
    "    meta_url = f\"https://data.humdata.org/dataset/{dataset_id}/download_metadata?format=json\"\n",
    "    resources: List[Dict[str, Any]] = [\n",
    "        {\n",
    "            \"id\": \"hdx_dataset_metadata_json\",\n",
    "            \"title\": \"HDX dataset metadata (JSON)\",\n",
    "            \"description\": \"Dataset-level metadata exported from HDX.\",\n",
    "            \"data_format\": \"JSON (json)\",\n",
    "            \"access_modality\": \"file_download\",\n",
    "            \"download_url\": meta_url,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for r in hdx.get(\"resources\", []) or []:\n",
    "        rid = (r.get(\"id\") or \"\").strip()\n",
    "        rname = (r.get(\"name\") or \"\").strip() or rid[:8] or \"resource\"\n",
    "        desc = (r.get(\"description\") or \"\").strip() or f\"HDX resource: {rname}\"\n",
    "        dl = (r.get(\"download_url\") or \"\").strip()\n",
    "        fmt = map_data_format(r.get(\"format\") or \"\", dl)\n",
    "        if not dl or not fmt:\n",
    "            # skip if we cannot provide required fields safely\n",
    "            continue\n",
    "\n",
    "        resources.append(\n",
    "            {\n",
    "                \"id\": f\"hdx_res_{rid[:8] or slugify_token(rname, 8)}\",\n",
    "                \"title\": rname,\n",
    "                \"description\": desc,\n",
    "                \"data_format\": fmt,\n",
    "                \"access_modality\": \"file_download\",\n",
    "                \"download_url\": dl,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Ensure resources are unique by id\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for rr in resources:\n",
    "        if rr[\"id\"] not in seen:\n",
    "            deduped.append(rr)\n",
    "            seen.add(rr[\"id\"])\n",
    "    return deduped\n",
    "\n",
    "def build_rdls_record(\n",
    "    hdx: Dict[str, Any],\n",
    "    class_row: pd.Series,\n",
    ") -> Tuple[Optional[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"Return (rdls_record_or_none, info_dict). If blocked, rdls_record is None and info has reasons.\"\"\"\n",
    "\n",
    "    dataset_id = str(class_row[\"dataset_id\"])\n",
    "    title = (hdx.get(\"title\") or class_row.get(\"title\") or \"\").strip()\n",
    "    notes = (hdx.get(\"notes\") or \"\").strip()\n",
    "\n",
    "    # Parse components (from Step 5) and apply gate\n",
    "    components = parse_components(class_row.get(\"rdls_components\"))\n",
    "    gate = apply_component_gate(components)\n",
    "    if not gate.ok:\n",
    "        return None, {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"blocked\": True,\n",
    "            \"blocked_reasons\": \";\".join(gate.reasons),\n",
    "            \"risk_data_type\": \";\".join(gate.risk_data_type),\n",
    "        }\n",
    "\n",
    "    # Spatial inference from groups\n",
    "    groups = split_semicolon_list(class_row.get(\"groups\"))\n",
    "    spatial = infer_spatial(groups)\n",
    "\n",
    "    # Hazard inference for naming (optional)\n",
    "    tags = split_semicolon_list(class_row.get(\"tags\"))\n",
    "    hazard_types = infer_hazard_types(tags, title=title, notes=notes)\n",
    "\n",
    "    dataset_page_url = f\"https://data.humdata.org/dataset/{dataset_id}\"\n",
    "\n",
    "    # Entity token for naming:\n",
    "    # Prefer HDX dataset slug (`name`) because it is usually descriptive and unique within HDX.\n",
    "    # Fall back to title if needed.\n",
    "    hdx_slug = slugify_token(str(hdx.get(\"name\") or \"\"), max_len=48)\n",
    "    title_slug = slugify_token(title, max_len=48)\n",
    "    dataset_slug = hdx_slug if hdx_slug != \"unknown\" else title_slug\n",
    "\n",
    "    # Organization token (short, stable)\n",
    "    org_token = slugify_token(str(class_row.get(\"organization\") or hdx.get(\"organization\") or \"unknown\"), max_len=20)\n",
    "\n",
    "    # Optional ISO3 token (only if exactly one country inferred)\n",
    "    iso3_tok = \"\"\n",
    "    if spatial.get(\"countries\") and len(spatial[\"countries\"]) == 1:\n",
    "        iso3_tok = str(spatial[\"countries\"][0]).lower()\n",
    "\n",
    "    # Compose an informative identifier: org + optional iso3 + dataset slug\n",
    "    parts = [org_token]\n",
    "    if iso3_tok:\n",
    "        parts.append(iso3_tok)\n",
    "    parts.append(dataset_slug)\n",
    "    entity_token = \"_\".join([p for p in parts if p])\n",
    "\n",
    "    # Prefix follows your component priority rules, with explicit HDX provenance marker.\n",
    "    prefix = choose_prefix(gate.risk_data_type) + \"hdx_\"\n",
    "\n",
    "    # Hazard suffix is helpful mainly when hazard/loss is present.\n",
    "    hz_suffix = hazard_suffix_for_filename(hazard_types) if (\"hazard\" in set(gate.risk_data_type) or \"loss\" in set(gate.risk_data_type)) else \"\"\n",
    "\n",
    "    stem_base = f\"{prefix}{entity_token}{hz_suffix}\"\n",
    "    stem = stem_base\n",
    "\n",
    "    # Collision-proofing (deterministic, short)\n",
    "    out_path = OUT_RECORDS_DIR / f\"{stem}.json\"\n",
    "    if out_path.exists():\n",
    "        stem = f\"{stem_base}__{dataset_id[:8]}\"\n",
    "        out_path = OUT_RECORDS_DIR / f\"{stem}.json\"\n",
    "\n",
    "    license_raw = str(class_row.get(\"license_title\") or hdx.get(\"license_title\") or \"\").strip()\n",
    "    license_mapped = map_license(license_raw or \"Custom\")\n",
    "\n",
    "    # Build minimal RDLS dataset record with schema-safe keys only\n",
    "    rdls_ds: Dict[str, Any] = {\n",
    "        \"id\": stem,\n",
    "        \"title\": title or f\"HDX dataset {dataset_id}\",\n",
    "        \"description\": notes or None,  # optional; omit if None below\n",
    "        \"risk_data_type\": gate.risk_data_type,  # required\n",
    "        \"spatial\": spatial,  # required\n",
    "        \"license\": license_mapped,\n",
    "        \"attributions\": build_attributions(hdx, dataset_id, dataset_page_url),  # required (minItems=3)\n",
    "        \"resources\": build_resources(hdx, dataset_id),  # required (>=1)\n",
    "        \"links\": [\n",
    "            {\n",
    "                \"href\": rdls_schema.get(\"$id\") or \"https://docs.riskdatalibrary.org/en/latest/reference/rdls_schema/\",  # best-effort\n",
    "                \"rel\": \"describedby\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Remove optional keys with None (avoid violating minLength constraints on optional fields)\n",
    "    rdls_ds = {k: v for k, v in rdls_ds.items() if v is not None}\n",
    "\n",
    "    # Filter strictly to schema-allowed keys (no extras)\n",
    "    rdls_ds = {k: v for k, v in rdls_ds.items() if k in RDLS_ALLOWED_KEYS}\n",
    "\n",
    "    # Wrap in top-level structure (template style)\n",
    "    rdls_record = {\"datasets\": [rdls_ds]}\n",
    "\n",
    "    info = {\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"rdls_id\": stem,                      # output_id\n",
    "        \"filename\": f\"{stem}.json\",\n",
    "        \"risk_data_type\": \";\".join(gate.risk_data_type),\n",
    "    \n",
    "        # --- QA fields ---\n",
    "        \"spatial_scale\": spatial.get(\"scale\", \"\"),\n",
    "        \"countries_count\": len(spatial.get(\"countries\", []) or []),\n",
    "        \"license_raw\": license_raw,\n",
    "        \"orgtoken\": org_token,\n",
    "        \"hazard_suffix\": hz_suffix.lstrip(\"_\"),\n",
    "    \n",
    "        # existing fields you already had\n",
    "        \"organization_token\": org_token,\n",
    "        \"iso3\": iso3_tok,\n",
    "        \"hazard_types\": \";\".join(hazard_types),\n",
    "        \"blocked\": False,\n",
    "        \"blocked_reasons\": \"\",\n",
    "    }\n",
    "\n",
    "    return rdls_record, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a594e-a2b0-4120-a369-07f2fae7d564",
   "metadata": {},
   "source": [
    "### Cell 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb562d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonschema validation enabled (Draft2020-12).\n",
      "Written: 50\n",
      "Skipped existing: 0\n",
      "Blocked: 0\n",
      "Schema valid: 50 of 50\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\index\\rdls_index.jsonl\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\\translation_blocked.csv\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\\schema_validation.csv\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\\translation_qa.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Validate + write outputs\n",
    "# ======================\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "qa_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "def try_import_jsonschema():\n",
    "    try:\n",
    "        import jsonschema  # type: ignore\n",
    "        return jsonschema\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_jsonschema = try_import_jsonschema()\n",
    "\n",
    "validator = None\n",
    "if _jsonschema is not None:\n",
    "    try:\n",
    "        validator = _jsonschema.Draft202012Validator(rdls_schema)  # type: ignore\n",
    "        print(\"jsonschema validation enabled (Draft2020-12).\")\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: jsonschema available but validator init failed:\", e)\n",
    "        validator = None\n",
    "else:\n",
    "    print(\"WARNING: jsonschema not installed; schema validation will be skipped.\")\n",
    "\n",
    "def validate_record(rec: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    \"\"\"Validate a top-level RDLS record wrapper of the form {'datasets':[...]}\"\"\"\n",
    "    if validator is None:\n",
    "        return True, \"\"\n",
    "    errors = sorted(validator.iter_errors(rec[\"datasets\"][0]), key=lambda e: e.path)\n",
    "    if not errors:\n",
    "        return True, \"\"\n",
    "    msgs = []\n",
    "    for e in errors[:10]:\n",
    "        path = \".\".join([str(p) for p in e.path])\n",
    "        msgs.append(f\"{path}: {e.message}\")\n",
    "    return False, \" | \".join(msgs)\n",
    "\n",
    "def append_jsonl(path: Path, obj: Dict[str, Any]) -> None:\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Fresh run outputs\n",
    "OUT_INDEX_JSONL.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "blocked_rows: List[Dict[str, Any]] = []\n",
    "validation_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "written = 0\n",
    "skipped_existing = 0\n",
    "blocked = 0\n",
    "validated_ok = 0\n",
    "\n",
    "for dataset_id in included_ids:\n",
    "    fp = dataset_file_index.get(dataset_id)\n",
    "    if fp is None or not fp.exists():\n",
    "        blocked += 1\n",
    "        blocked_rows.append(\n",
    "            {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"status\": \"blocked_missing_hdx_dataset_json\",\n",
    "                \"reason\": \"missing_hdx_dataset_json\",\n",
    "                \"risk_data_type\": \"\",\n",
    "            }\n",
    "        )\n",
    "        qa_rows.append(\n",
    "            {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"output_id\": \"\",\n",
    "                \"filename\": \"\",\n",
    "                \"risk_data_type\": \"\",\n",
    "                \"spatial_scale\": \"\",\n",
    "                \"countries_count\": 0,\n",
    "                \"license_raw\": \"\",\n",
    "                \"orgtoken\": \"\",\n",
    "                \"hazard_suffix\": \"\",\n",
    "                \"status\": \"blocked_missing_hdx_dataset_json\",\n",
    "                \"reason\": \"missing_hdx_dataset_json\",\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    hdx = safe_load_json(fp)\n",
    "    row = df.loc[dataset_id]\n",
    "\n",
    "    rdls_rec, info = build_rdls_record(hdx, row)\n",
    "    if rdls_rec is None:\n",
    "        blocked += 1\n",
    "        reason = info.get(\"blocked_reasons\") or \"blocked_by_policy\"\n",
    "        rdt = info.get(\"risk_data_type\") or \"\"\n",
    "        blocked_rows.append(\n",
    "            {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"status\": \"blocked_by_policy\",\n",
    "                \"reason\": reason,\n",
    "                \"risk_data_type\": rdt,\n",
    "            }\n",
    "        )\n",
    "        qa_rows.append(\n",
    "            {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"output_id\": \"\",\n",
    "                \"filename\": \"\",\n",
    "                \"risk_data_type\": rdt,\n",
    "                \"spatial_scale\": \"\",\n",
    "                \"countries_count\": 0,\n",
    "                \"license_raw\": \"\",\n",
    "                \"orgtoken\": \"\",\n",
    "                \"hazard_suffix\": \"\",\n",
    "                \"status\": \"blocked_by_policy\",\n",
    "                \"reason\": reason,\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    out_path = OUT_RECORDS_DIR / info[\"filename\"]\n",
    "    if SKIP_EXISTING and out_path.exists():\n",
    "        skipped_existing += 1\n",
    "        qa_rows.append(\n",
    "            {\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"output_id\": info.get(\"rdls_id\", \"\"),\n",
    "                \"filename\": info.get(\"filename\", \"\"),\n",
    "                \"risk_data_type\": info.get(\"risk_data_type\", \"\"),\n",
    "                \"spatial_scale\": info.get(\"spatial_scale\", \"\"),\n",
    "                \"countries_count\": info.get(\"countries_count\", 0),\n",
    "                \"license_raw\": info.get(\"license_raw\", \"\"),\n",
    "                \"orgtoken\": info.get(\"orgtoken\", \"\"),\n",
    "                \"hazard_suffix\": info.get(\"hazard_suffix\", \"\"),\n",
    "                \"status\": \"skipped_existing\",\n",
    "                \"reason\": \"\",\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    # Validate\n",
    "    ok, msg = validate_record(rdls_rec)\n",
    "    validation_rows.append(\n",
    "        {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"rdls_id\": info[\"rdls_id\"],\n",
    "            \"filename\": info[\"filename\"],\n",
    "            \"valid\": ok,\n",
    "            \"message\": msg,\n",
    "        }\n",
    "    )\n",
    "    if ok:\n",
    "        validated_ok += 1\n",
    "\n",
    "    # Write JSON\n",
    "    if WRITE_PRETTY_JSON:\n",
    "        out_path.write_text(json.dumps(rdls_rec, indent=2, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        out_path.write_text(json.dumps(rdls_rec, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    append_jsonl(OUT_INDEX_JSONL, info)\n",
    "    written += 1\n",
    "\n",
    "    qa_rows.append(\n",
    "        {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"output_id\": info.get(\"rdls_id\", \"\"),\n",
    "            \"filename\": info.get(\"filename\", \"\"),\n",
    "            \"risk_data_type\": info.get(\"risk_data_type\", \"\"),\n",
    "            \"spatial_scale\": info.get(\"spatial_scale\", \"\"),\n",
    "            \"countries_count\": info.get(\"countries_count\", 0),\n",
    "            \"license_raw\": info.get(\"license_raw\", \"\"),\n",
    "            \"orgtoken\": info.get(\"orgtoken\", \"\"),\n",
    "            \"hazard_suffix\": info.get(\"hazard_suffix\", \"\"),\n",
    "            \"status\": \"written\",\n",
    "            \"reason\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Written:\", written)\n",
    "print(\"Skipped existing:\", skipped_existing)\n",
    "print(\"Blocked:\", blocked)\n",
    "print(\"Schema valid:\", validated_ok, \"of\", len(validation_rows))\n",
    "\n",
    "# Save reports (always write headers, even if empty)\n",
    "blocked_df = pd.DataFrame(blocked_rows, columns=[\"dataset_id\", \"status\", \"reason\", \"risk_data_type\"])\n",
    "blocked_df.to_csv(OUT_BLOCKED_CSV, index=False)\n",
    "\n",
    "val_df = pd.DataFrame(validation_rows, columns=[\"dataset_id\", \"rdls_id\", \"filename\", \"valid\", \"message\"])\n",
    "val_df.to_csv(OUT_VALIDATION_CSV, index=False)\n",
    "\n",
    "qa_df = pd.DataFrame(\n",
    "    qa_rows,\n",
    "    columns=[\n",
    "        \"dataset_id\",\n",
    "        \"output_id\",\n",
    "        \"filename\",\n",
    "        \"risk_data_type\",\n",
    "        \"spatial_scale\",\n",
    "        \"countries_count\",\n",
    "        \"license_raw\",\n",
    "        \"orgtoken\",\n",
    "        \"hazard_suffix\",\n",
    "        \"status\",\n",
    "        \"reason\",\n",
    "    ],\n",
    ")\n",
    "qa_df.to_csv(OUT_QA_CSV, index=False)\n",
    "\n",
    "print(\"Wrote:\", OUT_INDEX_JSONL)\n",
    "print(\"Wrote:\", OUT_BLOCKED_CSV)\n",
    "print(\"Wrote:\", OUT_VALIDATION_CSV)\n",
    "print(\"Wrote:\", OUT_QA_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4b174-d387-4dcb-b5db-cb36188e20b5",
   "metadata": {},
   "source": [
    "### Cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2039926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index lines: 50\n",
      "Records on disk: 50\n",
      "Blocked rows: 0\n",
      "Validation failures: 0\n",
      "QA status counts:\n",
      "status\n",
      "written    50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Quick QA: summarize what happened\n",
    "# ======================\n",
    "from pandas.errors import EmptyDataError\n",
    "\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV safely:\n",
    "    - returns empty DataFrame if file is empty or has no parsable columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except EmptyDataError:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "idx_lines = OUT_INDEX_JSONL.read_text(encoding=\"utf-8\").strip().splitlines()\n",
    "print(\"Index lines:\", len(idx_lines))\n",
    "print(\"Records on disk:\", len(list(OUT_RECORDS_DIR.glob(\"*.json\"))))\n",
    "\n",
    "if OUT_BLOCKED_CSV.exists():\n",
    "    blocked_df = safe_read_csv(OUT_BLOCKED_CSV)\n",
    "    print(\"Blocked rows:\", len(blocked_df))\n",
    "    if not blocked_df.empty and \"reason\" in blocked_df.columns:\n",
    "        print(\"Blocked reasons (top 10):\")\n",
    "        print(blocked_df[\"reason\"].value_counts().head(10))\n",
    "\n",
    "if OUT_VALIDATION_CSV.exists():\n",
    "    val_df = safe_read_csv(OUT_VALIDATION_CSV)\n",
    "    if not val_df.empty:\n",
    "        failures = val_df.loc[val_df[\"valid\"] == False, \"message\"]\n",
    "        print(\"Validation failures:\", len(failures))\n",
    "        if len(failures) > 0:\n",
    "            print(\"Validation failures (top 10):\")\n",
    "            print(failures.value_counts().head(10))\n",
    "\n",
    "# Optional: show QA summary if you added it in Cell 7\n",
    "if \"OUT_QA_CSV\" in globals() and OUT_QA_CSV.exists():\n",
    "    qa_df = safe_read_csv(OUT_QA_CSV)\n",
    "    if not qa_df.empty and \"status\" in qa_df.columns:\n",
    "        print(\"QA status counts:\")\n",
    "        print(qa_df[\"status\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3162cf-a0a9-4ffa-998f-f2f4805b63a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
