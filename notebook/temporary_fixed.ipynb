{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f487623a",
   "metadata": {},
   "source": [
    "# Temporary Fix - Rebuild `manifest_datasets.jsonl` from Dataset JSON Folder\n",
    "\n",
    "## Why this notebook exists\n",
    "Your Step 1 run produced a complete dataset JSON dump, but the manifest write occasionally failed\n",
    "(e.g., Windows file lock / permission issues) causing missing entries in `manifest_datasets.jsonl`.\n",
    "\n",
    "This notebook **rebuilds a fresh, complete manifest** directly from the dataset JSON files already downloaded,\n",
    "without re-crawling HDX.\n",
    "\n",
    "## What it does\n",
    "1. Scans `hdx_dataset_metadata_dump/dataset_metadata/*.json`\n",
    "2. Reads each dataset JSON and extracts key fields:\n",
    "   - `dataset_id`, `dataset_name`, `dataset_title`\n",
    "   - `metadata_source` (assumed `download_metadata` unless a fallback marker is detected)\n",
    "   - `metadata_file` (relative path)\n",
    "   - `metadata_url` (reconstructed from dataset_id)\n",
    "3. Writes a brand-new:\n",
    "   - `hdx_dataset_metadata_dump/manifest_datasets.jsonl`\n",
    "\n",
    "Optionally, it can also:\n",
    "- write `manifest_datasets.csv` for quick inspection in Excel\n",
    "- write `manifest_datasets_duplicates.jsonl` if any duplicate IDs are found\n",
    "\n",
    "## Inputs\n",
    "- `hdx_dataset_metadata_dump/dataset_metadata/` (26k+ JSON files)\n",
    "\n",
    "## Outputs\n",
    "- `hdx_dataset_metadata_dump/manifest_datasets.jsonl` (complete, rebuilt)\n",
    "- optional: `hdx_dataset_metadata_dump/manifest_datasets.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01e4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "temporary_fixed.ipynb\n",
    "\n",
    "Rebuild a complete HDX dataset manifest (JSONL) from an existing dataset metadata dump.\n",
    "\n",
    "Design principles:\n",
    "- No network calls; purely local rebuild\n",
    "- Deterministic output ordering\n",
    "- Audit-friendly: detects duplicates and malformed files\n",
    "- Minimal dependencies: standard library only\n",
    "\n",
    "Author: <YOUR NAME/ORG>\n",
    "License: <YOUR LICENSE>\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f486e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\dataset_metadata\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration (EDIT HERE)\n",
    "# =========================\n",
    "\n",
    "# Root dump directory produced by Step 1:\n",
    "DUMP_DIR = Path(\"../hdx_dataset_metadata_dump\")\n",
    "\n",
    "# Dataset JSON folder (dataset-level metadata):\n",
    "DATASET_DIR = DUMP_DIR / \"dataset_metadata\"\n",
    "\n",
    "# Output manifest paths (will overwrite existing):\n",
    "OUT_MANIFEST_JSONL = DUMP_DIR / \"manifest_datasets.jsonl\"\n",
    "\n",
    "# Optional outputs:\n",
    "WRITE_CSV = True\n",
    "OUT_MANIFEST_CSV = DUMP_DIR / \"manifest_datasets.csv\"\n",
    "\n",
    "WRITE_DUPLICATES_JSONL = True\n",
    "OUT_DUPLICATES_JSONL = DUMP_DIR / \"manifest_datasets_duplicates.jsonl\"\n",
    "\n",
    "# Base URL to reconstruct metadata_url\n",
    "BASE_URL = \"https://data.humdata.org\"\n",
    "\n",
    "print(\"DATASET_DIR:\", DATASET_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcf51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in a folder (non-recursive), sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "def read_json(path: Path) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Read a JSON file; return None if unreadable.\"\"\"\n",
    "    try:\n",
    "        return json.loads(path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_dataset_record(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Handle possible fallback wrapper: {'dataset': {...}}.\"\"\"\n",
    "    if isinstance(raw, dict) and \"id\" in raw:\n",
    "        return raw\n",
    "    if isinstance(raw, dict) and \"dataset\" in raw and isinstance(raw[\"dataset\"], dict):\n",
    "        return raw[\"dataset\"]\n",
    "    return raw\n",
    "\n",
    "def reconstruct_metadata_url(dataset_id: str) -> str:\n",
    "    \"\"\"Reconstruct dataset-level download_metadata URL for a dataset UUID.\"\"\"\n",
    "    return f\"{BASE_URL}/dataset/{dataset_id}/download_metadata?format=json\"\n",
    "\n",
    "def detect_metadata_source(raw: Dict[str, Any]) -> str:\n",
    "    \"\"\"Infer whether this record looks like a fallback.\"\"\"\n",
    "    # Your Step 1 typically writes direct HDX export JSON with id at top-level.\n",
    "    # If it was a fallback, Step 1 wrapped it and added helper fields.\n",
    "    if isinstance(raw, dict) and (\"_fallback_reason\" in raw or \"_note\" in raw) and \"dataset\" in raw:\n",
    "        return \"ckan_package_show_fallback\"\n",
    "    return \"download_metadata\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d63e622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 26,246 dataset JSON files...\n",
      "  processed 1,000/26,246\n",
      "  processed 2,000/26,246\n",
      "  processed 3,000/26,246\n",
      "  processed 4,000/26,246\n",
      "  processed 5,000/26,246\n",
      "  processed 6,000/26,246\n",
      "  processed 7,000/26,246\n",
      "  processed 8,000/26,246\n",
      "  processed 9,000/26,246\n",
      "  processed 10,000/26,246\n",
      "  processed 11,000/26,246\n",
      "  processed 12,000/26,246\n",
      "  processed 13,000/26,246\n",
      "  processed 14,000/26,246\n",
      "  processed 15,000/26,246\n",
      "  processed 16,000/26,246\n",
      "  processed 17,000/26,246\n",
      "  processed 18,000/26,246\n",
      "  processed 19,000/26,246\n",
      "  processed 20,000/26,246\n",
      "  processed 21,000/26,246\n",
      "  processed 22,000/26,246\n",
      "  processed 23,000/26,246\n",
      "  processed 24,000/26,246\n",
      "  processed 25,000/26,246\n",
      "  processed 26,000/26,246\n",
      "  processed 26,246/26,246\n",
      "\n",
      "Manifest rows: 26,246\n",
      "Duplicate IDs:  0\n",
      "Bad files:      0\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Build manifest rows\n",
    "# =========================\n",
    "\n",
    "def build_manifest(dataset_dir: Path) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Path]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      rows: manifest rows\n",
    "      duplicates: rows that share the same dataset_id\n",
    "      bad_files: files that could not be parsed as JSON\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    bad_files: List[Path] = []\n",
    "    seen: Dict[str, Dict[str, Any]] = {}\n",
    "    duplicates: List[Dict[str, Any]] = []\n",
    "\n",
    "    files = list(iter_json_files(dataset_dir))\n",
    "    total = len(files)\n",
    "    print(f\"Scanning {total:,} dataset JSON files...\")\n",
    "\n",
    "    for i, path in enumerate(files, start=1):\n",
    "        if i % 1000 == 0 or i == total:\n",
    "            print(f\"  processed {i:,}/{total:,}\")\n",
    "\n",
    "        raw = read_json(path)\n",
    "        if raw is None:\n",
    "            bad_files.append(path)\n",
    "            continue\n",
    "\n",
    "        ds = normalize_dataset_record(raw)\n",
    "        dataset_id = ds.get(\"id\") or \"\"\n",
    "        dataset_name = ds.get(\"name\") or \"\"\n",
    "        dataset_title = ds.get(\"title\") or \"\"\n",
    "\n",
    "        if not dataset_id:\n",
    "            bad_files.append(path)\n",
    "            continue\n",
    "\n",
    "        source = detect_metadata_source(raw)\n",
    "\n",
    "        rel_path = path.relative_to(DUMP_DIR).as_posix() if path.is_relative_to(DUMP_DIR) else path.as_posix()\n",
    "\n",
    "        row = {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"dataset_title\": dataset_title,\n",
    "            \"metadata_source\": source,\n",
    "            \"metadata_file\": rel_path,\n",
    "            \"metadata_url\": reconstruct_metadata_url(dataset_id),\n",
    "        }\n",
    "\n",
    "        if dataset_id in seen:\n",
    "            duplicates.append(row)\n",
    "        else:\n",
    "            seen[dataset_id] = row\n",
    "            rows.append(row)\n",
    "\n",
    "    # Deterministic ordering: dataset_id ascending\n",
    "    rows.sort(key=lambda r: r[\"dataset_id\"])\n",
    "    duplicates.sort(key=lambda r: r[\"dataset_id\"])\n",
    "\n",
    "    return rows, duplicates, bad_files\n",
    "\n",
    "rows, duplicates, bad_files = build_manifest(DATASET_DIR)\n",
    "\n",
    "print(f\"\\nManifest rows: {len(rows):,}\")\n",
    "print(f\"Duplicate IDs:  {len(duplicates):,}\")\n",
    "print(f\"Bad files:      {len(bad_files):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd966d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\manifest_datasets.jsonl\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\manifest_datasets.csv\n",
      "No duplicates found; duplicates file not written.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Write manifest (JSONL + optional CSV)\n",
    "# =========================\n",
    "\n",
    "def write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def write_csv(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows:\n",
    "        return\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "write_jsonl(OUT_MANIFEST_JSONL, rows)\n",
    "print(\"Wrote:\", OUT_MANIFEST_JSONL.resolve())\n",
    "\n",
    "if WRITE_CSV:\n",
    "    write_csv(OUT_MANIFEST_CSV, rows)\n",
    "    print(\"Wrote:\", OUT_MANIFEST_CSV.resolve())\n",
    "\n",
    "if WRITE_DUPLICATES_JSONL and duplicates:\n",
    "    write_jsonl(OUT_DUPLICATES_JSONL, duplicates)\n",
    "    print(\"Wrote:\", OUT_DUPLICATES_JSONL.resolve())\n",
    "elif WRITE_DUPLICATES_JSONL:\n",
    "    print(\"No duplicates found; duplicates file not written.\")\n",
    "\n",
    "# Optional: write bad file list for debugging\n",
    "if bad_files:\n",
    "    bad_list = DUMP_DIR / \"manifest_datasets_bad_files.txt\"\n",
    "    with bad_list.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for p in bad_files:\n",
    "            f.write(p.as_posix() + \"\\n\")\n",
    "    print(\"Wrote:\", bad_list.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "815f7837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 3 rows:\n",
      "{'dataset_id': '00035653-2e52-4716-9121-7d6f6d9f961b', 'dataset_name': 'rasp4', 'dataset_title': 'Risk Assessment Site Priority (RASP)', 'metadata_source': 'download_metadata', 'metadata_file': 'dataset_metadata/00035653-2e52-4716-9121-7d6f6d9f961b__rasp4.json', 'metadata_url': 'https://data.humdata.org/dataset/00035653-2e52-4716-9121-7d6f6d9f961b/download_metadata?format=json'}\n",
      "{'dataset_id': '0004678f-50cb-4a07-8969-5a74bc492efb', 'dataset_name': 'financiamiento-y-personas-alcanzadas-por-el-cluster-salud-colombia', 'dataset_title': 'Financiamiento y personas alcanzadas por el Cl√∫ster salud - Colombia 2021', 'metadata_source': 'download_metadata', 'metadata_file': 'dataset_metadata/0004678f-50cb-4a07-8969-5a74bc492efb__financiamiento-y-personas-alcanzadas-por-el-cluster-salud-colombia.json', 'metadata_url': 'https://data.humdata.org/dataset/0004678f-50cb-4a07-8969-5a74bc492efb/download_metadata?format=json'}\n",
      "{'dataset_id': '00090357-0df0-4e33-9bc8-aa3ce425ef09', 'dataset_name': 'kenya_medium_term_projection__fewsnet_ipc_shapefile_for_2016', 'dataset_title': 'Kenya Medium Term Projection  FEWS NET Acute Food Insecurity Classifications Geographic Data for 2016', 'metadata_source': 'download_metadata', 'metadata_file': 'dataset_metadata/00090357-0df0-4e33-9bc8-aa3ce425ef09__kenya-medium-term-projection-fewsnet-ipc-shapefile-for-2016.json', 'metadata_url': 'https://data.humdata.org/dataset/00090357-0df0-4e33-9bc8-aa3ce425ef09/download_metadata?format=json'}\n",
      "\n",
      "Last 3 rows:\n",
      "{'dataset_id': 'fff94d0a-096b-4c5b-a860-345d982658e5', 'dataset_name': 'unhcr-lbn-2018-vasyr-v2-1', 'dataset_title': 'Lebanon - Vulnerability Assessment of Syrian Refugees in Lebanon, 2018', 'metadata_source': 'download_metadata', 'metadata_file': 'dataset_metadata/fff94d0a-096b-4c5b-a860-345d982658e5__unhcr-lbn-2018-vasyr-v2-1.json', 'metadata_url': 'https://data.humdata.org/dataset/fff94d0a-096b-4c5b-a860-345d982658e5/download_metadata?format=json'}\n",
      "{'dataset_id': 'fffd4e59-99b3-49d1-b68e-d3a4c1984d25', 'dataset_name': 'faostat-food-security-indicators-for-greenland', 'dataset_title': 'Greenland - Food Security and Nutrition Indicators', 'metadata_source': 'download_metadata', 'metadata_file': 'dataset_metadata/fffd4e59-99b3-49d1-b68e-d3a4c1984d25__faostat-food-security-indicators-for-greenland.json', 'metadata_url': 'https://data.humdata.org/dataset/fffd4e59-99b3-49d1-b68e-d3a4c1984d25/download_metadata?format=json'}\n",
      "{'dataset_id': 'ffff2ed7-8df6-4935-a2a0-7180a1861631', 'dataset_name': 'ethiopia_most_likely_fewsnet_fipe', 'dataset_title': 'Ethiopia Most Likely FEWS NET Acutely Food Insecure Population Estimates Data', 'metadata_source': 'download_metadata', 'metadata_file': 'dataset_metadata/ffff2ed7-8df6-4935-a2a0-7180a1861631__ethiopia-most-likely-fewsnet-fipe.json', 'metadata_url': 'https://data.humdata.org/dataset/ffff2ed7-8df6-4935-a2a0-7180a1861631/download_metadata?format=json'}\n",
      "\n",
      "JSON files in folder: 26,246\n",
      "Manifest unique rows:  26,246\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Quick sanity checks\n",
    "# =========================\n",
    "\n",
    "# 1) Sample first/last entries\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "for r in rows[:3]:\n",
    "    print(r)\n",
    "\n",
    "print(\"\\nLast 3 rows:\")\n",
    "for r in rows[-3:]:\n",
    "    print(r)\n",
    "\n",
    "# 2) Count check vs number of JSON files\n",
    "n_json = len(list(iter_json_files(DATASET_DIR)))\n",
    "print(f\"\\nJSON files in folder: {n_json:,}\")\n",
    "print(f\"Manifest unique rows:  {len(rows):,}\")\n",
    "if n_json != len(rows):\n",
    "    print(\"NOTE: Folder file count != unique manifest rows. See duplicates/bad files outputs above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5dcff-73c5-430e-a573-1be9b0d2f7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
