{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c955e6",
   "metadata": {},
   "source": [
    "# Notebook 11: RDLS Vulnerability Block Extractor\n",
    "\n",
    "**Purpose**: Extract and populate RDLS v0.3 Vulnerability component blocks from HDX metadata.\n",
    "\n",
    "**RDLS Vulnerability Block Structure (v0.3)**:\n",
    "```\n",
    "vulnerability:\n",
    "  functions:\n",
    "    vulnerability: [...]        # vulnerability curves (damage ratio vs intensity)\n",
    "    fragility: [...]            # fragility curves (P(damage state) vs intensity)\n",
    "    damage_to_loss: [...]       # consequence functions (loss given damage)\n",
    "    engineering_demand: [...]   # engineering demand functions\n",
    "  socio_economic: [...]         # socio-economic vulnerability indicators/indices\n",
    "```\n",
    "\n",
    "**Two Extraction Pathways**:\n",
    "1. **Functions** (vulnerability/fragility/damage_to_loss/engineering_demand):\n",
    "   - Require 10 mandatory fields per entry from closed codelists\n",
    "   - Rare in HDX metadata â€” typically only in specialized risk model datasets\n",
    "2. **Socio-economic indicators**:\n",
    "   - Common in HDX â€” poverty, displacement, food security, health, education indices\n",
    "   - Require indicator_name, indicator_code, description, reference_year\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR    \n",
    "**Version**: 2026.2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5299bfb",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52fbd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started: 2026-02-11T17:41:17.346813\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any, Set\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45470e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hazard cross-reference: 3,208 records\n",
      "Configuration loaded.\n",
      "  Signal Dictionary sections: ['hazard_type', 'process_type', 'exposure_category', 'analysis_type', 'return_period', 'spatial_scale', 'vulnerability_indicators', 'loss_indicators', 'format_hints', 'organization_hints', 'exclusion_patterns']\n",
      "  Base: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler\n",
      "  Output: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted\n",
      "  Cleanup mode: replace\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Paths and Configuration\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input paths\n",
    "DATASET_METADATA_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'dataset_metadata'\n",
    "SIGNAL_DICT_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'config' / 'signal_dictionary.yaml'\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'extracted'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Output cleanup mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Controls what happens to old output files when this notebook is re-run.\n",
    "#   \"replace\" - Auto-delete old outputs and continue (default)\n",
    "#   \"prompt\"  - Show what will be deleted, ask user to confirm\n",
    "#   \"skip\"    - Keep old files, write new on top (may leave orphans)\n",
    "#   \"abort\"   - Stop if old outputs exist (for CI/automated runs)\n",
    "CLEANUP_MODE = \"replace\"\n",
    "\n",
    "# Hazard extraction results from NB 09 (for cross-referencing)\n",
    "HAZARD_CSV_PATH = OUTPUT_DIR / 'hazard_extraction_results.csv'\n",
    "\n",
    "# Verify paths\n",
    "assert DATASET_METADATA_DIR.exists(), f\"Not found: {DATASET_METADATA_DIR}\"\n",
    "assert SIGNAL_DICT_PATH.exists(), f\"Not found: {SIGNAL_DICT_PATH}\"\n",
    "assert RDLS_SCHEMA_PATH.exists(), f\"Not found: {RDLS_SCHEMA_PATH}\"\n",
    "\n",
    "# Load configs\n",
    "with open(SIGNAL_DICT_PATH, 'r', encoding='utf-8') as f:\n",
    "    SIGNAL_DICT = yaml.safe_load(f)\n",
    "\n",
    "with open(RDLS_SCHEMA_PATH, 'r', encoding='utf-8') as f:\n",
    "    RDLS_SCHEMA = json.load(f)\n",
    "\n",
    "# Load hazard extraction cross-reference (optional)\n",
    "HAZARD_XREF = {}\n",
    "if HAZARD_CSV_PATH.exists():\n",
    "    _hdf = pd.read_csv(HAZARD_CSV_PATH)\n",
    "    for _, row in _hdf.iterrows():\n",
    "        if row.get('has_hazard') and pd.notna(row.get('hazard_types')):\n",
    "            HAZARD_XREF[str(row['id'])] = {\n",
    "                'hazard_types': str(row['hazard_types']).split('|'),\n",
    "                'process_types': str(row.get('process_types', '')).split('|') if pd.notna(row.get('process_types')) else [],\n",
    "                'analysis_type': str(row.get('analysis_type', '')) if pd.notna(row.get('analysis_type')) else None,\n",
    "                'intensity_measures': str(row.get('intensity_measures', '')).split('|') if pd.notna(row.get('intensity_measures')) else [],\n",
    "            }\n",
    "    print(f\"Loaded hazard cross-reference: {len(HAZARD_XREF):,} records\")\n",
    "else:\n",
    "    print(\"WARNING: Hazard extraction CSV not found â€” will infer hazard context from text\")\n",
    "\n",
    "print(f\"Configuration loaded.\")\n",
    "print(f\"  Signal Dictionary sections: {list(SIGNAL_DICT.keys())}\")\n",
    "print(f\"  Base: {BASE_DIR}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")\n",
    "print(f\"  Cleanup mode: {CLEANUP_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8536b64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema constants loaded:\n",
      "  Function approaches (4): ['analytical', 'empirical', 'hybrid', 'judgement']\n",
      "  Relationship types (3): ['discrete', 'math_bespoke', 'math_parametric']\n",
      "  Hazard types (11): 11 values\n",
      "  Impact types (3): ['direct', 'indirect', 'total']\n",
      "  Calculation types (3): ['inferred', 'observed', 'simulated']\n",
      "  Impact metrics (20): 20 values\n",
      "  Taxonomies (12): ['CDC-SVI', 'Custom', 'EMDAT', 'EMS-98', 'GED4ALL', 'GLIDE', 'HAZUS', 'INFORM', 'MOVER', 'OED', 'PAGER', 'USGS_EHP']\n",
      "  Exposure categories (7): ['agriculture', 'buildings', 'development_index', 'economic_indicator', 'infrastructure', 'natural_environment', 'population']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Load Schema Constants\n",
    "\n",
    "Load all closed codelist enums from the RDLS v0.3 schema for validation.\n",
    "\"\"\"\n",
    "\n",
    "# --- Closed codelist enums ---\n",
    "VALID_FUNCTION_APPROACHES: Set[str] = set(RDLS_SCHEMA['$defs']['function_approach']['enum'])\n",
    "VALID_RELATIONSHIP_TYPES: Set[str] = set(RDLS_SCHEMA['$defs']['relationship_type']['enum'])\n",
    "VALID_HAZARD_TYPES: Set[str] = set(RDLS_SCHEMA['$defs']['hazard_type']['enum'])\n",
    "VALID_PROCESS_TYPES: Set[str] = set(RDLS_SCHEMA['$defs']['process_type']['enum'])\n",
    "VALID_ANALYSIS_TYPES: Set[str] = set(RDLS_SCHEMA['$defs']['analysis_type']['enum'])\n",
    "VALID_EXPOSURE_CATEGORIES: Set[str] = set(RDLS_SCHEMA['$defs']['exposure_category']['enum'])\n",
    "VALID_IMPACT_TYPES: Set[str] = set(RDLS_SCHEMA['$defs']['VulnerabilityCommonFields']['properties']['impact_type']['enum'])\n",
    "VALID_CALCULATION_TYPES: Set[str] = set(RDLS_SCHEMA['$defs']['data_calculation_type']['enum'])\n",
    "VALID_IMPACT_METRICS: Set[str] = set(RDLS_SCHEMA['$defs']['impact_metric']['enum'])\n",
    "VALID_TAXONOMIES: Set[str] = set(RDLS_SCHEMA['$defs']['taxonomy']['enum'])\n",
    "\n",
    "# --- Hazard type -> process type mappings ---\n",
    "HAZARD_PROCESS_MAPPINGS: Dict[str, List[str]] = RDLS_SCHEMA.get('hazard_process_mappings', {})\n",
    "\n",
    "# --- Intensity measure mappings (if available) ---\n",
    "INTENSITY_MEASURE_MAPPINGS: Dict[str, List[str]] = RDLS_SCHEMA.get('intensity_measure_mappings', {})\n",
    "\n",
    "# --- Default hazard process per hazard type ---\n",
    "HAZARD_PROCESS_DEFAULT = {\n",
    "    'flood': 'fluvial_flood',\n",
    "    'earthquake': 'ground_motion',\n",
    "    'tsunami': 'tsunami',\n",
    "    'drought': 'meteorological_drought',\n",
    "    'landslide': 'landslide_general',\n",
    "    'wildfire': 'wildfire',\n",
    "    'volcanic': 'ashfall',\n",
    "    'extreme_temperature': 'extreme_heat',\n",
    "    'strong_wind': 'tropical_cyclone',\n",
    "    'convective_storm': 'tornado',\n",
    "    'coastal_flood': 'storm_surge',\n",
    "}\n",
    "\n",
    "# --- Default intensity measure per hazard type ---\n",
    "DEFAULT_INTENSITY_MEASURE = {}\n",
    "for ht in VALID_HAZARD_TYPES:\n",
    "    measures = INTENSITY_MEASURE_MAPPINGS.get(ht, [])\n",
    "    if measures:\n",
    "        DEFAULT_INTENSITY_MEASURE[ht] = measures[0]\n",
    "\n",
    "# Fallback defaults if schema doesn't have intensity_measure_mappings\n",
    "_IM_FALLBACK = {\n",
    "    'flood': 'wd:m', 'coastal_flood': 'wd:m', 'earthquake': 'PGA:g',\n",
    "    'tsunami': 'Rh_tsi:m', 'drought': 'SPI:-', 'wildfire': 'FWI:-',\n",
    "    'convective_storm': 'sws_10m:m/s', 'strong_wind': 'sws_10m:m/s',\n",
    "    'landslide': 'PGA:g', 'volcanic': 'AirTemp:C', 'extreme_temperature': 'AirTemp:C',\n",
    "}\n",
    "for ht, im in _IM_FALLBACK.items():\n",
    "    if ht not in DEFAULT_INTENSITY_MEASURE:\n",
    "        DEFAULT_INTENSITY_MEASURE[ht] = im\n",
    "\n",
    "print(\"Schema constants loaded:\")\n",
    "print(f\"  Function approaches ({len(VALID_FUNCTION_APPROACHES)}): {sorted(VALID_FUNCTION_APPROACHES)}\")\n",
    "print(f\"  Relationship types ({len(VALID_RELATIONSHIP_TYPES)}): {sorted(VALID_RELATIONSHIP_TYPES)}\")\n",
    "print(f\"  Hazard types ({len(VALID_HAZARD_TYPES)}): {len(VALID_HAZARD_TYPES)} values\")\n",
    "print(f\"  Impact types ({len(VALID_IMPACT_TYPES)}): {sorted(VALID_IMPACT_TYPES)}\")\n",
    "print(f\"  Calculation types ({len(VALID_CALCULATION_TYPES)}): {sorted(VALID_CALCULATION_TYPES)}\")\n",
    "print(f\"  Impact metrics ({len(VALID_IMPACT_METRICS)}): {len(VALID_IMPACT_METRICS)} values\")\n",
    "print(f\"  Taxonomies ({len(VALID_TAXONOMIES)}): {sorted(VALID_TAXONOMIES)}\")\n",
    "print(f\"  Exposure categories ({len(VALID_EXPOSURE_CATEGORIES)}): {sorted(VALID_EXPOSURE_CATEGORIES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7f929",
   "metadata": {},
   "source": [
    "## 2. Vulnerability Detection Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e92ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection patterns defined.\n",
      "  Function types: ['vulnerability', 'fragility', 'damage_to_loss', 'engineering_demand']\n",
      "  Approach patterns: ['analytical', 'empirical', 'hybrid', 'judgement']\n",
      "  Socio-economic indicators: 18\n",
      "  Hazard type patterns: 11\n",
      "  Exposure category patterns: 7\n",
      "\n",
      "Vulnerability constraint tables defined.\n",
      "  Group 1 - Function type constraints: 4 types\n",
      "  Group 2 - Category defaults: 7 categories\n",
      "  Group 3 - Reuses IMPACT_METRIC_CONSTRAINTS from Loss section\n",
      "  Group 4 - Approach defaults: 4 types\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 Define Detection Patterns\n",
    "\n",
    "Patterns for detecting vulnerability function types, socio-economic indicators,\n",
    "and inferring mandatory field values from HDX metadata text.\n",
    "\"\"\"\n",
    "\n",
    "# --- Function type detection ---\n",
    "FUNCTION_TYPE_PATTERNS = {\n",
    "    'vulnerability': [\n",
    "        r'\\b(vulnerability[\\s._-]?curve|vulnerability[\\s._-]?function)\\b',\n",
    "        r'\\b(damage[\\s._-]?curve|damage[\\s._-]?function)\\b',\n",
    "        r'\\b(mean[\\s._-]?damage[\\s._-]?ratio|mdr)\\b',\n",
    "        r'\\b(damage[\\s._-]?ratio[\\s._-]?(?:vs|versus|function))\\b',\n",
    "        r'\\b(depth[\\s._-]?damage)\\b',\n",
    "    ],\n",
    "    'fragility': [\n",
    "        r'\\b(fragility[\\s._-]?curve|fragility[\\s._-]?function)\\b',\n",
    "        r'\\b(probability[\\s._-]?of[\\s._-]?damage|failure[\\s._-]?probability)\\b',\n",
    "        r'\\b(capacity[\\s._-]?spectrum|pushover)\\b',\n",
    "        r'\\b(damage[\\s._-]?state[\\s._-]?(?:ds|probability))\\b',\n",
    "        r'\\b(lognormal[\\s._-]?fragility)\\b',\n",
    "    ],\n",
    "    'damage_to_loss': [\n",
    "        r'\\b(damage[\\s._-]?to[\\s._-]?loss|consequence[\\s._-]?function)\\b',\n",
    "        r'\\b(loss[\\s._-]?function|loss[\\s._-]?model)\\b',\n",
    "        r'\\b(repair[\\s._-]?cost[\\s._-]?(?:function|ratio|curve))\\b',\n",
    "        r'\\b(replacement[\\s._-]?cost[\\s._-]?(?:function|ratio))\\b',\n",
    "    ],\n",
    "    'engineering_demand': [\n",
    "        r'\\b(engineering[\\s._-]?demand)\\b',\n",
    "        r'\\b(interstorey[\\s._-]?drift|inter[\\s._-]?storey[\\s._-]?drift)\\b',\n",
    "        r'\\b(floor[\\s._-]?acceleration|peak[\\s._-]?floor)\\b',\n",
    "        r'\\b(spectral[\\s._-]?displacement|demand[\\s._-]?capacity[\\s._-]?ratio)\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Approach inference ---\n",
    "APPROACH_PATTERNS = {\n",
    "    'analytical': [\n",
    "        r'\\b(analytical|numerical|finite[\\s._-]?element|simulation[\\s._-]?based)\\b',\n",
    "        r'\\b(capacity[\\s._-]?spectrum|pushover[\\s._-]?analysis|nonlinear[\\s._-]?analysis)\\b',\n",
    "        r'\\b(time[\\s._-]?history[\\s._-]?analysis|dynamic[\\s._-]?analysis)\\b',\n",
    "    ],\n",
    "    'empirical': [\n",
    "        r'\\b(empirical|observed|survey[\\s._-]?based|field[\\s._-]?data)\\b',\n",
    "        r'\\b(post[\\s._-]?disaster|post[\\s._-]?event|damage[\\s._-]?survey)\\b',\n",
    "        r'\\b(historical[\\s._-]?data|real[\\s._-]?event)\\b',\n",
    "    ],\n",
    "    'hybrid': [\n",
    "        r'\\b(hybrid|combined|mixed[\\s._-]?method)\\b',\n",
    "    ],\n",
    "    'judgement': [\n",
    "        r'\\b(expert[\\s._-]?judg[e]?ment|expert[\\s._-]?opinion|elicitation)\\b',\n",
    "        r'\\b(heuristic|rule[\\s._-]?based)\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Relationship type inference ---\n",
    "RELATIONSHIP_PATTERNS = {\n",
    "    'math_parametric': [\n",
    "        r'\\b(parametric|lognormal|normal[\\s._-]?distribution|cumulative[\\s._-]?distribution)\\b',\n",
    "        r'\\b(cdf|probability[\\s._-]?distribution|log[\\s._-]?normal)\\b',\n",
    "        r'\\b(median[\\s._-]?and[\\s._-]?dispersion|mu[\\s._-]?and[\\s._-]?sigma)\\b',\n",
    "    ],\n",
    "    'math_bespoke': [\n",
    "        r'\\b(bespoke|custom[\\s._-]?function|non[\\s._-]?standard)\\b',\n",
    "        r'\\b(piecewise|polynomial|spline)\\b',\n",
    "    ],\n",
    "    'discrete': [\n",
    "        r'\\b(discrete|tabular|lookup[\\s._-]?table|step[\\s._-]?function)\\b',\n",
    "        r'\\b(depth[\\s._-]?damage[\\s._-]?table|damage[\\s._-]?matrix)\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Impact type inference ---\n",
    "IMPACT_TYPE_PATTERNS = {\n",
    "    'direct': [r'\\b(direct[\\s._-]?(?:loss|damage|impact))\\b'],\n",
    "    'indirect': [r'\\b(indirect[\\s._-]?(?:loss|damage)|business[\\s._-]?interruption|downtime)\\b'],\n",
    "    'total': [r'\\b(total[\\s._-]?(?:loss|damage|impact)|combined[\\s._-]?loss)\\b'],\n",
    "}\n",
    "\n",
    "# --- Impact modelling inference ---\n",
    "IMPACT_MODELLING_PATTERNS = {\n",
    "    'simulated': [r'\\b(simulat|model(?:led|ed)|scenario[\\s._-]?based)\\b'],\n",
    "    'observed': [r'\\b(observed|recorded|actual|measured|field[\\s._-]?survey)\\b'],\n",
    "    'inferred': [r'\\b(inferred|derived|estimated|statistical)\\b'],\n",
    "}\n",
    "\n",
    "# --- Default impact_metric per function type ---\n",
    "DEFAULT_IMPACT_METRIC = {\n",
    "    'vulnerability': 'damage_ratio',\n",
    "    'fragility': 'probability',\n",
    "    'damage_to_loss': 'loss_ratio',\n",
    "    'engineering_demand': 'damage_index',\n",
    "}\n",
    "\n",
    "# --- Socio-economic indicator detection ---\n",
    "SOCIOECONOMIC_INDICATORS = [\n",
    "    {\n",
    "        'patterns': [r'\\b(poverty[\\s._-]?headcount|poverty[\\s._-]?ratio|below[\\s._-]?poverty[\\s._-]?line)\\b',\n",
    "                     r'\\b(poverty[\\s._-]?index|poverty[\\s._-]?rate|poor[\\s._-]?population)\\b'],\n",
    "        'indicator_name': 'Poverty headcount ratio',\n",
    "        'indicator_code': 'POV_HEADCOUNT',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Population living below the poverty line with limited resources for disaster preparedness and recovery',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(human[\\s._-]?development[\\s._-]?index|hdi)\\b'],\n",
    "        'indicator_name': 'Human Development Index',\n",
    "        'indicator_code': 'HDI',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Composite index measuring average achievement in health, education, and standard of living',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(social[\\s._-]?vulnerability[\\s._-]?index|svi)\\b',\n",
    "                     r'\\b(socio[\\s._-]?economic[\\s._-]?vulnerability[\\s._-]?index)\\b'],\n",
    "        'indicator_name': 'Social Vulnerability Index',\n",
    "        'indicator_code': 'SVI_OVERALL',\n",
    "        'scheme': 'CDC-SVI',\n",
    "        'description': 'Overall social vulnerability score combining socioeconomic status, household composition, minority status, and housing factors',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(food[\\s._-]?security|food[\\s._-]?insecurity|ipc[\\s._-]?phase|ipc[\\s._-]?classification)\\b',\n",
    "                     r'\\b(food[\\s._-]?crisis|famine[\\s._-]?early[\\s._-]?warning)\\b'],\n",
    "        'indicator_name': 'Food security classification',\n",
    "        'indicator_code': 'FOOD_SECURITY',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Food security status indicating population vulnerability to food crises and famine',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(population[\\s._-]?density)\\b'],\n",
    "        'indicator_name': 'Population density',\n",
    "        'indicator_code': 'POP_DENSITY',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Number of people per unit area, indicating exposure concentration and potential vulnerability',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(elderly[\\s._-]?population|aging[\\s._-]?population|aged[\\s._-]?(?:65|over))\\b',\n",
    "                     r'\\b(population[\\s._-]?(?:over|above)[\\s._-]?65)\\b'],\n",
    "        'indicator_name': 'Elderly population percentage',\n",
    "        'indicator_code': 'AGE_65_PLUS',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Population aged 65 years and older, more vulnerable to hazard-related health impacts',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(education[\\s._-]?attainment|literacy[\\s._-]?rate|school[\\s._-]?enrollment)\\b',\n",
    "                     r'\\b(out[\\s._-]?of[\\s._-]?school|educational[\\s._-]?level)\\b'],\n",
    "        'indicator_name': 'Educational attainment',\n",
    "        'indicator_code': 'EDU_ATTAINMENT',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Level of educational attainment indicating capacity to understand and respond to hazard warnings',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(health[\\s._-]?(?:access|facility|service|indicator))\\b',\n",
    "                     r'\\b(healthcare[\\s._-]?access|medical[\\s._-]?facility)\\b'],\n",
    "        'indicator_name': 'Access to healthcare facilities',\n",
    "        'indicator_code': 'HEALTH_ACCESS',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Proximity and access to healthcare services affecting disaster response and recovery capacity',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(inform[\\s._-]?(?:risk|index|severity))\\b'],\n",
    "        'indicator_name': 'INFORM Risk Index',\n",
    "        'indicator_code': 'INFORM_RISK',\n",
    "        'scheme': 'INFORM',\n",
    "        'description': 'Composite risk index combining hazard exposure, vulnerability, and lack of coping capacity',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(displaced|displacement|idp|internally[\\s._-]?displaced)\\b',\n",
    "                     r'\\b(refugee[\\s._-]?(?:population|camp|settlement))\\b'],\n",
    "        'indicator_name': 'Displacement indicator',\n",
    "        'indicator_code': 'DISPLACEMENT',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Population displaced by conflict or disaster, indicating heightened vulnerability',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(coping[\\s._-]?capacity|adaptive[\\s._-]?capacity)\\b',\n",
    "                     r'\\b(lack[\\s._-]?of[\\s._-]?coping[\\s._-]?capacity)\\b'],\n",
    "        'indicator_name': 'Coping capacity index',\n",
    "        'indicator_code': 'COPING_CAPACITY',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Capacity of communities to cope with and recover from hazard impacts',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(resilience[\\s._-]?index|community[\\s._-]?resilience)\\b'],\n",
    "        'indicator_name': 'Resilience index',\n",
    "        'indicator_code': 'RESILIENCE',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Composite index measuring community resilience to natural hazards and disasters',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(deprivation[\\s._-]?index|multi[\\s._-]?dimensional[\\s._-]?poverty)\\b'],\n",
    "        'indicator_name': 'Deprivation index',\n",
    "        'indicator_code': 'DEPRIVATION',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Multi-dimensional deprivation index measuring socio-economic disadvantage',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(malnutrition|stunting|wasting|underweight|nutrition[\\s._-]?status)\\b',\n",
    "                     r'\\b(global[\\s._-]?acute[\\s._-]?malnutrition|gam)\\b'],\n",
    "        'indicator_name': 'Malnutrition prevalence',\n",
    "        'indicator_code': 'MALNUTRITION',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Prevalence of malnutrition indicating population health vulnerability to hazard impacts',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(vulnerability[\\s._-]?index|nexus[\\s._-]?risk)\\b',\n",
    "                     r'\\b(climate[\\s._-]?vulnerability|climate[\\s._-]?risk[\\s._-]?index)\\b'],\n",
    "        'indicator_name': 'Vulnerability index',\n",
    "        'indicator_code': 'VULN_INDEX',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Composite vulnerability index combining multiple socio-economic and environmental factors',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(gender[\\s._-]?(?:inequality|index|gap))\\b',\n",
    "                     r'\\b(women[\\s._-]?(?:vulnerability|empowerment))\\b'],\n",
    "        'indicator_name': 'Gender inequality index',\n",
    "        'indicator_code': 'GENDER_INEQUALITY',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Gender inequality indicator reflecting differential vulnerability to hazard impacts',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(disability[\\s._-]?(?:prevalence|rate|population))\\b',\n",
    "                     r'\\b(persons[\\s._-]?with[\\s._-]?disabilities)\\b'],\n",
    "        'indicator_name': 'Disability prevalence',\n",
    "        'indicator_code': 'DISABILITY',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Prevalence of disability in the population indicating heightened vulnerability to hazard impacts',\n",
    "    },\n",
    "    {\n",
    "        'patterns': [r'\\b(livelihood[\\s._-]?(?:zone|index|vulnerability))\\b',\n",
    "                     r'\\b(livelihood[\\s._-]?coping[\\s._-]?strategy)\\b'],\n",
    "        'indicator_name': 'Livelihood vulnerability',\n",
    "        'indicator_code': 'LIVELIHOOD',\n",
    "        'scheme': 'Custom',\n",
    "        'description': 'Livelihood vulnerability indicator measuring economic susceptibility to hazard disruption',\n",
    "    },\n",
    "]\n",
    "\n",
    "# --- Generic socio-economic catch-all patterns ---\n",
    "GENERIC_SOCIOECONOMIC_PATTERNS = [\n",
    "    r'\\b(socio[\\s._-]?economic[\\s._-]?vulnerability)\\b',\n",
    "    r'\\b(socioeconomic[\\s._-]?(?:indicator|index|data))\\b',\n",
    "    r'\\b(vulnerability[\\s._-]?assessment[\\s._-]?(?:data|indicator))\\b',\n",
    "]\n",
    "\n",
    "# --- Hazard type inference patterns (reuse from signal dictionary) ---\n",
    "HAZARD_TYPE_PATTERNS = {}\n",
    "for hazard, config in SIGNAL_DICT.get('hazard_type', {}).items():\n",
    "    HAZARD_TYPE_PATTERNS[hazard] = [\n",
    "        re.compile(p, re.IGNORECASE) for p in config.get('patterns', [])\n",
    "    ]\n",
    "\n",
    "# --- Exposure category patterns (reuse from signal dictionary) ---\n",
    "EXPOSURE_CATEGORY_PATTERNS = {}\n",
    "for cat, config in SIGNAL_DICT.get('exposure_category', {}).items():\n",
    "    EXPOSURE_CATEGORY_PATTERNS[cat] = [\n",
    "        re.compile(p, re.IGNORECASE) for p in config.get('patterns', [])\n",
    "    ]\n",
    "# Note: All 7 exposure categories (including economic_indicator and\n",
    "# development_index) are now defined in signal_dictionary.yaml\n",
    "\n",
    "print(\"Detection patterns defined.\")\n",
    "print(f\"  Function types: {list(FUNCTION_TYPE_PATTERNS.keys())}\")\n",
    "print(f\"  Approach patterns: {list(APPROACH_PATTERNS.keys())}\")\n",
    "print(f\"  Socio-economic indicators: {len(SOCIOECONOMIC_INDICATORS)}\")\n",
    "print(f\"  Hazard type patterns: {len(HAZARD_TYPE_PATTERNS)}\")\n",
    "print(f\"  Exposure category patterns: {len(EXPOSURE_CATEGORY_PATTERNS)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VULNERABILITY CONSTRAINT TABLES\n",
    "# =============================================================================\n",
    "#\n",
    "# Derived from RDLS v0.3 schema. Used by VulnerabilityExtractor to validate\n",
    "# field combinations for vulnerability function entries.\n",
    "#\n",
    "# Group 1: FUNCTION_TYPE_CONSTRAINTS\n",
    "#   function_type -> default impact_metric, quantity_kind, allowed metrics\n",
    "#\n",
    "# Group 2: VULN_CATEGORY_DEFAULTS\n",
    "#   exposure_category -> default function_type + metric overrides\n",
    "#   (Which function types / metrics make sense for each asset category)\n",
    "#\n",
    "# Group 3: Reuses IMPACT_METRIC_CONSTRAINTS from Loss section (cell 23)\n",
    "#   impact_metric -> (quantity_kind, allowed impact_types)\n",
    "#\n",
    "# Group 4: FUNCTION_TYPE_APPROACH_DEFAULTS\n",
    "#   function_type -> (typical_approach, typical_relationship)\n",
    "# =============================================================================\n",
    "\n",
    "# --- SHARED: impact_metric -> (quantity_kind, allowed_impact_types) ---\n",
    "# Shared between Vulnerability (cell 8) and Loss (cell 24) extractors.\n",
    "# Maps each of the 20 impact_metric values to its expected quantity_kind\n",
    "# and which impact_types it logically applies to.\n",
    "# quantity_kind is open codelist so we add 'ratio' (used in Chattogram example).\n",
    "IMPACT_METRIC_CONSTRAINTS = {\n",
    "    # Physical damage metrics (direct only)\n",
    "    'damage_ratio':               ('ratio',    {'direct'}),\n",
    "    'mean_damage_ratio':          ('ratio',    {'direct'}),\n",
    "    'damage_index':               ('count',    {'direct'}),\n",
    "    # Loss ratio metrics\n",
    "    'loss_ratio':                 ('ratio',    {'direct', 'indirect', 'total'}),\n",
    "    'mean_loss_ratio':            ('ratio',    {'direct', 'indirect', 'total'}),\n",
    "    # Probability\n",
    "    'probability':                ('ratio',    {'direct', 'indirect', 'total'}),\n",
    "    # Vulnerability metrics\n",
    "    'downtime_vulnerability':     ('time',     {'indirect'}),\n",
    "    'casualty_ratio_vulnerability': ('ratio',  {'direct'}),\n",
    "    # Economic/monetary loss metrics\n",
    "    'economic_loss_value':        ('monetary', {'direct', 'indirect', 'total'}),\n",
    "    'insured_loss_value':         ('monetary', {'direct', 'indirect', 'total'}),\n",
    "    'loss_annual_average_value':  ('monetary', {'total'}),\n",
    "    'loss_probable_maximum_value': ('monetary', {'total'}),\n",
    "    'at_risk_value':              ('monetary', {'total'}),\n",
    "    'at_risk_tail_value':         ('monetary', {'total'}),\n",
    "    'asset_loss':                 ('monetary', {'direct'}),\n",
    "    # Time-based loss\n",
    "    'downtime_loss':              ('time',     {'indirect'}),\n",
    "    # Count-based metrics\n",
    "    'casualty_count':             ('count',    {'direct'}),\n",
    "    'casualty_ratio_loss':        ('ratio',    {'direct'}),\n",
    "    'displaced_count':            ('count',    {'direct'}),\n",
    "    'exposure_to_hazard':         ('count',    {'direct'}),\n",
    "}\n",
    "\n",
    "# --- Group 1: function_type -> impact_metric + quantity_kind constraints ---\n",
    "# Each function type has a natural default metric. The 'allowed' set lists\n",
    "# all impact_metrics that are semantically valid for that function type.\n",
    "FUNCTION_TYPE_CONSTRAINTS = {\n",
    "    'vulnerability': {\n",
    "        'default_metric':    'damage_ratio',\n",
    "        'default_qty':       'ratio',\n",
    "        'allowed_metrics':   {\n",
    "            'damage_ratio', 'mean_damage_ratio',       # physical damage\n",
    "            'casualty_ratio_vulnerability',             # human vulnerability\n",
    "            'downtime_vulnerability',                   # service disruption\n",
    "        },\n",
    "    },\n",
    "    'fragility': {\n",
    "        'default_metric':    'probability',\n",
    "        'default_qty':       'ratio',\n",
    "        'allowed_metrics':   {\n",
    "            'probability',                              # P(damage_state | intensity)\n",
    "            'damage_index',                             # ordinal damage state\n",
    "            'damage_ratio', 'mean_damage_ratio',        # continuous damage\n",
    "        },\n",
    "    },\n",
    "    'damage_to_loss': {\n",
    "        'default_metric':    'loss_ratio',\n",
    "        'default_qty':       'ratio',\n",
    "        'allowed_metrics':   {\n",
    "            'loss_ratio', 'mean_loss_ratio',            # fractional loss\n",
    "            'economic_loss_value', 'insured_loss_value', # absolute monetary\n",
    "            'asset_loss',                                # asset replacement\n",
    "            'downtime_loss',                             # service interruption\n",
    "            'casualty_count', 'displaced_count',         # human consequences\n",
    "        },\n",
    "    },\n",
    "    'engineering_demand': {\n",
    "        'default_metric':    'damage_index',\n",
    "        'default_qty':       'count',\n",
    "        'allowed_metrics':   {\n",
    "            'damage_index',                             # ordinal state\n",
    "            'damage_ratio', 'mean_damage_ratio',        # continuous damage\n",
    "            'probability',                              # exceedance\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Group 2: category -> default function type & metric overrides ---\n",
    "# When a category is detected, what function behaviour makes most sense?\n",
    "VULN_CATEGORY_DEFAULTS = {\n",
    "    'buildings': {\n",
    "        'typical_function':  'fragility',\n",
    "        'metric_override':   'damage_ratio',\n",
    "        'qty_override':      'ratio',\n",
    "    },\n",
    "    'infrastructure': {\n",
    "        'typical_function':  'vulnerability',\n",
    "        'metric_override':   'downtime_vulnerability',\n",
    "        'qty_override':      'time',\n",
    "    },\n",
    "    'population': {\n",
    "        'typical_function':  'vulnerability',\n",
    "        'metric_override':   'casualty_ratio_vulnerability',\n",
    "        'qty_override':      'ratio',\n",
    "    },\n",
    "    'agriculture': {\n",
    "        'typical_function':  'vulnerability',\n",
    "        'metric_override':   'damage_ratio',\n",
    "        'qty_override':      'ratio',\n",
    "    },\n",
    "    'natural_environment': {\n",
    "        'typical_function':  'vulnerability',\n",
    "        'metric_override':   'damage_ratio',\n",
    "        'qty_override':      'ratio',\n",
    "    },\n",
    "    'economic_indicator': {\n",
    "        'typical_function':  'damage_to_loss',\n",
    "        'metric_override':   'economic_loss_value',\n",
    "        'qty_override':      'monetary',\n",
    "    },\n",
    "    'development_index': {\n",
    "        'typical_function':  'vulnerability',\n",
    "        'metric_override':   'damage_index',\n",
    "        'qty_override':      'count',\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Group 4: function_type -> typical approach + relationship ---\n",
    "# Fragility functions are usually parametric (lognormal CDF);\n",
    "# vulnerability functions are often discrete (depth-damage tables).\n",
    "FUNCTION_TYPE_APPROACH_DEFAULTS = {\n",
    "    'vulnerability': {\n",
    "        'typical_approach':     'empirical',\n",
    "        'typical_relationship': 'discrete',\n",
    "    },\n",
    "    'fragility': {\n",
    "        'typical_approach':     'analytical',\n",
    "        'typical_relationship': 'math_parametric',\n",
    "    },\n",
    "    'damage_to_loss': {\n",
    "        'typical_approach':     'empirical',\n",
    "        'typical_relationship': 'discrete',\n",
    "    },\n",
    "    'engineering_demand': {\n",
    "        'typical_approach':     'analytical',\n",
    "        'typical_relationship': 'math_parametric',\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"\\nVulnerability constraint tables defined.\")\n",
    "print(f\"  Group 1 - Function type constraints: {len(FUNCTION_TYPE_CONSTRAINTS)} types\")\n",
    "print(f\"  Group 2 - Category defaults: {len(VULN_CATEGORY_DEFAULTS)} categories\")\n",
    "print(f\"  Group 3 - Reuses IMPACT_METRIC_CONSTRAINTS from Loss section\")\n",
    "print(f\"  Group 4 - Approach defaults: {len(FUNCTION_TYPE_APPROACH_DEFAULTS)} types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa9b96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data classes defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Data Classes\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class FunctionExtraction:\n",
    "    \"\"\"Extraction result for a single vulnerability function.\"\"\"\n",
    "    function_type: str                  # vulnerability, fragility, damage_to_loss, engineering_demand\n",
    "    approach: str = 'empirical'         # function_approach codelist\n",
    "    relationship: str = 'discrete'      # relationship_type codelist\n",
    "    hazard_primary: Optional[str] = None  # hazard_type codelist\n",
    "    hazard_secondary: Optional[str] = None\n",
    "    hazard_process_primary: Optional[str] = None\n",
    "    hazard_process_secondary: Optional[str] = None\n",
    "    hazard_analysis_type: str = 'empirical'  # analysis_type codelist\n",
    "    intensity_measure: Optional[str] = None\n",
    "    category: Optional[str] = None       # exposure_category codelist\n",
    "    impact_type: str = 'direct'          # impact_type codelist\n",
    "    impact_modelling: str = 'observed'   # data_calculation_type codelist\n",
    "    impact_metric: Optional[str] = None  # impact_metric codelist\n",
    "    quantity_kind: str = 'ratio'         # open codelist\n",
    "    taxonomy: Optional[str] = None       # taxonomy codelist\n",
    "    analysis_details: Optional[str] = None\n",
    "    damage_scale_name: Optional[str] = None\n",
    "    damage_states_names: Optional[List[str]] = None\n",
    "    parameter: Optional[str] = None      # engineering_demand only\n",
    "    confidence: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class SocioEconomicExtraction:\n",
    "    \"\"\"Extraction result for a socio-economic indicator.\"\"\"\n",
    "    indicator_name: str\n",
    "    indicator_code: str\n",
    "    description: str\n",
    "    reference_year: Optional[int] = None\n",
    "    scheme: str = 'Custom'\n",
    "    threshold: Optional[str] = None\n",
    "    uri: Optional[str] = None\n",
    "    analysis_details: Optional[str] = None\n",
    "    confidence: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class VulnerabilityExtraction:\n",
    "    \"\"\"Complete vulnerability extraction for a dataset.\"\"\"\n",
    "    functions: List[FunctionExtraction] = field(default_factory=list)\n",
    "    socio_economic: List[SocioEconomicExtraction] = field(default_factory=list)\n",
    "    overall_confidence: float = 0.0\n",
    "\n",
    "    def has_any_signal(self) -> bool:\n",
    "        return len(self.functions) > 0 or len(self.socio_economic) > 0\n",
    "\n",
    "print(\"Data classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a55549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VulnerabilityExtractor initialized (constraint-validated).\n",
      "  Function type patterns: 4\n",
      "  Socio-economic indicators: 18\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.3 VulnerabilityExtractor Class\n",
    "\n",
    "Main extraction engine using signal dictionary + schema constants.\n",
    "All inferred field combinations are validated against constraint tables:\n",
    "  Group 1: FUNCTION_TYPE_CONSTRAINTS (function_type -> metrics)\n",
    "  Group 2: VULN_CATEGORY_DEFAULTS (category -> function type + metric overrides)\n",
    "  Group 3: IMPACT_METRIC_CONSTRAINTS (metric -> quantity_kind + impact_type)\n",
    "  Group 4: FUNCTION_TYPE_APPROACH_DEFAULTS (function_type -> approach + relationship)\n",
    "\"\"\"\n",
    "\n",
    "class VulnerabilityExtractor:\n",
    "    \"\"\"\n",
    "    Extracts RDLS Vulnerability block components from HDX metadata.\n",
    "\n",
    "    Two pathways:\n",
    "    1. Functions: Detect vulnerability/fragility/damage_to_loss/engineering_demand\n",
    "       curves, infer all mandatory fields from text + hazard cross-reference.\n",
    "    2. Socio-economic: Detect indicator datasets (poverty, HDI, SVI, etc.),\n",
    "       extract indicator_name, indicator_code, description, reference_year.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, signal_dict: Dict[str, Any], hazard_xref: Dict[str, Dict]):\n",
    "        self.signal_dict = signal_dict\n",
    "        self.hazard_xref = hazard_xref\n",
    "        self._compile_patterns()\n",
    "\n",
    "    def _compile_patterns(self) -> None:\n",
    "        \"\"\"Pre-compile regex patterns.\"\"\"\n",
    "        self.func_type_patterns = {}\n",
    "        for ftype, patterns in FUNCTION_TYPE_PATTERNS.items():\n",
    "            self.func_type_patterns[ftype] = [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "\n",
    "        self.approach_patterns = {}\n",
    "        for approach, patterns in APPROACH_PATTERNS.items():\n",
    "            self.approach_patterns[approach] = [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "\n",
    "        self.relationship_patterns = {}\n",
    "        for rel, patterns in RELATIONSHIP_PATTERNS.items():\n",
    "            self.relationship_patterns[rel] = [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "\n",
    "        self.impact_type_patterns = {}\n",
    "        for itype, patterns in IMPACT_TYPE_PATTERNS.items():\n",
    "            self.impact_type_patterns[itype] = [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "\n",
    "        self.impact_modelling_patterns = {}\n",
    "        for mod, patterns in IMPACT_MODELLING_PATTERNS.items():\n",
    "            self.impact_modelling_patterns[mod] = [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "\n",
    "        self.socio_indicator_patterns = []\n",
    "        for ind in SOCIOECONOMIC_INDICATORS:\n",
    "            compiled = [re.compile(p, re.IGNORECASE) for p in ind['patterns']]\n",
    "            self.socio_indicator_patterns.append({**ind, 'compiled': compiled})\n",
    "\n",
    "        self.generic_socio_patterns = [re.compile(p, re.IGNORECASE) for p in GENERIC_SOCIOECONOMIC_PATTERNS]\n",
    "\n",
    "    def _get_all_text(self, record: Dict[str, Any]) -> str:\n",
    "        \"\"\"Concatenate all searchable text fields for pattern matching.\n",
    "\n",
    "        Note: methodology_other is deliberately excluded. It describes\n",
    "        how analysis was performed (e.g. 'vulnerability models used in\n",
    "        risk calculation'), not what data the dataset contains. Including\n",
    "        it causes false positives where methodology text about risk\n",
    "        assessment triggers vulnerability/loss detection on hazard-only\n",
    "        datasets.\n",
    "        \"\"\"\n",
    "        parts = [\n",
    "            record.get('title', ''),\n",
    "            record.get('name', ''),\n",
    "            record.get('notes', ''),\n",
    "        ]\n",
    "        for tag in record.get('tags', []):\n",
    "            if isinstance(tag, dict):\n",
    "                parts.append(tag.get('name', ''))\n",
    "            elif isinstance(tag, str):\n",
    "                parts.append(tag)\n",
    "        for r in record.get('resources', []):\n",
    "            parts.append(r.get('name', '') or '')\n",
    "            parts.append(r.get('description', '') or '')\n",
    "        return ' '.join(filter(None, parts))\n",
    "\n",
    "    def _detect_function_types(self, text: str) -> List[str]:\n",
    "        \"\"\"Detect which vulnerability function types are present.\"\"\"\n",
    "        detected = []\n",
    "        for ftype, patterns in self.func_type_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    detected.append(ftype)\n",
    "                    break\n",
    "        return detected\n",
    "\n",
    "    def _infer_approach(self, text: str, function_type: str) -> str:\n",
    "        \"\"\"\n",
    "        Infer function approach from text, with function-type defaults.\n",
    "        Uses Group 4 defaults when text provides no signal.\n",
    "        \"\"\"\n",
    "        scores = {k: 0 for k in VALID_FUNCTION_APPROACHES}\n",
    "        for approach, patterns in self.approach_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    scores[approach] += 1\n",
    "        best = max(scores, key=scores.get)\n",
    "        if scores[best] > 0:\n",
    "            return best\n",
    "        # Fallback to Group 4 default for this function type\n",
    "        defaults = FUNCTION_TYPE_APPROACH_DEFAULTS.get(function_type, {})\n",
    "        return defaults.get('typical_approach', 'empirical')\n",
    "\n",
    "    def _infer_relationship(self, text: str, function_type: str) -> str:\n",
    "        \"\"\"\n",
    "        Infer relationship type from text, with function-type defaults.\n",
    "        Uses Group 4 defaults when text provides no signal.\n",
    "        \"\"\"\n",
    "        scores = {k: 0 for k in VALID_RELATIONSHIP_TYPES}\n",
    "        for rel, patterns in self.relationship_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    scores[rel] += 1\n",
    "        best = max(scores, key=scores.get)\n",
    "        if scores[best] > 0:\n",
    "            return best\n",
    "        # Fallback to Group 4 default\n",
    "        defaults = FUNCTION_TYPE_APPROACH_DEFAULTS.get(function_type, {})\n",
    "        return defaults.get('typical_relationship', 'discrete')\n",
    "\n",
    "    def _infer_hazard_context(self, record: Dict[str, Any], text: str) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"Infer hazard context from cross-reference or text.\"\"\"\n",
    "        dataset_id = record.get('id', '')\n",
    "\n",
    "        if dataset_id in self.hazard_xref:\n",
    "            xref = self.hazard_xref[dataset_id]\n",
    "            ht_list = [h for h in xref['hazard_types'] if h in VALID_HAZARD_TYPES]\n",
    "            pt_list = [p for p in xref['process_types'] if p in VALID_PROCESS_TYPES]\n",
    "            at = xref.get('analysis_type')\n",
    "            im_list = xref.get('intensity_measures', [])\n",
    "\n",
    "            result = {\n",
    "                'hazard_primary': ht_list[0] if ht_list else None,\n",
    "                'hazard_secondary': ht_list[1] if len(ht_list) > 1 else None,\n",
    "                'hazard_process_primary': pt_list[0] if pt_list else None,\n",
    "                'hazard_analysis_type': at if at in VALID_ANALYSIS_TYPES else 'empirical',\n",
    "                'intensity_measure': im_list[0] if im_list else None,\n",
    "            }\n",
    "            if not result['hazard_process_primary'] and result['hazard_primary']:\n",
    "                result['hazard_process_primary'] = HAZARD_PROCESS_DEFAULT.get(result['hazard_primary'])\n",
    "            if not result['intensity_measure'] and result['hazard_primary']:\n",
    "                result['intensity_measure'] = DEFAULT_INTENSITY_MEASURE.get(result['hazard_primary'])\n",
    "            return result\n",
    "\n",
    "        text_lower = text.lower()\n",
    "        hazard_primary = None\n",
    "        for ht, patterns in HAZARD_TYPE_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text_lower):\n",
    "                    hazard_primary = ht\n",
    "                    break\n",
    "            if hazard_primary:\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            'hazard_primary': hazard_primary,\n",
    "            'hazard_secondary': None,\n",
    "            'hazard_process_primary': HAZARD_PROCESS_DEFAULT.get(hazard_primary) if hazard_primary else None,\n",
    "            'hazard_analysis_type': 'empirical',\n",
    "            'intensity_measure': DEFAULT_INTENSITY_MEASURE.get(hazard_primary) if hazard_primary else None,\n",
    "        }\n",
    "\n",
    "    def _infer_category(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Infer exposure category from text.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        for cat, patterns in EXPOSURE_CATEGORY_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text_lower):\n",
    "                    if cat in VALID_EXPOSURE_CATEGORIES:\n",
    "                        return cat\n",
    "        return 'buildings'  # Default for vulnerability functions\n",
    "\n",
    "    def _infer_impact_type(self, text: str) -> str:\n",
    "        \"\"\"Infer impact type from text.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        for itype, patterns in self.impact_type_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text_lower):\n",
    "                    return itype\n",
    "        return 'direct'\n",
    "\n",
    "    def _infer_impact_modelling(self, text: str) -> str:\n",
    "        \"\"\"Infer impact modelling method from text.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        scores = {k: 0 for k in VALID_CALCULATION_TYPES}\n",
    "        for mod, patterns in self.impact_modelling_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text_lower):\n",
    "                    scores[mod] += 1\n",
    "        best = max(scores, key=scores.get)\n",
    "        return best if scores[best] > 0 else 'observed'\n",
    "\n",
    "    def _validate_function_metrics(self, function_type: str, category: str,\n",
    "                                    impact_metric: str, quantity_kind: str,\n",
    "                                    impact_type: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Validate impact_metric + quantity_kind against constraint tables.\n",
    "\n",
    "        Checks:\n",
    "        1. Group 1: Is impact_metric allowed for this function_type?\n",
    "        2. Group 3: Does quantity_kind match the metric? (IMPACT_METRIC_CONSTRAINTS)\n",
    "        3. Group 3: Does impact_type match the metric?\n",
    "\n",
    "        Falls back to Group 2 category defaults if function_type constraints fail.\n",
    "        \"\"\"\n",
    "        ftype_constraints = FUNCTION_TYPE_CONSTRAINTS.get(function_type)\n",
    "\n",
    "        if ftype_constraints:\n",
    "            # Check if the impact_metric is in the allowed set for this function type\n",
    "            if impact_metric not in ftype_constraints['allowed_metrics']:\n",
    "                # Try category override (Group 2)\n",
    "                cat_defaults = VULN_CATEGORY_DEFAULTS.get(category, {})\n",
    "                override_metric = cat_defaults.get('metric_override')\n",
    "                if override_metric and override_metric in ftype_constraints['allowed_metrics']:\n",
    "                    impact_metric = override_metric\n",
    "                    quantity_kind = cat_defaults.get('qty_override', ftype_constraints['default_qty'])\n",
    "                else:\n",
    "                    # Fall back to function type default\n",
    "                    impact_metric = ftype_constraints['default_metric']\n",
    "                    quantity_kind = ftype_constraints['default_qty']\n",
    "\n",
    "        # Group 3: validate metric -> quantity_kind + impact_type\n",
    "        # (IMPACT_METRIC_CONSTRAINTS is defined in cell 23 for Loss; reuse it)\n",
    "        metric_constraint = IMPACT_METRIC_CONSTRAINTS.get(impact_metric)\n",
    "        if metric_constraint:\n",
    "            expected_qty, allowed_types = metric_constraint\n",
    "            if quantity_kind != expected_qty:\n",
    "                quantity_kind = expected_qty\n",
    "            if impact_type not in allowed_types:\n",
    "                impact_type = 'direct' if 'direct' in allowed_types else sorted(allowed_types)[0]\n",
    "\n",
    "        return {\n",
    "            'impact_metric': impact_metric,\n",
    "            'quantity_kind': quantity_kind,\n",
    "            'impact_type': impact_type,\n",
    "        }\n",
    "\n",
    "    def _extract_reference_year(self, record: Dict[str, Any]) -> Optional[int]:\n",
    "        \"\"\"Extract reference year from dataset metadata.\"\"\"\n",
    "        dataset_date = record.get('dataset_date', '') or ''\n",
    "        year_match = re.search(r'(\\d{4})', dataset_date)\n",
    "        if year_match:\n",
    "            year = int(year_match.group(1))\n",
    "            if 1900 <= year <= 2100:\n",
    "                return year\n",
    "        last_mod = record.get('last_modified', '') or record.get('metadata_modified', '') or ''\n",
    "        year_match = re.search(r'(\\d{4})', last_mod)\n",
    "        if year_match:\n",
    "            year = int(year_match.group(1))\n",
    "            if 1900 <= year <= 2100:\n",
    "                return year\n",
    "        return None\n",
    "\n",
    "    def _extract_functions(self, record: Dict[str, Any], text: str) -> List[FunctionExtraction]:\n",
    "        \"\"\"Extract vulnerability function information with constraint validation.\"\"\"\n",
    "        func_types = self._detect_function_types(text)\n",
    "        if not func_types:\n",
    "            return []\n",
    "\n",
    "        # Infer shared context\n",
    "        hazard_ctx = self._infer_hazard_context(record, text)\n",
    "        category = self._infer_category(text)\n",
    "        text_impact_type = self._infer_impact_type(text)\n",
    "        impact_modelling = self._infer_impact_modelling(text)\n",
    "\n",
    "        title = record.get('title', '')\n",
    "        notes = record.get('notes', '')\n",
    "        analysis_details = f\"Extracted from HDX dataset: {title[:200]}\"\n",
    "        if notes:\n",
    "            analysis_details += f\". {notes[:300]}\"\n",
    "\n",
    "        functions = []\n",
    "        for ftype in func_types:\n",
    "            # Group 4: approach + relationship defaults per function type\n",
    "            approach = self._infer_approach(text, ftype)\n",
    "            relationship = self._infer_relationship(text, ftype)\n",
    "\n",
    "            # Group 1: default metric for this function type\n",
    "            ftype_constraints = FUNCTION_TYPE_CONSTRAINTS.get(ftype, {})\n",
    "            default_metric = ftype_constraints.get('default_metric', 'damage_ratio')\n",
    "            default_qty = ftype_constraints.get('default_qty', 'ratio')\n",
    "\n",
    "            # Group 2: category may override metric\n",
    "            cat_defaults = VULN_CATEGORY_DEFAULTS.get(category, {})\n",
    "            impact_metric = default_metric\n",
    "            quantity_kind = default_qty\n",
    "\n",
    "            # If category has a metric override that's valid for this function type\n",
    "            cat_metric = cat_defaults.get('metric_override')\n",
    "            allowed = ftype_constraints.get('allowed_metrics', set())\n",
    "            if cat_metric and cat_metric in allowed:\n",
    "                impact_metric = cat_metric\n",
    "                quantity_kind = cat_defaults.get('qty_override', default_qty)\n",
    "\n",
    "            # Group 1+3: validate the full combination\n",
    "            validated = self._validate_function_metrics(\n",
    "                ftype, category, impact_metric, quantity_kind, text_impact_type\n",
    "            )\n",
    "\n",
    "            func = FunctionExtraction(\n",
    "                function_type=ftype,\n",
    "                approach=approach,\n",
    "                relationship=relationship,\n",
    "                hazard_primary=hazard_ctx['hazard_primary'],\n",
    "                hazard_secondary=hazard_ctx['hazard_secondary'],\n",
    "                hazard_process_primary=hazard_ctx['hazard_process_primary'],\n",
    "                hazard_analysis_type=hazard_ctx['hazard_analysis_type'] or 'empirical',\n",
    "                intensity_measure=hazard_ctx['intensity_measure'],\n",
    "                category=category,\n",
    "                impact_type=validated['impact_type'],\n",
    "                impact_modelling=impact_modelling,\n",
    "                impact_metric=validated['impact_metric'],\n",
    "                quantity_kind=validated['quantity_kind'],\n",
    "                analysis_details=analysis_details[:500],\n",
    "                confidence=0.8 if hazard_ctx['hazard_primary'] else 0.6,\n",
    "            )\n",
    "            functions.append(func)\n",
    "\n",
    "        return functions\n",
    "\n",
    "    def _extract_socio_economic(self, record: Dict[str, Any], text: str) -> List[SocioEconomicExtraction]:\n",
    "        \"\"\"Extract socio-economic indicator information.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        indicators = []\n",
    "        matched_codes = set()\n",
    "\n",
    "        for ind_def in self.socio_indicator_patterns:\n",
    "            for p in ind_def['compiled']:\n",
    "                if p.search(text_lower):\n",
    "                    code = ind_def['indicator_code']\n",
    "                    if code not in matched_codes:\n",
    "                        matched_codes.add(code)\n",
    "                        ref_year = self._extract_reference_year(record)\n",
    "                        title = record.get('title', '')\n",
    "                        description = ind_def['description']\n",
    "                        if title:\n",
    "                            description = f\"{description}. Source: {title[:150]}\"\n",
    "\n",
    "                        indicators.append(SocioEconomicExtraction(\n",
    "                            indicator_name=ind_def['indicator_name'],\n",
    "                            indicator_code=code,\n",
    "                            description=description[:500],\n",
    "                            reference_year=ref_year,\n",
    "                            scheme=ind_def['scheme'],\n",
    "                            confidence=0.7,\n",
    "                        ))\n",
    "                    break\n",
    "\n",
    "        if not indicators:\n",
    "            for p in self.generic_socio_patterns:\n",
    "                if p.search(text_lower):\n",
    "                    title = record.get('title', '')\n",
    "                    ref_year = self._extract_reference_year(record)\n",
    "                    indicators.append(SocioEconomicExtraction(\n",
    "                        indicator_name='Socio-economic vulnerability indicator',\n",
    "                        indicator_code='SOCIO_VULN',\n",
    "                        description=f\"Socio-economic vulnerability data from: {title[:200]}\",\n",
    "                        reference_year=ref_year,\n",
    "                        scheme='Custom',\n",
    "                        confidence=0.5,\n",
    "                    ))\n",
    "                    break\n",
    "\n",
    "        # --- Single-indicator false positive filter ---\n",
    "        # A single generic indicator is insufficient evidence for vulnerability.\n",
    "        # DISPLACEMENT alone belongs in loss, POP_DENSITY alone belongs in\n",
    "        # exposure, and standalone SOCIO_VULN is too ambiguous.\n",
    "        # Require either >=2 distinct indicators or at least 1 specific\n",
    "        # composite indicator to flag as vulnerability.\n",
    "        SINGLE_INDICATOR_INSUFFICIENT = {\n",
    "            'DISPLACEMENT', 'POP_DENSITY', 'SOCIO_VULN',\n",
    "        }\n",
    "        if len(indicators) == 1 and indicators[0].indicator_code in SINGLE_INDICATOR_INSUFFICIENT:\n",
    "            return []  # Not enough evidence for vulnerability\n",
    "\n",
    "        return indicators\n",
    "\n",
    "    def extract(self, record: Dict[str, Any]) -> VulnerabilityExtraction:\n",
    "        \"\"\"Extract vulnerability information from HDX record.\"\"\"\n",
    "        text = self._get_all_text(record)\n",
    "\n",
    "        functions = self._extract_functions(record, text)\n",
    "        socio_economic = self._extract_socio_economic(record, text)\n",
    "\n",
    "        confidences = [f.confidence for f in functions] + [s.confidence for s in socio_economic]\n",
    "        overall = float(np.mean(confidences)) if confidences else 0.0\n",
    "\n",
    "        return VulnerabilityExtraction(\n",
    "            functions=functions,\n",
    "            socio_economic=socio_economic,\n",
    "            overall_confidence=overall,\n",
    "        )\n",
    "\n",
    "# Initialize\n",
    "vuln_extractor = VulnerabilityExtractor(SIGNAL_DICT, HAZARD_XREF)\n",
    "print(\"VulnerabilityExtractor initialized (constraint-validated).\")\n",
    "print(f\"  Function type patterns: {len(vuln_extractor.func_type_patterns)}\")\n",
    "print(f\"  Socio-economic indicators: {len(vuln_extractor.socio_indicator_patterns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc724bc",
   "metadata": {},
   "source": [
    "## 3. RDLS Vulnerability Block Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac662fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vulnerability block builder defined (constraint-validated).\n",
      "  - Group 1: impact_metric validated for function_type\n",
      "  - Group 3: quantity_kind + impact_type validated for impact_metric\n",
      "  - Group 4: approach + relationship defaults per function type\n",
      "  - All entries validated against schema codelists\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.1 Build RDLS Vulnerability Block\n",
    "\n",
    "Builds schema-compliant vulnerability block with:\n",
    "- functions: vulnerability, fragility, damage_to_loss, engineering_demand\n",
    "- socio_economic: indicator entries with all required fields\n",
    "- All function entries validated against constraint tables:\n",
    "  Group 1: impact_metric valid for function_type\n",
    "  Group 3: quantity_kind valid for impact_metric\n",
    "\"\"\"\n",
    "\n",
    "def build_vulnerability_block(\n",
    "    extraction: VulnerabilityExtraction,\n",
    "    dataset_id: str,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS vulnerability block from extraction results.\n",
    "\n",
    "    All function entries include 10+ mandatory fields validated against schema\n",
    "    AND constraint tables. All socio-economic entries include indicator_name,\n",
    "    indicator_code, description, reference_year, and id.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    extraction : VulnerabilityExtraction\n",
    "        Extraction results (already constraint-validated by extractor)\n",
    "    dataset_id : str\n",
    "        Dataset identifier for building unique IDs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optional[Dict[str, Any]]\n",
    "        RDLS vulnerability block or None if no data\n",
    "    \"\"\"\n",
    "    if not extraction.has_any_signal():\n",
    "        return None\n",
    "\n",
    "    block = {}\n",
    "\n",
    "    # --- Build functions ---\n",
    "    if extraction.functions:\n",
    "        functions_dict = {\n",
    "            'vulnerability': [],\n",
    "            'fragility': [],\n",
    "            'damage_to_loss': [],\n",
    "            'engineering_demand': [],\n",
    "        }\n",
    "\n",
    "        for idx, func in enumerate(extraction.functions):\n",
    "            ftype = func.function_type\n",
    "            if ftype not in functions_dict:\n",
    "                continue\n",
    "\n",
    "            # --- Codelist validation ---\n",
    "            approach = func.approach if func.approach in VALID_FUNCTION_APPROACHES else 'empirical'\n",
    "            relationship = func.relationship if func.relationship in VALID_RELATIONSHIP_TYPES else 'discrete'\n",
    "            hazard_analysis_type = func.hazard_analysis_type if func.hazard_analysis_type in VALID_ANALYSIS_TYPES else 'empirical'\n",
    "            impact_type = func.impact_type if func.impact_type in VALID_IMPACT_TYPES else 'direct'\n",
    "            impact_modelling = func.impact_modelling if func.impact_modelling in VALID_CALCULATION_TYPES else 'observed'\n",
    "            impact_metric = func.impact_metric if func.impact_metric in VALID_IMPACT_METRICS else DEFAULT_IMPACT_METRIC.get(ftype, 'damage_ratio')\n",
    "            category = func.category if func.category in VALID_EXPOSURE_CATEGORIES else None\n",
    "\n",
    "            # --- Group 1 re-validation: metric valid for function type ---\n",
    "            ftype_constraints = FUNCTION_TYPE_CONSTRAINTS.get(ftype)\n",
    "            if ftype_constraints and impact_metric not in ftype_constraints['allowed_metrics']:\n",
    "                impact_metric = ftype_constraints['default_metric']\n",
    "                quantity_kind = ftype_constraints['default_qty']\n",
    "            else:\n",
    "                quantity_kind = func.quantity_kind or 'ratio'\n",
    "\n",
    "            # --- Group 3 re-validation: quantity_kind for metric ---\n",
    "            metric_constraint = IMPACT_METRIC_CONSTRAINTS.get(impact_metric)\n",
    "            if metric_constraint:\n",
    "                expected_qty, allowed_types = metric_constraint\n",
    "                quantity_kind = expected_qty\n",
    "                if impact_type not in allowed_types:\n",
    "                    impact_type = 'direct' if 'direct' in allowed_types else sorted(allowed_types)[0]\n",
    "\n",
    "            # --- Group 4: approach/relationship defaults if not text-detected ---\n",
    "            ft_defaults = FUNCTION_TYPE_APPROACH_DEFAULTS.get(ftype, {})\n",
    "\n",
    "            entry = {\n",
    "                'approach': approach,\n",
    "                'relationship': relationship,\n",
    "                'hazard_primary': func.hazard_primary if func.hazard_primary in VALID_HAZARD_TYPES else None,\n",
    "                'hazard_analysis_type': hazard_analysis_type,\n",
    "                'intensity_measure': func.intensity_measure or DEFAULT_INTENSITY_MEASURE.get(func.hazard_primary or 'flood', 'wd:m'),\n",
    "                'category': category,\n",
    "                'impact_type': impact_type,\n",
    "                'impact_modelling': impact_modelling,\n",
    "                'impact_metric': impact_metric,\n",
    "                'quantity_kind': quantity_kind,\n",
    "                'id': f\"vuln_func_{dataset_id[:8]}_{ftype}_{idx + 1}\",\n",
    "            }\n",
    "\n",
    "            # P1+P2 fix: Skip entries with no determinable hazard or category\n",
    "            if entry['hazard_primary'] is None or entry['category'] is None:\n",
    "                continue\n",
    "\n",
    "            # Optional standard fields\n",
    "            if func.hazard_secondary and func.hazard_secondary in VALID_HAZARD_TYPES:\n",
    "                entry['hazard_secondary'] = func.hazard_secondary\n",
    "            if func.hazard_process_primary and func.hazard_process_primary in VALID_PROCESS_TYPES:\n",
    "                entry['hazard_process_primary'] = func.hazard_process_primary\n",
    "            if func.hazard_process_secondary and func.hazard_process_secondary in VALID_PROCESS_TYPES:\n",
    "                entry['hazard_process_secondary'] = func.hazard_process_secondary\n",
    "            if func.taxonomy and func.taxonomy in VALID_TAXONOMIES:\n",
    "                entry['taxonomy'] = func.taxonomy\n",
    "            if func.analysis_details:\n",
    "                entry['analysis_details'] = func.analysis_details\n",
    "\n",
    "            # Type-specific fields\n",
    "            if ftype in ('fragility', 'damage_to_loss', 'engineering_demand'):\n",
    "                if func.damage_scale_name:\n",
    "                    entry['damage_scale_name'] = func.damage_scale_name\n",
    "                if func.damage_states_names:\n",
    "                    entry['damage_states_names'] = func.damage_states_names\n",
    "            if ftype == 'engineering_demand' and func.parameter:\n",
    "                entry['parameter'] = func.parameter\n",
    "\n",
    "            functions_dict[ftype].append(entry)\n",
    "\n",
    "        non_empty = {k: v for k, v in functions_dict.items() if v}\n",
    "        if non_empty:\n",
    "            block['functions'] = non_empty\n",
    "\n",
    "    # --- Build socio_economic ---\n",
    "    if extraction.socio_economic:\n",
    "        socio_list = []\n",
    "        for idx, se in enumerate(extraction.socio_economic):\n",
    "            entry = {\n",
    "                'indicator_name': se.indicator_name,\n",
    "                'indicator_code': se.indicator_code,\n",
    "                'description': se.description,\n",
    "                'id': f\"socio_{dataset_id[:8]}_{idx + 1}\",\n",
    "            }\n",
    "\n",
    "            if se.reference_year and 1900 <= se.reference_year <= 2100:\n",
    "                entry['reference_year'] = se.reference_year\n",
    "            else:\n",
    "                entry['reference_year'] = datetime.now().year\n",
    "\n",
    "            if se.scheme and se.scheme in VALID_TAXONOMIES:\n",
    "                entry['scheme'] = se.scheme\n",
    "            elif se.scheme == 'Custom':\n",
    "                entry['scheme'] = 'Custom'\n",
    "\n",
    "            if se.threshold:\n",
    "                entry['threshold'] = se.threshold\n",
    "            if se.uri:\n",
    "                entry['uri'] = se.uri\n",
    "            if se.analysis_details:\n",
    "                entry['analysis_details'] = se.analysis_details\n",
    "\n",
    "            socio_list.append(entry)\n",
    "\n",
    "        block['socio_economic'] = socio_list\n",
    "\n",
    "    return block if block else None\n",
    "\n",
    "\n",
    "print(\"Vulnerability block builder defined (constraint-validated).\")\n",
    "print(\"  - Group 1: impact_metric validated for function_type\")\n",
    "print(\"  - Group 3: quantity_kind + impact_type validated for impact_metric\")\n",
    "print(\"  - Group 4: approach + relationship defaults per function type\")\n",
    "print(\"  - All entries validated against schema codelists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b3c22",
   "metadata": {},
   "source": [
    "## 4. Test Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa0e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57 unique test samples across 7 categories.\n",
      "\n",
      "Samples per category:\n",
      "  vulnerability_function: 10\n",
      "  socioeconomic_poverty: 5\n",
      "  socioeconomic_health: 5\n",
      "  socioeconomic_displacement: 9\n",
      "  socioeconomic_index: 8\n",
      "  socioeconomic_food: 10\n",
      "  edge_cases: 10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.1 Load Curated Test Samples\n",
    "\n",
    "Organized by expected vulnerability type to stress-test extraction.\n",
    "\"\"\"\n",
    "\n",
    "VULN_TEST_SAMPLES = {\n",
    "    'vulnerability_function': [\n",
    "        # Datasets likely to have vulnerability/fragility function signals\n",
    "        ('vulnerability', 'Vulnerability function signals'),\n",
    "        ('fragility', 'Fragility function signals'),\n",
    "        ('damage', 'Damage assessment/function signals'),\n",
    "    ],\n",
    "    'socioeconomic_poverty': [\n",
    "        ('poverty', 'Poverty indicators'),\n",
    "        ('deprivation', 'Deprivation/inequality'),\n",
    "    ],\n",
    "    'socioeconomic_health': [\n",
    "        ('nutrition', 'Nutrition/health vulnerability'),\n",
    "        ('malnutrition', 'Malnutrition signals'),\n",
    "    ],\n",
    "    'socioeconomic_displacement': [\n",
    "        ('displacement', 'Displacement data'),\n",
    "        ('idp', 'IDP settlement data'),\n",
    "    ],\n",
    "    'socioeconomic_index': [\n",
    "        ('vulnerability-index', 'Vulnerability indices'),\n",
    "        ('resilience', 'Resilience indices'),\n",
    "        ('inform', 'INFORM risk index'),\n",
    "    ],\n",
    "    'socioeconomic_food': [\n",
    "        ('food-security', 'Food security assessments'),\n",
    "        ('ipc', 'IPC classification'),\n",
    "    ],\n",
    "    'edge_cases': [\n",
    "        ('risk', 'Risk datasets â€” may or may not have vulnerability'),\n",
    "        ('climate', 'Climate datasets â€” edge case'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Load samples by searching filenames\n",
    "sample_records = []\n",
    "sample_meta = []\n",
    "loaded_ids = set()\n",
    "\n",
    "for category, keyword_list in VULN_TEST_SAMPLES.items():\n",
    "    for keyword, note in keyword_list:\n",
    "        files = sorted(DATASET_METADATA_DIR.glob(f'*{keyword}*.json'))[:5]\n",
    "        for fp in files:\n",
    "            try:\n",
    "                with open(fp, 'r', encoding='utf-8') as f:\n",
    "                    record = json.load(f)\n",
    "                rid = record.get('id', fp.stem)\n",
    "                if rid not in loaded_ids:\n",
    "                    loaded_ids.add(rid)\n",
    "                    sample_records.append(record)\n",
    "                    sample_meta.append({\n",
    "                        'category': category,\n",
    "                        'note': note,\n",
    "                        'filename': fp.name,\n",
    "                    })\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(f\"Loaded {len(sample_records)} unique test samples across {len(VULN_TEST_SAMPLES)} categories.\")\n",
    "print(f\"\\nSamples per category:\")\n",
    "for cat in VULN_TEST_SAMPLES:\n",
    "    count = sum(1 for m in sample_meta if m['category'] == cat)\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74aead62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "VULNERABILITY EXTRACTION TEST RESULTS\n",
      "Testing 57 samples\n",
      "==========================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[vulnerability_function] Afghanistan Displacement Data - Climate Vulnerability Assessment [IOM DTM]\n",
      "  SOCIO-ECON: Displacement indicator (DISPLACEMENT) | scheme=Custom | year=2024 | conf=0.70\n",
      "  SOCIO-ECON: Vulnerability index (VULN_INDEX) | scheme=Custom | year=2024 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[vulnerability_function] Malawi National Vulnerability Index\n",
      "  SOCIO-ECON: Coping capacity index (COPING_CAPACITY) | scheme=Custom | year=2015 | conf=0.70\n",
      "  SOCIO-ECON: Vulnerability index (VULN_INDEX) | scheme=Custom | year=2015 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[vulnerability_function] Climate Conflict Vulnerability Index\n",
      "  SOCIO-ECON: Vulnerability index (VULN_INDEX) | scheme=Custom | year=2015 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[vulnerability_function] Vulnerability Assessment of Syrian Refugees in Lebanon\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2014 | conf=0.70\n",
      "  SOCIO-ECON: Displacement indicator (DISPLACEMENT) | scheme=Custom | year=2014 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_poverty] Ukraine - Poverty\n",
      "  SOCIO-ECON: Poverty headcount ratio (POV_HEADCOUNT) | scheme=Custom | year=1992 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_poverty] Israel - Poverty\n",
      "  SOCIO-ECON: Poverty headcount ratio (POV_HEADCOUNT) | scheme=Custom | year=1979 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_poverty] Algeria - Poverty\n",
      "  SOCIO-ECON: Poverty headcount ratio (POV_HEADCOUNT) | scheme=Custom | year=1988 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_health] Burundi: Treatment of moderate acute malnutrition\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=2021 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_health] Haiti - Acute Malnutrition\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=2021 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_health] Mali: Malnutrition Aiguë\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=2024 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_health] Ethiopia - Malnutrition Prevalence\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=2021 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_health] Somalia : Acute Malnutrition\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=2024 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_index] South Sudan: Resilience Capacity Index\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2021 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_index] INFORM subnational risk index 2021, South East Europe\n",
      "  SOCIO-ECON: INFORM Risk Index (INFORM_RISK) | scheme=INFORM | year=2021 | conf=0.70\n",
      "  SOCIO-ECON: Coping capacity index (COPING_CAPACITY) | scheme=Custom | year=2021 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Saint Lucia - Food Security and Nutrition Indicators\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2000 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] China - Food Security and Nutrition Indicators\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2000 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Azerbaijan - Food Security and Nutrition Indicators\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2000 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Barbados - Food Security and Nutrition Indicators\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2000 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Suriname - Food Security and Nutrition Indicators\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2000 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Kenya Medium Term Projection  FEWS NET Acute Food Insecurity Classification\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2016 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Yemen Medium Term Projection  FEWS NET Acute Food Insecurity Classification\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2014 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Madagascar Current Situation FEWS NET Acute Food Insecurity Classifications\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2018 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Somalia Near Term Projection  FEWS NET Acute Food Insecurity Classification\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2012 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[socioeconomic_food] Zimbabwe Current Situation FEWS NET Acute Food Insecurity Classifications G\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2019 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Afghanistan - Nexus Risks Index\n",
      "  SOCIO-ECON: Food security classification (FOOD_SECURITY) | scheme=Custom | year=2025 | conf=0.70\n",
      "  SOCIO-ECON: INFORM Risk Index (INFORM_RISK) | scheme=INFORM | year=2025 | conf=0.70\n",
      "  SOCIO-ECON: Vulnerability index (VULN_INDEX) | scheme=Custom | year=2025 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Kenya Children’s Climate Risk Index-Disaster Risk Model (CCRI-DRM) subnatio\n",
      "  SOCIO-ECON: Vulnerability index (VULN_INDEX) | scheme=Custom | year=2023 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Kiribati - Climate Change\n",
      "  SOCIO-ECON: Poverty headcount ratio (POV_HEADCOUNT) | scheme=Custom | year=1960 | conf=0.70\n",
      "  SOCIO-ECON: Educational attainment (EDU_ATTAINMENT) | scheme=Custom | year=1960 | conf=0.70\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=1960 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Myanmar - Climate Change\n",
      "  SOCIO-ECON: Poverty headcount ratio (POV_HEADCOUNT) | scheme=Custom | year=1960 | conf=0.70\n",
      "  SOCIO-ECON: Educational attainment (EDU_ATTAINMENT) | scheme=Custom | year=1960 | conf=0.70\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=1960 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Malawi - Climate Change\n",
      "  SOCIO-ECON: Poverty headcount ratio (POV_HEADCOUNT) | scheme=Custom | year=1960 | conf=0.70\n",
      "  SOCIO-ECON: Educational attainment (EDU_ATTAINMENT) | scheme=Custom | year=1960 | conf=0.70\n",
      "  SOCIO-ECON: Malnutrition prevalence (MALNUTRITION) | scheme=Custom | year=1960 | conf=0.70\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Macao SAR, China - Climate Change\n",
      "  SOCIO-ECON: Educational attainment (EDU_ATTAINMENT) | scheme=Custom | year=1960 | conf=0.70\n",
      "\n",
      "==========================================================================================\n",
      "TEST SUMMARY\n",
      "==========================================================================================\n",
      "  Total samples: 57\n",
      "  With vulnerability signal: 30 (52.6%)\n",
      "  Function extractions: 0\n",
      "  Socio-economic extractions: 42\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.2 Run Extraction on Test Samples\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"VULNERABILITY EXTRACTION TEST RESULTS\")\n",
    "print(f\"Testing {len(sample_records)} samples\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "test_results = []\n",
    "func_count = 0\n",
    "socio_count = 0\n",
    "\n",
    "for record, meta in zip(sample_records, sample_meta):\n",
    "    extraction = vuln_extractor.extract(record)\n",
    "\n",
    "    test_results.append({\n",
    "        'id': record.get('id'),\n",
    "        'title': record.get('title', '')[:70],\n",
    "        'category': meta['category'],\n",
    "        'extraction': extraction,\n",
    "    })\n",
    "\n",
    "    if extraction.has_any_signal():\n",
    "        func_count += len(extraction.functions)\n",
    "        socio_count += len(extraction.socio_economic)\n",
    "\n",
    "        print(f\"\\n{'â”€' * 90}\")\n",
    "        print(f\"[{meta['category']}] {record.get('title', '')[:75]}\")\n",
    "\n",
    "        if extraction.functions:\n",
    "            for func in extraction.functions:\n",
    "                print(f\"  FUNCTION: {func.function_type} | approach={func.approach} | \"\n",
    "                      f\"hazard={func.hazard_primary} | category={func.category} | \"\n",
    "                      f\"metric={func.impact_metric} | conf={func.confidence:.2f}\")\n",
    "\n",
    "        if extraction.socio_economic:\n",
    "            for se in extraction.socio_economic:\n",
    "                print(f\"  SOCIO-ECON: {se.indicator_name} ({se.indicator_code}) | \"\n",
    "                      f\"scheme={se.scheme} | year={se.reference_year} | conf={se.confidence:.2f}\")\n",
    "\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"TEST SUMMARY\")\n",
    "print(f\"{'=' * 90}\")\n",
    "total = len(test_results)\n",
    "with_signal = sum(1 for r in test_results if r['extraction'].has_any_signal())\n",
    "print(f\"  Total samples: {total}\")\n",
    "print(f\"  With vulnerability signal: {with_signal} ({with_signal/total*100:.1f}%)\")\n",
    "print(f\"  Function extractions: {func_count}\")\n",
    "print(f\"  Socio-economic extractions: {socio_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682acce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "STRUCTURAL COMPLIANCE VERIFICATION\n",
      "==========================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Block preview: Afghanistan Displacement Data - Climate Vulnerability Assessment [IOM \n",
      "{\n",
      "  \"socio_economic\": [\n",
      "    {\n",
      "      \"indicator_name\": \"Displacement indicator\",\n",
      "      \"indicator_code\": \"DISPLACEMENT\",\n",
      "      \"description\": \"Population displaced by conflict or disaster, indicating heightened vulnerability. Source: Afghanistan Displacement Data - Climate Vulnerability Assessment [IOM DTM]\",\n",
      "      \"id\": \"socio_1e6fc369_1\",\n",
      "      \"reference_year\": 2024,\n",
      "      \"scheme\": \"Custom\"\n",
      "    },\n",
      "    {\n",
      "      \"indicator_name\": \"Vulnerability index\",\n",
      "      \"indicator_code\": \"VULN_INDEX\",\n",
      "      \"description\": \"Composite vulnerability index combining multiple socio-economic and environmental factors. Source: Afghanistan Displacement Data - Climate Vulnerability Assessment [IOM DTM]\",\n",
      "      \"id\": \"socio_1e6fc369_2\",\n",
      "      \"reference_year\": 2024,\n",
      "      \"scheme\": \"Custom\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Block preview: Malawi National Vulnerability Index\n",
      "{\n",
      "  \"socio_economic\": [\n",
      "    {\n",
      "      \"indicator_name\": \"Coping capacity index\",\n",
      "      \"indicator_code\": \"COPING_CAPACITY\",\n",
      "      \"description\": \"Capacity of communities to cope with and recover from hazard impacts. Source: Malawi National Vulnerability Index\",\n",
      "      \"id\": \"socio_4b01d1b9_1\",\n",
      "      \"reference_year\": 2015,\n",
      "      \"scheme\": \"Custom\"\n",
      "    },\n",
      "    {\n",
      "      \"indicator_name\": \"Vulnerability index\",\n",
      "      \"indicator_code\": \"VULN_INDEX\",\n",
      "      \"description\": \"Composite vulnerability index combining multiple socio-economic and environmental factors. Source: Malawi National Vulnerability Index\",\n",
      "      \"id\": \"socio_4b01d1b9_2\",\n",
      "      \"reference_year\": 2015,\n",
      "      \"scheme\": \"Custom\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Block preview: Climate Conflict Vulnerability Index\n",
      "{\n",
      "  \"socio_economic\": [\n",
      "    {\n",
      "      \"indicator_name\": \"Vulnerability index\",\n",
      "      \"indicator_code\": \"VULN_INDEX\",\n",
      "      \"description\": \"Composite vulnerability index combining multiple socio-economic and environmental factors. Source: Climate Conflict Vulnerability Index\",\n",
      "      \"id\": \"socio_664098ac_1\",\n",
      "      \"reference_year\": 2015,\n",
      "      \"scheme\": \"Custom\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "==========================================================================================\n",
      "COMPLIANCE REPORT\n",
      "==========================================================================================\n",
      "  Total blocks built:          30\n",
      "  Total function entries:       0\n",
      "  Total socio-economic entries: 42\n",
      "\n",
      "  Function mandatory fields:    PASS\n",
      "  Socio-economic mandatory:     PASS\n",
      "  Codelist compliance:          PASS\n",
      "  ID uniqueness:                PASS\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.3 Build Blocks and Verify Structural Compliance\n",
    "\n",
    "Verify all mandatory fields, codelist values, and structural requirements.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STRUCTURAL COMPLIANCE VERIFICATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "total_blocks = 0\n",
    "total_functions = 0\n",
    "total_socio = 0\n",
    "\n",
    "# Compliance trackers\n",
    "func_field_violations = []\n",
    "socio_field_violations = []\n",
    "codelist_violations = []\n",
    "\n",
    "FUNCTION_MANDATORY_FIELDS = [\n",
    "    'approach', 'relationship', 'hazard_primary', 'hazard_analysis_type',\n",
    "    'intensity_measure', 'category', 'impact_type', 'impact_modelling',\n",
    "    'impact_metric', 'quantity_kind', 'id'\n",
    "]\n",
    "SOCIO_MANDATORY_FIELDS = ['indicator_name', 'indicator_code', 'description', 'reference_year', 'id']\n",
    "\n",
    "for result in test_results:\n",
    "    extraction = result['extraction']\n",
    "    if not extraction.has_any_signal():\n",
    "        continue\n",
    "\n",
    "    block = build_vulnerability_block(extraction, result['id'])\n",
    "    if not block:\n",
    "        continue\n",
    "\n",
    "    total_blocks += 1\n",
    "\n",
    "    # Check functions\n",
    "    for ftype_key in ['vulnerability', 'fragility', 'damage_to_loss', 'engineering_demand']:\n",
    "        for entry in block.get('functions', {}).get(ftype_key, []):\n",
    "            total_functions += 1\n",
    "\n",
    "            # Check mandatory fields\n",
    "            for field in FUNCTION_MANDATORY_FIELDS:\n",
    "                if field not in entry or not entry[field]:\n",
    "                    func_field_violations.append(f\"{result['id'][:8]}/{ftype_key}: missing {field}\")\n",
    "\n",
    "            # Check codelist values\n",
    "            if entry.get('approach') and entry['approach'] not in VALID_FUNCTION_APPROACHES:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: approach='{entry['approach']}'\")\n",
    "            if entry.get('relationship') and entry['relationship'] not in VALID_RELATIONSHIP_TYPES:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: relationship='{entry['relationship']}'\")\n",
    "            if entry.get('hazard_primary') and entry['hazard_primary'] not in VALID_HAZARD_TYPES:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: hazard_primary='{entry['hazard_primary']}'\")\n",
    "            if entry.get('hazard_analysis_type') and entry['hazard_analysis_type'] not in VALID_ANALYSIS_TYPES:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: hazard_analysis_type='{entry['hazard_analysis_type']}'\")\n",
    "            if entry.get('category') and entry['category'] not in VALID_EXPOSURE_CATEGORIES:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: category='{entry['category']}'\")\n",
    "            if entry.get('impact_type') and entry['impact_type'] not in VALID_IMPACT_TYPES:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: impact_type='{entry['impact_type']}'\")\n",
    "            if entry.get('impact_modelling') and entry['impact_modelling'] not in VALID_CALCULATION_TYPES:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: impact_modelling='{entry['impact_modelling']}'\")\n",
    "            if entry.get('impact_metric') and entry['impact_metric'] not in VALID_IMPACT_METRICS:\n",
    "                codelist_violations.append(f\"{result['id'][:8]}: impact_metric='{entry['impact_metric']}'\")\n",
    "\n",
    "    # Check socio-economic\n",
    "    for entry in block.get('socio_economic', []):\n",
    "        total_socio += 1\n",
    "\n",
    "        for field in SOCIO_MANDATORY_FIELDS:\n",
    "            if field not in entry or (field != 'reference_year' and not entry[field]):\n",
    "                socio_field_violations.append(f\"{result['id'][:8]}: missing {field}\")\n",
    "            elif field == 'reference_year':\n",
    "                yr = entry.get('reference_year')\n",
    "                if not isinstance(yr, int) or yr < 1900 or yr > 2100:\n",
    "                    socio_field_violations.append(f\"{result['id'][:8]}: invalid reference_year={yr}\")\n",
    "\n",
    "        if entry.get('scheme') and entry['scheme'] not in VALID_TAXONOMIES:\n",
    "            codelist_violations.append(f\"{result['id'][:8]}: scheme='{entry['scheme']}'\")\n",
    "\n",
    "    # Show first 3 block previews\n",
    "    if total_blocks <= 3:\n",
    "        print(f\"\\n{'â”€' * 90}\")\n",
    "        print(f\"Block preview: {result['title']}\")\n",
    "        print(json.dumps(block, indent=2)[:2000])\n",
    "\n",
    "# --- Compliance Report ---\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"COMPLIANCE REPORT\")\n",
    "print(f\"{'=' * 90}\")\n",
    "print(f\"  Total blocks built:          {total_blocks}\")\n",
    "print(f\"  Total function entries:       {total_functions}\")\n",
    "print(f\"  Total socio-economic entries: {total_socio}\")\n",
    "print()\n",
    "print(f\"  Function mandatory fields:    {'PASS' if not func_field_violations else f'FAIL ({len(func_field_violations)} violations)'}\")\n",
    "for v in func_field_violations[:5]:\n",
    "    print(f\"    - {v}\")\n",
    "print(f\"  Socio-economic mandatory:     {'PASS' if not socio_field_violations else f'FAIL ({len(socio_field_violations)} violations)'}\")\n",
    "for v in socio_field_violations[:5]:\n",
    "    print(f\"    - {v}\")\n",
    "print(f\"  Codelist compliance:          {'PASS' if not codelist_violations else f'FAIL ({len(codelist_violations)} violations)'}\")\n",
    "for v in codelist_violations[:5]:\n",
    "    print(f\"    - {v}\")\n",
    "\n",
    "# ID uniqueness check\n",
    "all_ids = []\n",
    "for result in test_results:\n",
    "    extraction = result['extraction']\n",
    "    if not extraction.has_any_signal():\n",
    "        continue\n",
    "    block = build_vulnerability_block(extraction, result['id'])\n",
    "    if not block:\n",
    "        continue\n",
    "    for ftype_key in ['vulnerability', 'fragility', 'damage_to_loss', 'engineering_demand']:\n",
    "        for entry in block.get('functions', {}).get(ftype_key, []):\n",
    "            all_ids.append(entry['id'])\n",
    "    for entry in block.get('socio_economic', []):\n",
    "        all_ids.append(entry['id'])\n",
    "\n",
    "dup_ids = [id for id in all_ids if all_ids.count(id) > 1]\n",
    "print(f\"  ID uniqueness:                {'PASS' if not dup_ids else f'FAIL ({len(set(dup_ids))} duplicate IDs)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff4293",
   "metadata": {},
   "source": [
    "## 5. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "691998a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all records...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6ab5960edb47b386a0aed3a348c949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting vulnerability:   0%|          | 0/26246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.1 Process Full Corpus\n",
    "\"\"\"\n",
    "\n",
    "def process_vulnerability_extraction(\n",
    "    metadata_dir: Path,\n",
    "    extractor: VulnerabilityExtractor,\n",
    "    limit: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process all records for vulnerability extraction.\"\"\"\n",
    "    json_files = sorted(metadata_dir.glob('*.json'))\n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "\n",
    "    results = []\n",
    "    iterator = tqdm(json_files, desc=\"Extracting vulnerability\") if HAS_TQDM else json_files\n",
    "\n",
    "    for filepath in iterator:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                record = json.load(f)\n",
    "\n",
    "            extraction = extractor.extract(record)\n",
    "\n",
    "            results.append({\n",
    "                'id': record.get('id'),\n",
    "                'title': record.get('title'),\n",
    "                'organization': record.get('organization'),\n",
    "                'has_functions': len(extraction.functions) > 0,\n",
    "                'function_types': [f.function_type for f in extraction.functions],\n",
    "                'has_socio_economic': len(extraction.socio_economic) > 0,\n",
    "                'socio_indicators': [s.indicator_code for s in extraction.socio_economic],\n",
    "                'overall_confidence': extraction.overall_confidence,\n",
    "                'has_vulnerability': extraction.has_any_signal(),\n",
    "                'extraction': extraction,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({'id': filepath.stem, 'error': str(e)})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "PROCESS_LIMIT = None  # Set to e.g. 2000 for testing, None for full corpus\n",
    "\n",
    "print(f\"Processing {'all' if PROCESS_LIMIT is None else PROCESS_LIMIT} records...\")\n",
    "df_vuln = process_vulnerability_extraction(DATASET_METADATA_DIR, vuln_extractor, limit=PROCESS_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e56ed6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VULNERABILITY EXTRACTION STATISTICS\n",
      "============================================================\n",
      "\n",
      "Total records processed: 26,246\n",
      "  With any vulnerability signal: 5,327 (20.3%)\n",
      "  With function detection:       393 (1.5%)\n",
      "  With socio-economic detection: 5,327 (20.3%)\n",
      "\n",
      "Function Type Distribution:\n",
      "  vulnerability: 393\n",
      "\n",
      "Socio-Economic Indicator Distribution:\n",
      "  FOOD_SECURITY: 2448\n",
      "  POV_HEADCOUNT: 1346\n",
      "  EDU_ATTAINMENT: 1342\n",
      "  MALNUTRITION: 1273\n",
      "  HEALTH_ACCESS: 599\n",
      "  DEPRIVATION: 401\n",
      "  DISPLACEMENT: 291\n",
      "  AGE_65_PLUS: 247\n",
      "  HDI: 218\n",
      "  COPING_CAPACITY: 68\n",
      "  DISABILITY: 64\n",
      "  POP_DENSITY: 28\n",
      "  INFORM_RISK: 15\n",
      "  VULN_INDEX: 9\n",
      "  LIVELIHOOD: 9\n",
      "\n",
      "Confidence Distribution:\n",
      "  Mean: 0.70\n",
      "  Median: 0.70\n",
      "  High (>=0.7): 4099\n",
      "  Medium (0.5-0.7): 1228\n",
      "  Low (<0.5): 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.2 Extraction Statistics\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VULNERABILITY EXTRACTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_vuln)\n",
    "with_vuln = df_vuln['has_vulnerability'].sum()\n",
    "with_func = df_vuln['has_functions'].sum()\n",
    "with_socio = df_vuln['has_socio_economic'].sum()\n",
    "\n",
    "print(f\"\\nTotal records processed: {total:,}\")\n",
    "print(f\"  With any vulnerability signal: {with_vuln:,} ({with_vuln/total*100:.1f}%)\")\n",
    "print(f\"  With function detection:       {with_func:,} ({with_func/total*100:.1f}%)\")\n",
    "print(f\"  With socio-economic detection: {with_socio:,} ({with_socio/total*100:.1f}%)\")\n",
    "\n",
    "# Function type distribution\n",
    "func_type_counts = Counter()\n",
    "for ftypes in df_vuln['function_types'].dropna():\n",
    "    if isinstance(ftypes, list):\n",
    "        func_type_counts.update(ftypes)\n",
    "\n",
    "if func_type_counts:\n",
    "    print(f\"\\nFunction Type Distribution:\")\n",
    "    for ft, count in func_type_counts.most_common():\n",
    "        print(f\"  {ft}: {count}\")\n",
    "\n",
    "# Socio-economic indicator distribution\n",
    "indicator_counts = Counter()\n",
    "for indicators in df_vuln['socio_indicators'].dropna():\n",
    "    if isinstance(indicators, list):\n",
    "        indicator_counts.update(indicators)\n",
    "\n",
    "if indicator_counts:\n",
    "    print(f\"\\nSocio-Economic Indicator Distribution:\")\n",
    "    for ind, count in indicator_counts.most_common(15):\n",
    "        print(f\"  {ind}: {count}\")\n",
    "\n",
    "# Confidence distribution\n",
    "conf = df_vuln[df_vuln['has_vulnerability']]['overall_confidence']\n",
    "if len(conf) > 0:\n",
    "    print(f\"\\nConfidence Distribution:\")\n",
    "    print(f\"  Mean: {conf.mean():.2f}\")\n",
    "    print(f\"  Median: {conf.median():.2f}\")\n",
    "    print(f\"  High (>=0.7): {(conf >= 0.7).sum()}\")\n",
    "    print(f\"  Medium (0.5-0.7): {((conf >= 0.5) & (conf < 0.7)).sum()}\")\n",
    "    print(f\"  Low (<0.5): {(conf < 0.5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a028176",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cleanup_11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 11 Vulnerability + Loss Extraction]:\n",
      "  rdls_vln-hdx_*.json                     : 5,327 files\n",
      "  vulnerability_extraction_results.csv    : 1 files\n",
      "  vulnerability_detected_records.csv      : 1 files\n",
      "  rdls_lss-hdx_*.json                     : 821 files\n",
      "  loss_extraction_results.csv             : 1 files\n",
      "  loss_detected_records.csv               : 1 files\n",
      "  Cleaned 6,152 files. Ready for fresh output.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deleted': 6152, 'skipped': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.0 Clean Previous Outputs (Vulnerability + Loss)\n",
    "\n",
    "Removes stale output files before writing new ones.\n",
    "Controlled by CLEANUP_MODE in cell 1.2 above.\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice \"{choice}\", defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "\n",
    "# -- Run cleanup for NB 11 Vulnerability + Loss Extraction outputs --\n",
    "clean_previous_outputs(\n",
    "    OUTPUT_DIR,\n",
    "    patterns=[\n",
    "        \"rdls_vln-hdx_*.json\",\n",
    "        \"vulnerability_extraction_results.csv\",\n",
    "        \"vulnerability_detected_records.csv\",\n",
    "        \"rdls_lss-hdx_*.json\",\n",
    "        \"loss_extraction_results.csv\",\n",
    "        \"loss_detected_records.csv\",\n",
    "    ],\n",
    "    label=\"NB 11 Vulnerability + Loss Extraction\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a66ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted/vulnerability_extraction_results.csv\n",
      "Saved: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted/vulnerability_detected_records.csv (5327 records)\n",
      "\n",
      "Generating RDLS vulnerability block JSONs for 5,327 datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798ade71f37542719a897ccb692ada80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vuln JSONs:   0%|          | 0/5327 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "  Generated: 5,327 vulnerability block JSONs\n",
      "  Skipped (no valid block): 0\n",
      "  Output: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.1 Export Results and Generate RDLS Vulnerability Block JSONs\n",
    "\"\"\"\n",
    "\n",
    "# Prepare export DataFrame\n",
    "export_df = df_vuln[[\n",
    "    'id', 'title', 'organization', 'has_functions', 'function_types',\n",
    "    'has_socio_economic', 'socio_indicators', 'overall_confidence', 'has_vulnerability'\n",
    "]].copy()\n",
    "\n",
    "# Convert lists to pipe-separated for CSV\n",
    "for col in ['function_types', 'socio_indicators']:\n",
    "    export_df[col] = export_df[col].apply(\n",
    "        lambda x: '|'.join(x) if isinstance(x, list) else ''\n",
    "    )\n",
    "\n",
    "# Save full results\n",
    "output_file = OUTPUT_DIR / 'vulnerability_extraction_results.csv'\n",
    "export_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Save records with vulnerability signals\n",
    "vuln_records = export_df[export_df['has_vulnerability']]\n",
    "vuln_file = OUTPUT_DIR / 'vulnerability_detected_records.csv'\n",
    "vuln_records.to_csv(vuln_file, index=False)\n",
    "print(f\"Saved: {vuln_file} ({len(vuln_records)} records)\")\n",
    "\n",
    "# --- Generate RDLS vulnerability block JSONs for ALL flagged datasets ---\n",
    "all_vuln = df_vuln[\n",
    "    df_vuln['has_vulnerability'] &\n",
    "    (df_vuln['overall_confidence'] >= 0.5)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nGenerating RDLS vulnerability block JSONs for {len(all_vuln):,} datasets...\")\n",
    "\n",
    "generated = 0\n",
    "skipped = 0\n",
    "\n",
    "iterator = tqdm(all_vuln.iterrows(), total=len(all_vuln), desc=\"Building vuln JSONs\") if HAS_TQDM else all_vuln.iterrows()\n",
    "\n",
    "for idx, row in iterator:\n",
    "    extraction = row['extraction']\n",
    "    vuln_block = build_vulnerability_block(extraction, row['id'])\n",
    "\n",
    "    if vuln_block:\n",
    "        rdls_record = {\n",
    "            'datasets': [{\n",
    "                'id': f\"rdls_vln-hdx_{row['id'][:8]}\",\n",
    "                'title': row['title'],\n",
    "                'risk_data_type': ['vulnerability'],\n",
    "                'vulnerability': vuln_block,\n",
    "                'links': [{\n",
    "                    'href': 'https://docs.riskdatalibrary.org/en/0__3__0/rdls_schema.json',\n",
    "                    'rel': 'describedby'\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        output_path = OUTPUT_DIR / f\"rdls_vln-hdx_{row['id'][:8]}.json\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rdls_record, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        generated += 1\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"\\nDone.\")\n",
    "print(f\"  Generated: {generated:,} vulnerability block JSONs\")\n",
    "print(f\"  Skipped (no valid block): {skipped:,}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_header_01",
   "metadata": {},
   "source": [
    "## 8. RDLS Loss Block Extractor\n",
    "\n",
    "**Purpose**: Extract and populate RDLS v0.3 Loss component blocks from HDX metadata.\n",
    "\n",
    "**RDLS Loss Block Structure (v0.3)**:\n",
    "```\n",
    "loss:\n",
    "  losses:\n",
    "    - id: string (required)\n",
    "      hazard_type: hazard_type codelist (required)\n",
    "      hazard_process: process_type codelist (optional)\n",
    "      asset_category: exposure_category codelist (required)\n",
    "      asset_dimension: metric_dimension codelist (required)\n",
    "      impact_and_losses:  (required, 7 required sub-fields)\n",
    "        impact_type: direct | indirect | total\n",
    "        impact_modelling: inferred | observed | simulated\n",
    "        impact_metric: impact_metric codelist\n",
    "        quantity_kind: open codelist\n",
    "        loss_type: ground_up | insured | gross | count | net_precat | net_postcat\n",
    "        loss_approach: analytical | empirical | hybrid | judgement\n",
    "        loss_frequency_type: probabilistic | deterministic | empirical\n",
    "        currency: ISO 4217 (optional, when monetary)\n",
    "      lineage: (optional)\n",
    "        hazard_dataset: string\n",
    "        exposure_dataset: string\n",
    "        vulnerability_dataset: string\n",
    "      description: string (optional)\n",
    "```\n",
    "\n",
    "**Extraction Strategy**:\n",
    "- Detect loss signals from HDX metadata (damage, fatality, casualty, economic loss, AAL)\n",
    "- Cross-reference hazard extraction from NB 09 for hazard_type and hazard_process\n",
    "- Infer asset_category from exposure patterns, asset_dimension from context\n",
    "- Populate all 7 required impact_and_losses sub-fields with codelist-valid values\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_setup_02",
   "metadata": {},
   "source": [
    "### 8.1 Loss Detection Patterns and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "loss_patterns_03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss schema constants loaded:\n",
      "  Loss types (6): ['count', 'gross', 'ground_up', 'insured', 'net_postcat', 'net_precat']\n",
      "  Metric dimensions (6): ['content', 'disruption', 'index', 'population', 'product', 'structure']\n",
      "  Currencies: 302 ISO 4217 codes\n",
      "  (Reusing from vulnerability: hazard_types, process_types, analysis_types, etc.)\n",
      "Loss detection patterns defined.\n",
      "  Loss signal categories: ['human_loss', 'displacement', 'affected_population', 'economic_loss', 'structural_damage', 'agricultural_loss', 'catastrophe_model', 'general_loss']\n",
      "  Exclusion patterns: 4\n",
      "  Currency detection: 25 currencies\n",
      "\n",
      "Loss constraint tables defined.\n",
      "  Group 1 - Asset triplets: 7 categories\n",
      "  Group 2 - Impact metric constraints: 20 metrics\n",
      "  Group 3 - Signal defaults: 8 signal types\n",
      "  Loss-type approach rules: 6 loss types\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.1 Loss Detection Patterns and Data Classes\n",
    "\n",
    "Patterns for detecting loss types, asset dimensions, and inferring\n",
    "mandatory field values from HDX metadata text.\n",
    "\"\"\"\n",
    "\n",
    "# --- Load additional schema constants for Loss ---\n",
    "VALID_LOSS_TYPES: Set[str] = set(RDLS_SCHEMA['$defs']['Losses']['properties']['impact_and_losses']['properties']['loss_type']['enum'])\n",
    "VALID_METRIC_DIMENSIONS: Set[str] = set(RDLS_SCHEMA['$defs']['metric_dimension']['enum'])\n",
    "# Currency: large ISO 4217 codelist â€” load from schema\n",
    "VALID_CURRENCIES: Set[str] = set(RDLS_SCHEMA['$defs']['Losses']['properties']['impact_and_losses']['properties']['currency']['enum'])\n",
    "\n",
    "print(f\"Loss schema constants loaded:\")\n",
    "print(f\"  Loss types ({len(VALID_LOSS_TYPES)}): {sorted(VALID_LOSS_TYPES)}\")\n",
    "print(f\"  Metric dimensions ({len(VALID_METRIC_DIMENSIONS)}): {sorted(VALID_METRIC_DIMENSIONS)}\")\n",
    "print(f\"  Currencies: {len(VALID_CURRENCIES)} ISO 4217 codes\")\n",
    "print(f\"  (Reusing from vulnerability: hazard_types, process_types, analysis_types, etc.)\")\n",
    "\n",
    "# --- Loss signal detection patterns ---\n",
    "LOSS_SIGNAL_PATTERNS = {\n",
    "    'human_loss': [\n",
    "        re.compile(r'\\b(casualt(?:y|ies)|fatalit(?:y|ies)|mortalit(?:y|ies)|death)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(killed|dead|perished|deceased)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(injur(?:y|ies|ed)|wounded|hospitalized)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(missing[\\s._-]?persons?|unaccounted)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'displacement': [\n",
    "        re.compile(r'\\b(displaced|displacement|evacuated|evacuation)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(homeless|shelter[\\s._-]?(?:less|need))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(internally[\\s._-]?displaced|idp)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(refugee[\\s._-]?(?:flow|movement|crisis))\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'affected_population': [\n",
    "        re.compile(r'\\b(affected[\\s._-]?(?:population|people|person|household|communit))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(people[\\s._-]?(?:affected|impacted|in[\\s._-]?need))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(population[\\s._-]?(?:affected|exposed|at[\\s._-]?risk))\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'economic_loss': [\n",
    "        re.compile(r'\\b(economic[\\s._-]?loss|financial[\\s._-]?loss|monetary[\\s._-]?loss)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(damage[\\s._-]?cost|repair[\\s._-]?cost|replacement[\\s._-]?cost)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(insured[\\s._-]?loss|insurance[\\s._-]?claim)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(aal|average[\\s._-]?annual[\\s._-]?loss)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(expected[\\s._-]?loss|probable[\\s._-]?maximum[\\s._-]?loss|pml)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'structural_damage': [\n",
    "        re.compile(r'\\b(building[\\s._-]?(?:damage|destroyed|collapsed|affected))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(structural[\\s._-]?damage|house[\\s._-]?(?:damage|destroyed))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(infrastructure[\\s._-]?(?:damage|destroyed|loss))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(damage[\\s._-]?(?:state|ratio|assessment|survey))\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'agricultural_loss': [\n",
    "        re.compile(r'\\b(crop[\\s._-]?(?:loss|damage|failure|destroyed))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(agricultural[\\s._-]?(?:loss|damage|impact))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(livestock[\\s._-]?(?:loss|death|mortality))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(harvest[\\s._-]?(?:loss|failure|damage))\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'catastrophe_model': [\n",
    "        re.compile(r'\\b(cat[\\s._-]?model|catastrophe[\\s._-]?model)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(risk[\\s._-]?model|loss[\\s._-]?model)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(loss[\\s._-]?exceedance|ep[\\s._-]?curve)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'general_loss': [\n",
    "        re.compile(r'\\b(disaster[\\s._-]?(?:loss|damage|impact|incident))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(natural[\\s._-]?disaster[\\s._-]?(?:loss|damage|impact|incident))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(damage[\\s._-]?and[\\s._-]?loss(?:es)?)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(post[\\s._-]?disaster[\\s._-]?(?:need|assessment|damage))\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(pdna|dala|rapid[\\s._-]?damage[\\s._-]?assessment)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Exclusion patterns: things that look like loss but aren't ---\n",
    "LOSS_EXCLUSION_PATTERNS = [\n",
    "    re.compile(r'\\b(data[\\s._-]?loss|packet[\\s._-]?loss|signal[\\s._-]?loss)\\b', re.IGNORECASE),\n",
    "    re.compile(r'\\b(weight[\\s._-]?loss|hair[\\s._-]?loss|blood[\\s._-]?loss)\\b', re.IGNORECASE),\n",
    "    re.compile(r'\\b(loss[\\s._-]?of[\\s._-]?(?:data|signal|connectivity|precision))\\b', re.IGNORECASE),\n",
    "    re.compile(r'\\b(profit[\\s._-]?and[\\s._-]?loss|p&l)\\b', re.IGNORECASE),\n",
    "]\n",
    "\n",
    "\n",
    "# --- Currency detection patterns ---\n",
    "CURRENCY_PATTERNS = [\n",
    "    (re.compile(r'\\b(usd|us[\\s._-]?dollar|united[\\s._-]?states[\\s._-]?dollar)\\b', re.IGNORECASE), 'USD'),\n",
    "    (re.compile(r'\\b(eur|euro)\\b', re.IGNORECASE), 'EUR'),\n",
    "    (re.compile(r'\\b(gbp|british[\\s._-]?pound|pound[\\s._-]?sterling)\\b', re.IGNORECASE), 'GBP'),\n",
    "    (re.compile(r'\\b(jpy|japanese[\\s._-]?yen)\\b', re.IGNORECASE), 'JPY'),\n",
    "    (re.compile(r'\\b(cny|chinese[\\s._-]?yuan|rmb|renminbi)\\b', re.IGNORECASE), 'CNY'),\n",
    "    (re.compile(r'\\b(inr|indian[\\s._-]?rupee)\\b', re.IGNORECASE), 'INR'),\n",
    "    (re.compile(r'\\b(aud|australian[\\s._-]?dollar)\\b', re.IGNORECASE), 'AUD'),\n",
    "    (re.compile(r'\\b(cad|canadian[\\s._-]?dollar)\\b', re.IGNORECASE), 'CAD'),\n",
    "    (re.compile(r'\\b(chf|swiss[\\s._-]?franc)\\b', re.IGNORECASE), 'CHF'),\n",
    "    (re.compile(r'\\b(bdt|bangladeshi[\\s._-]?taka|taka)\\b', re.IGNORECASE), 'BDT'),\n",
    "    (re.compile(r'\\b(pkr|pakistani[\\s._-]?rupee)\\b', re.IGNORECASE), 'PKR'),\n",
    "    (re.compile(r'\\b(php|philippine[\\s._-]?peso)\\b', re.IGNORECASE), 'PHP'),\n",
    "    (re.compile(r'\\b(idr|indonesian[\\s._-]?rupiah|rupiah)\\b', re.IGNORECASE), 'IDR'),\n",
    "    (re.compile(r'\\b(kes|kenyan[\\s._-]?shilling)\\b', re.IGNORECASE), 'KES'),\n",
    "    (re.compile(r'\\b(ngn|nigerian[\\s._-]?naira|naira)\\b', re.IGNORECASE), 'NGN'),\n",
    "    (re.compile(r'\\b(etb|ethiopian[\\s._-]?birr|birr)\\b', re.IGNORECASE), 'ETB'),\n",
    "    (re.compile(r'\\b(mmk|myanmar[\\s._-]?kyat|kyat)\\b', re.IGNORECASE), 'MMK'),\n",
    "    (re.compile(r'\\b(afn|afghani)\\b', re.IGNORECASE), 'AFN'),\n",
    "    (re.compile(r'\\b(htg|haitian[\\s._-]?gourde|gourde)\\b', re.IGNORECASE), 'HTG'),\n",
    "    (re.compile(r'\\b(ssp|south[\\s._-]?sudanese[\\s._-]?pound)\\b', re.IGNORECASE), 'SSP'),\n",
    "    (re.compile(r'\\b(yer|yemeni[\\s._-]?rial)\\b', re.IGNORECASE), 'YER'),\n",
    "    (re.compile(r'\\b(sdg|sudanese[\\s._-]?pound)\\b', re.IGNORECASE), 'SDG'),\n",
    "    (re.compile(r'\\b(syp|syrian[\\s._-]?pound)\\b', re.IGNORECASE), 'SYP'),\n",
    "    (re.compile(r'\\b(cdf|congolese[\\s._-]?franc)\\b', re.IGNORECASE), 'CDF'),\n",
    "    (re.compile(r'\\b(mzn|mozambican[\\s._-]?metical|metical)\\b', re.IGNORECASE), 'MZN'),\n",
    "]\n",
    "\n",
    "# --- Insured loss detection ---\n",
    "INSURED_LOSS_PATTERNS = [\n",
    "    re.compile(r'\\b(insured[\\s._-]?loss|insurance[\\s._-]?claim|insured[\\s._-]?damage)\\b', re.IGNORECASE),\n",
    "    re.compile(r'\\b(insurance[\\s._-]?payout|claim[\\s._-]?amount)\\b', re.IGNORECASE),\n",
    "]\n",
    "\n",
    "# --- Loss approach inference (parallels function_approach) ---\n",
    "LOSS_APPROACH_PATTERNS = {\n",
    "    'analytical': [\n",
    "        re.compile(r'\\b(analytical|simulation[\\s._-]?based|modelled|modeled|cat[\\s._-]?model)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(catastrophe[\\s._-]?model|risk[\\s._-]?model)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'empirical': [\n",
    "        re.compile(r'\\b(empirical|observed|survey|historical|field[\\s._-]?data)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(post[\\s._-]?disaster|post[\\s._-]?event|damage[\\s._-]?survey)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(actual|recorded|reported|pdna|dala)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'hybrid': [\n",
    "        re.compile(r'\\b(hybrid|combined|mixed[\\s._-]?method)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'judgement': [\n",
    "        re.compile(r'\\b(expert[\\s._-]?judg[e]?ment|expert[\\s._-]?opinion|estimated)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(rapid[\\s._-]?assessment|preliminary[\\s._-]?estimate)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# --- Loss frequency type inference (parallels analysis_type) ---\n",
    "LOSS_FREQUENCY_PATTERNS = {\n",
    "    'probabilistic': [\n",
    "        re.compile(r'\\b(probabilistic|stochastic|return[\\s._-]?period|aal)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(average[\\s._-]?annual[\\s._-]?loss|expected[\\s._-]?loss)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(ep[\\s._-]?curve|loss[\\s._-]?exceedance|exceedance[\\s._-]?probability)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(probable[\\s._-]?maximum[\\s._-]?loss|pml|annual[\\s._-]?exceedance)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'deterministic': [\n",
    "        re.compile(r'\\b(deterministic|scenario[\\s._-]?based|single[\\s._-]?event)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(worst[\\s._-]?case|maximum[\\s._-]?credible)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "    'empirical': [\n",
    "        re.compile(r'\\b(empirical|historical|observed|actual[\\s._-]?event)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(recorded|reported|real[\\s._-]?event|past[\\s._-]?event)\\b', re.IGNORECASE),\n",
    "        re.compile(r'\\b(disaster[\\s._-]?incident|event[\\s._-]?based)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"Loss detection patterns defined.\")\n",
    "print(f\"  Loss signal categories: {list(LOSS_SIGNAL_PATTERNS.keys())}\")\n",
    "print(f\"  Exclusion patterns: {len(LOSS_EXCLUSION_PATTERNS)}\")\n",
    "print(f\"  Currency detection: {len(CURRENCY_PATTERNS)} currencies\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOSS CONSTRAINT TABLES\n",
    "# =============================================================================\n",
    "#\n",
    "# Three constraint groups derived from RDLS v0.3 schema + Chattogram example.\n",
    "# Used by LossExtractor to validate field combinations.\n",
    "#\n",
    "# Group 1: VALID_ASSET_TRIPLETS â€” asset_category -> allowed asset_dimensions\n",
    "#           (mirrors exposure VALID_TRIPLETS dimension column)\n",
    "#\n",
    "# Group 2: IMPACT_METRIC_CONSTRAINTS â€” impact_metric -> (quantity_kind, impact_types)\n",
    "#           Maps each of the 19 impact_metric values to its expected quantity_kind\n",
    "#           and which impact_types it logically applies to.\n",
    "#\n",
    "# Group 3: LOSS_SIGNAL_DEFAULTS â€” loss_signal_type -> full field defaults\n",
    "#           Single lookup replacing the 4 separate LOSS_SIGNAL_TO_* dicts.\n",
    "#           Each signal type maps to a complete, validated set of defaults.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Group 1: asset_category -> allowed asset_dimensions ---\n",
    "# First entry is the default for each category.\n",
    "VALID_ASSET_TRIPLETS = {\n",
    "    'agriculture':          ['product', 'structure', 'content'],\n",
    "    'buildings':            ['structure', 'content'],\n",
    "    'infrastructure':       ['structure', 'disruption'],\n",
    "    'population':           ['population'],\n",
    "    'natural_environment':  ['structure', 'index'],\n",
    "    'economic_indicator':   ['product', 'index'],\n",
    "    'development_index':    ['index'],\n",
    "}\n",
    "\n",
    "# --- Group 2: impact_metric -> (quantity_kind, impact_types) ---\n",
    "# IMPACT_METRIC_CONSTRAINTS is defined in cell 6 (shared with vulnerability).\n",
    "# It is available here as a global variable.\n",
    "\n",
    "\n",
    "# --- Group 3: Unified signal defaults ---\n",
    "# Replaces LOSS_SIGNAL_TO_LOSS_TYPE, LOSS_SIGNAL_TO_IMPACT_METRIC,\n",
    "# LOSS_SIGNAL_TO_QUANTITY_KIND, and LOSS_SIGNAL_TO_ASSET with one table.\n",
    "# Each signal type provides a complete, internally-consistent set of defaults\n",
    "# that are guaranteed valid against Groups 1 and 2.\n",
    "LOSS_SIGNAL_DEFAULTS = {\n",
    "    'human_loss': {\n",
    "        'asset_category':    'population',\n",
    "        'asset_dimension':   'population',\n",
    "        'impact_type':       'direct',\n",
    "        'impact_metric':     'casualty_count',\n",
    "        'quantity_kind':     'count',\n",
    "        'loss_type':         'count',\n",
    "    },\n",
    "    'displacement': {\n",
    "        'asset_category':    'population',\n",
    "        'asset_dimension':   'population',\n",
    "        'impact_type':       'direct',\n",
    "        'impact_metric':     'displaced_count',\n",
    "        'quantity_kind':     'count',\n",
    "        'loss_type':         'count',\n",
    "    },\n",
    "    'affected_population': {\n",
    "        'asset_category':    'population',\n",
    "        'asset_dimension':   'population',\n",
    "        'impact_type':       'direct',\n",
    "        'impact_metric':     'exposure_to_hazard',\n",
    "        'quantity_kind':     'count',\n",
    "        'loss_type':         'count',\n",
    "    },\n",
    "    'economic_loss': {\n",
    "        'asset_category':    'buildings',\n",
    "        'asset_dimension':   'structure',\n",
    "        'impact_type':       'direct',\n",
    "        'impact_metric':     'economic_loss_value',\n",
    "        'quantity_kind':     'monetary',\n",
    "        'loss_type':         'ground_up',\n",
    "    },\n",
    "    'structural_damage': {\n",
    "        'asset_category':    'buildings',\n",
    "        'asset_dimension':   'structure',\n",
    "        'impact_type':       'direct',\n",
    "        'impact_metric':     'damage_ratio',\n",
    "        'quantity_kind':     'ratio',\n",
    "        'loss_type':         'ground_up',\n",
    "    },\n",
    "    'agricultural_loss': {\n",
    "        'asset_category':    'agriculture',\n",
    "        'asset_dimension':   'product',\n",
    "        'impact_type':       'direct',\n",
    "        'impact_metric':     'asset_loss',\n",
    "        'quantity_kind':     'monetary',\n",
    "        'loss_type':         'ground_up',\n",
    "    },\n",
    "    'catastrophe_model': {\n",
    "        'asset_category':    'buildings',\n",
    "        'asset_dimension':   'structure',\n",
    "        'impact_type':       'total',\n",
    "        'impact_metric':     'loss_annual_average_value',\n",
    "        'quantity_kind':     'monetary',\n",
    "        'loss_type':         'ground_up',\n",
    "    },\n",
    "    'general_loss': {\n",
    "        'asset_category':    'population',\n",
    "        'asset_dimension':   'population',\n",
    "        'impact_type':       'direct',\n",
    "        'impact_metric':     'asset_loss',\n",
    "        'quantity_kind':     'count',\n",
    "        'loss_type':         'ground_up',\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- Loss-type inference rules ---\n",
    "# loss_type values that require specific loss_approach combinations.\n",
    "# net_precat/net_postcat only valid for analytical (cat model) approaches.\n",
    "LOSS_TYPE_APPROACH_RULES = {\n",
    "    'net_precat':  {'analytical'},\n",
    "    'net_postcat': {'analytical'},\n",
    "    'insured':     {'analytical', 'empirical', 'hybrid'},\n",
    "    'gross':       {'analytical', 'empirical', 'hybrid'},\n",
    "    'ground_up':   {'analytical', 'empirical', 'hybrid', 'judgement'},\n",
    "    'count':       {'analytical', 'empirical', 'hybrid', 'judgement'},\n",
    "}\n",
    "\n",
    "print(\"\\nLoss constraint tables defined.\")\n",
    "print(f\"  Group 1 - Asset triplets: {len(VALID_ASSET_TRIPLETS)} categories\")\n",
    "print(f\"  Group 2 - Impact metric constraints: {len(IMPACT_METRIC_CONSTRAINTS)} metrics\")\n",
    "print(f\"  Group 3 - Signal defaults: {len(LOSS_SIGNAL_DEFAULTS)} signal types\")\n",
    "print(f\"  Loss-type approach rules: {len(LOSS_TYPE_APPROACH_RULES)} loss types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loss_dataclass_04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LossExtractor initialized (constraint-validated).\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.2 Loss Data Classes and Extractor\n",
    "\n",
    "Updated to use VALID_LOSS_TRIPLETS constraint tables for consistent\n",
    "field combinations. All inferred values are validated against the 3 groups:\n",
    "  Group 1: asset_category -> asset_dimension\n",
    "  Group 2: impact_metric -> quantity_kind + impact_type\n",
    "  Group 3: loss_signal -> unified defaults\n",
    "\"\"\"\n",
    "\n",
    "# Re-import 'field' in case it was shadowed by loop variables in earlier cells\n",
    "from dataclasses import field\n",
    "\n",
    "@dataclass\n",
    "class LossEntryExtraction:\n",
    "    \"\"\"Extraction result for a single loss entry.\"\"\"\n",
    "    loss_signal_type: str                    # human_loss, economic_loss, etc.\n",
    "    hazard_type: Optional[str] = None        # hazard_type codelist\n",
    "    hazard_process: Optional[str] = None     # process_type codelist\n",
    "    asset_category: str = 'population'       # exposure_category codelist\n",
    "    asset_dimension: str = 'population'      # metric_dimension codelist\n",
    "    impact_type: str = 'direct'              # impact_type codelist\n",
    "    impact_modelling: str = 'observed'       # data_calculation_type codelist\n",
    "    impact_metric: str = 'asset_loss'        # impact_metric codelist\n",
    "    quantity_kind: str = 'count'             # open codelist\n",
    "    loss_type: str = 'ground_up'             # loss_type codelist\n",
    "    loss_approach: str = 'empirical'         # function_approach codelist\n",
    "    loss_frequency_type: str = 'empirical'   # analysis_type codelist\n",
    "    currency: Optional[str] = None           # ISO 4217\n",
    "    description: Optional[str] = None\n",
    "    # lineage\n",
    "    hazard_dataset: Optional[str] = None\n",
    "    exposure_dataset: Optional[str] = None\n",
    "    vulnerability_dataset: Optional[str] = None\n",
    "    confidence: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class LossExtraction:\n",
    "    \"\"\"Complete loss extraction for a dataset.\"\"\"\n",
    "    losses: List[LossEntryExtraction] = field(default_factory=list)\n",
    "    overall_confidence: float = 0.0\n",
    "\n",
    "    def has_any_signal(self) -> bool:\n",
    "        return len(self.losses) > 0\n",
    "\n",
    "\n",
    "class LossExtractor:\n",
    "    \"\"\"\n",
    "    Extracts RDLS Loss block components from HDX metadata.\n",
    "\n",
    "    Detects loss signals (human loss, economic loss, structural damage, etc.),\n",
    "    infers hazard context from NB09 cross-reference, and populates all required\n",
    "    fields with codelist-valid values.\n",
    "\n",
    "    All field combinations are validated against the 3 constraint groups:\n",
    "      Group 1: VALID_ASSET_TRIPLETS (asset_category -> asset_dimension)\n",
    "      Group 2: IMPACT_METRIC_CONSTRAINTS (metric -> quantity_kind + impact_type)\n",
    "      Group 3: LOSS_SIGNAL_DEFAULTS (unified per-signal defaults)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, signal_dict: Dict[str, Any], hazard_xref: Dict[str, Dict]):\n",
    "        self.signal_dict = signal_dict\n",
    "        self.hazard_xref = hazard_xref\n",
    "\n",
    "    def _get_all_text(self, record: Dict[str, Any]) -> str:\n",
    "        \"\"\"Concatenate all searchable text fields for pattern matching.\n",
    "\n",
    "        Note: methodology_other is deliberately excluded. It describes\n",
    "        how analysis was performed (e.g. 'produced AAL and PML values'),\n",
    "        not what data the dataset contains. Including it causes false\n",
    "        positives where methodology text about loss calculations triggers\n",
    "        loss detection on hazard-only datasets.\n",
    "        \"\"\"\n",
    "        parts = [\n",
    "            record.get('title', ''),\n",
    "            record.get('name', ''),\n",
    "            record.get('notes', ''),\n",
    "        ]\n",
    "        for tag in record.get('tags', []):\n",
    "            if isinstance(tag, dict):\n",
    "                parts.append(tag.get('name', ''))\n",
    "            elif isinstance(tag, str):\n",
    "                parts.append(tag)\n",
    "        for r in record.get('resources', []):\n",
    "            parts.append(r.get('name', '') or '')\n",
    "            parts.append(r.get('description', '') or '')\n",
    "        return ' '.join(filter(None, parts))\n",
    "\n",
    "    def _check_exclusions(self, text: str) -> bool:\n",
    "        \"\"\"Check if text matches exclusion patterns (false positive filter).\"\"\"\n",
    "        for p in LOSS_EXCLUSION_PATTERNS:\n",
    "            if p.search(text):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _detect_loss_signals(self, text: str) -> List[str]:\n",
    "        \"\"\"Detect which loss signal categories are present.\"\"\"\n",
    "        detected = []\n",
    "        for signal_type, patterns in LOSS_SIGNAL_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    detected.append(signal_type)\n",
    "                    break\n",
    "        return detected\n",
    "\n",
    "    def _infer_hazard_context(self, record: Dict[str, Any], text: str) -> Dict[str, Optional[str]]:\n",
    "        \"\"\"Infer hazard context from cross-reference or text.\"\"\"\n",
    "        dataset_id = record.get('id', '')\n",
    "\n",
    "        # Try cross-reference first (from NB 09)\n",
    "        if dataset_id in self.hazard_xref:\n",
    "            xref = self.hazard_xref[dataset_id]\n",
    "            ht_list = [h for h in xref['hazard_types'] if h in VALID_HAZARD_TYPES]\n",
    "            pt_list = [p for p in xref['process_types'] if p in VALID_PROCESS_TYPES]\n",
    "            return {\n",
    "                'hazard_type': ht_list[0] if ht_list else None,\n",
    "                'hazard_process': pt_list[0] if pt_list else None,\n",
    "            }\n",
    "\n",
    "        # Fallback: infer from text using signal dictionary patterns\n",
    "        text_lower = text.lower()\n",
    "        for ht, patterns in HAZARD_TYPE_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text_lower):\n",
    "                    return {\n",
    "                        'hazard_type': ht,\n",
    "                        'hazard_process': HAZARD_PROCESS_DEFAULT.get(ht),\n",
    "                    }\n",
    "\n",
    "        return {'hazard_type': None, 'hazard_process': None}\n",
    "\n",
    "    def _infer_asset_context(self, text: str, signal_type: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Infer asset_category and asset_dimension from text + signal type.\n",
    "\n",
    "        Validates against Group 1 (VALID_ASSET_TRIPLETS): the inferred\n",
    "        asset_dimension must be allowed for the asset_category. If not,\n",
    "        falls back to the category's default dimension (first in list).\n",
    "        \"\"\"\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # Try to detect specific asset category from text\n",
    "        detected_category = None\n",
    "        for cat, patterns in EXPOSURE_CATEGORY_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text_lower):\n",
    "                    if cat in VALID_EXPOSURE_CATEGORIES:\n",
    "                        detected_category = cat\n",
    "                        break\n",
    "            if detected_category:\n",
    "                break\n",
    "\n",
    "        # Get defaults from Group 3\n",
    "        defaults = LOSS_SIGNAL_DEFAULTS.get(signal_type, {\n",
    "            'asset_category': 'population',\n",
    "            'asset_dimension': 'population',\n",
    "        })\n",
    "\n",
    "        asset_category = detected_category or defaults['asset_category']\n",
    "\n",
    "        # --- Group 1 validation: asset_dimension must be valid for category ---\n",
    "        allowed_dims = VALID_ASSET_TRIPLETS.get(asset_category, ['structure'])\n",
    "\n",
    "        # Try signal-type default dimension first\n",
    "        default_dim = defaults.get('asset_dimension', allowed_dims[0])\n",
    "\n",
    "        if default_dim in allowed_dims:\n",
    "            asset_dimension = default_dim\n",
    "        else:\n",
    "            # Dimension not valid for this category; use category's default\n",
    "            asset_dimension = allowed_dims[0]\n",
    "\n",
    "        # Ensure asset_dimension is valid codelist value\n",
    "        if asset_dimension not in VALID_METRIC_DIMENSIONS:\n",
    "            asset_dimension = allowed_dims[0] if allowed_dims[0] in VALID_METRIC_DIMENSIONS else 'structure'\n",
    "\n",
    "        return {\n",
    "            'asset_category': asset_category,\n",
    "            'asset_dimension': asset_dimension,\n",
    "        }\n",
    "\n",
    "    def _validate_impact_metric(self, impact_metric: str, quantity_kind: str,\n",
    "                                 impact_type: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Validate impact_metric + quantity_kind + impact_type against Group 2.\n",
    "\n",
    "        Returns corrected values if the combination is invalid.\n",
    "        \"\"\"\n",
    "        constraints = IMPACT_METRIC_CONSTRAINTS.get(impact_metric)\n",
    "\n",
    "        if constraints is None:\n",
    "            # Unknown metric â€” keep as-is (open codelist may extend)\n",
    "            return {\n",
    "                'impact_metric': impact_metric,\n",
    "                'quantity_kind': quantity_kind,\n",
    "                'impact_type': impact_type,\n",
    "            }\n",
    "\n",
    "        expected_qty, allowed_types = constraints\n",
    "\n",
    "        # Fix quantity_kind if wrong\n",
    "        if quantity_kind != expected_qty:\n",
    "            quantity_kind = expected_qty\n",
    "\n",
    "        # Fix impact_type if not allowed for this metric\n",
    "        if impact_type not in allowed_types:\n",
    "            # Pick first allowed type (prefer 'direct')\n",
    "            impact_type = 'direct' if 'direct' in allowed_types else sorted(allowed_types)[0]\n",
    "\n",
    "        return {\n",
    "            'impact_metric': impact_metric,\n",
    "            'quantity_kind': quantity_kind,\n",
    "            'impact_type': impact_type,\n",
    "        }\n",
    "\n",
    "    def _validate_loss_approach(self, loss_type: str, loss_approach: str) -> str:\n",
    "        \"\"\"\n",
    "        Validate loss_type + loss_approach against LOSS_TYPE_APPROACH_RULES.\n",
    "\n",
    "        If the approach is invalid for the loss_type, returns a valid one.\n",
    "        \"\"\"\n",
    "        allowed = LOSS_TYPE_APPROACH_RULES.get(loss_type)\n",
    "        if allowed and loss_approach not in allowed:\n",
    "            # Pick empirical as a safe default, or first allowed\n",
    "            return 'empirical' if 'empirical' in allowed else sorted(allowed)[0]\n",
    "        return loss_approach\n",
    "\n",
    "    def _infer_loss_approach(self, text: str) -> str:\n",
    "        \"\"\"Infer loss approach from text.\"\"\"\n",
    "        scores = {k: 0 for k in VALID_FUNCTION_APPROACHES}\n",
    "        for approach, patterns in LOSS_APPROACH_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    scores[approach] += 1\n",
    "        best = max(scores, key=scores.get)\n",
    "        return best if scores[best] > 0 else 'empirical'\n",
    "\n",
    "    def _infer_loss_frequency(self, text: str) -> str:\n",
    "        \"\"\"Infer loss frequency type from text.\"\"\"\n",
    "        scores = {k: 0 for k in VALID_ANALYSIS_TYPES}\n",
    "        for freq, patterns in LOSS_FREQUENCY_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    scores[freq] += 1\n",
    "        best = max(scores, key=scores.get)\n",
    "        return best if scores[best] > 0 else 'empirical'\n",
    "\n",
    "    def _infer_impact_type(self, text: str) -> str:\n",
    "        \"\"\"Infer impact type from text.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        for itype, patterns in IMPACT_TYPE_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if isinstance(p, str):\n",
    "                    if re.search(p, text_lower, re.IGNORECASE):\n",
    "                        return itype\n",
    "                else:\n",
    "                    if p.search(text_lower):\n",
    "                        return itype\n",
    "        return 'direct'\n",
    "\n",
    "    def _infer_impact_modelling(self, text: str) -> str:\n",
    "        \"\"\"Infer impact modelling method from text.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        scores = {k: 0 for k in VALID_CALCULATION_TYPES}\n",
    "        for mod, patterns in IMPACT_MODELLING_PATTERNS.items():\n",
    "            for p in patterns:\n",
    "                if isinstance(p, str):\n",
    "                    if re.search(p, text_lower, re.IGNORECASE):\n",
    "                        scores[mod] += 1\n",
    "                else:\n",
    "                    if p.search(text_lower):\n",
    "                        scores[mod] += 1\n",
    "        best = max(scores, key=scores.get)\n",
    "        return best if scores[best] > 0 else 'observed'\n",
    "\n",
    "    def _detect_currency(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Detect currency from text.\"\"\"\n",
    "        for pattern, currency_code in CURRENCY_PATTERNS:\n",
    "            if pattern.search(text):\n",
    "                if currency_code in VALID_CURRENCIES:\n",
    "                    return currency_code\n",
    "        return None\n",
    "\n",
    "    def _detect_insured(self, text: str) -> bool:\n",
    "        \"\"\"Detect if losses are insured losses.\"\"\"\n",
    "        for p in INSURED_LOSS_PATTERNS:\n",
    "            if p.search(text):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _extract_reference_year(self, record: Dict[str, Any]) -> Optional[int]:\n",
    "        \"\"\"Extract reference year from dataset metadata.\"\"\"\n",
    "        dataset_date = record.get('dataset_date', '') or ''\n",
    "        year_match = re.search(r'(\\d{4})', dataset_date)\n",
    "        if year_match:\n",
    "            year = int(year_match.group(1))\n",
    "            if 1900 <= year <= 2100:\n",
    "                return year\n",
    "        last_mod = record.get('last_modified', '') or record.get('metadata_modified', '') or ''\n",
    "        year_match = re.search(r'(\\d{4})', last_mod)\n",
    "        if year_match:\n",
    "            year = int(year_match.group(1))\n",
    "            if 1900 <= year <= 2100:\n",
    "                return year\n",
    "        return None\n",
    "\n",
    "    def extract(self, record: Dict[str, Any]) -> LossExtraction:\n",
    "        \"\"\"\n",
    "        Extract loss information from HDX record.\n",
    "\n",
    "        All inferred field combinations are validated against the 3 constraint\n",
    "        groups before building LossEntryExtraction objects.\n",
    "        \"\"\"\n",
    "        text = self._get_all_text(record)\n",
    "\n",
    "        # Check for exclusion patterns\n",
    "        has_exclusion = self._check_exclusions(text)\n",
    "\n",
    "        # Detect loss signal types\n",
    "        signal_types = self._detect_loss_signals(text)\n",
    "        if not signal_types:\n",
    "            return LossExtraction()\n",
    "\n",
    "        # P5 fix: If exclusion patterns matched and only weak/generic signals remain,\n",
    "        # filter out the generic signals to reduce false positives\n",
    "        if has_exclusion:\n",
    "            strong_signals = [s for s in signal_types if s not in ('general_loss',)]\n",
    "            if not strong_signals:\n",
    "                return LossExtraction()\n",
    "            signal_types = strong_signals\n",
    "\n",
    "        # Get shared context\n",
    "        hazard_ctx = self._infer_hazard_context(record, text)\n",
    "        loss_approach = self._infer_loss_approach(text)\n",
    "        loss_frequency = self._infer_loss_frequency(text)\n",
    "        text_impact_type = self._infer_impact_type(text)\n",
    "        impact_modelling = self._infer_impact_modelling(text)\n",
    "        currency = self._detect_currency(text)\n",
    "        is_insured = self._detect_insured(text)\n",
    "        title = record.get('title', '')\n",
    "\n",
    "        # Deduplicate: group signals by (asset_category, impact_metric)\n",
    "        seen_keys = set()\n",
    "        losses = []\n",
    "\n",
    "        for signal_type in signal_types:\n",
    "            # --- Group 3: Get unified defaults for this signal ---\n",
    "            sig_defaults = LOSS_SIGNAL_DEFAULTS.get(signal_type, LOSS_SIGNAL_DEFAULTS['general_loss'])\n",
    "\n",
    "            # --- Group 1: Validate asset_category -> asset_dimension ---\n",
    "            asset_ctx = self._infer_asset_context(text, signal_type)\n",
    "\n",
    "            # Start with signal defaults for impact fields\n",
    "            impact_metric = sig_defaults['impact_metric']\n",
    "            quantity_kind = sig_defaults['quantity_kind']\n",
    "            impact_type = sig_defaults.get('impact_type', text_impact_type)\n",
    "            loss_type = sig_defaults['loss_type']\n",
    "\n",
    "            # Override impact_type from text if more specific\n",
    "            if text_impact_type != 'direct' and impact_type == 'direct':\n",
    "                impact_type = text_impact_type\n",
    "\n",
    "            # --- Group 2: Validate impact_metric + quantity_kind + impact_type ---\n",
    "            validated = self._validate_impact_metric(impact_metric, quantity_kind, impact_type)\n",
    "            impact_metric = validated['impact_metric']\n",
    "            quantity_kind = validated['quantity_kind']\n",
    "            impact_type = validated['impact_type']\n",
    "\n",
    "            # Override loss_type for insured losses\n",
    "            if is_insured and signal_type in ('economic_loss', 'structural_damage', 'catastrophe_model'):\n",
    "                loss_type = 'insured'\n",
    "\n",
    "            # --- Validate loss_type + loss_approach ---\n",
    "            validated_approach = self._validate_loss_approach(loss_type, loss_approach)\n",
    "\n",
    "            # Dedup key\n",
    "            key = (asset_ctx['asset_category'], impact_metric)\n",
    "            if key in seen_keys:\n",
    "                continue\n",
    "            seen_keys.add(key)\n",
    "\n",
    "            # Build description\n",
    "            description = f\"Loss data from HDX dataset: {title[:200]}\"\n",
    "\n",
    "            # Confidence based on hazard context availability\n",
    "            confidence = 0.8 if hazard_ctx['hazard_type'] else 0.6\n",
    "\n",
    "            entry = LossEntryExtraction(\n",
    "                loss_signal_type=signal_type,\n",
    "                hazard_type=hazard_ctx['hazard_type'],\n",
    "                hazard_process=hazard_ctx['hazard_process'],\n",
    "                asset_category=asset_ctx['asset_category'],\n",
    "                asset_dimension=asset_ctx['asset_dimension'],\n",
    "                impact_type=impact_type,\n",
    "                impact_modelling=impact_modelling,\n",
    "                impact_metric=impact_metric,\n",
    "                quantity_kind=quantity_kind,\n",
    "                loss_type=loss_type,\n",
    "                loss_approach=validated_approach,\n",
    "                loss_frequency_type=loss_frequency,\n",
    "                currency=currency if quantity_kind == 'monetary' else None,\n",
    "                description=description[:500],\n",
    "                confidence=confidence,\n",
    "            )\n",
    "            losses.append(entry)\n",
    "\n",
    "        confidences = [e.confidence for e in losses]\n",
    "        overall = float(np.mean(confidences)) if confidences else 0.0\n",
    "\n",
    "        return LossExtraction(losses=losses, overall_confidence=overall)\n",
    "\n",
    "\n",
    "# Initialize\n",
    "loss_extractor = LossExtractor(SIGNAL_DICT, HAZARD_XREF)\n",
    "print(f\"\\nLossExtractor initialized (constraint-validated).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_builder_md_05",
   "metadata": {},
   "source": [
    "### 8.3 Loss Block Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "loss_builder_06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss block builder defined (constraint-validated).\n",
      "  - Group 1: asset_dimension validated for asset_category\n",
      "  - Group 2: quantity_kind + impact_type validated for impact_metric\n",
      "  - All entries validated against schema codelists\n",
      "  - impact_and_losses includes all 7 required fields\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.3 Build RDLS Loss Block\n",
    "\n",
    "Builds schema-compliant loss block with:\n",
    "- losses[]: array of loss entries, each with all required fields\n",
    "- impact_and_losses: nested sub-object with 7 required fields\n",
    "- All values validated against closed codelists AND constraint tables\n",
    "- Group 1: asset_dimension valid for asset_category\n",
    "- Group 2: quantity_kind valid for impact_metric\n",
    "\"\"\"\n",
    "\n",
    "def build_loss_block(\n",
    "    extraction: LossExtraction,\n",
    "    dataset_id: str,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS loss block from extraction results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    extraction : LossExtraction\n",
    "        Extraction results (already constraint-validated by LossExtractor)\n",
    "    dataset_id : str\n",
    "        Dataset identifier for building unique IDs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optional[Dict[str, Any]]\n",
    "        RDLS loss block or None if no data\n",
    "    \"\"\"\n",
    "    if not extraction.has_any_signal():\n",
    "        return None\n",
    "\n",
    "    losses_list = []\n",
    "    for idx, entry in enumerate(extraction.losses):\n",
    "        # --- Final codelist validation ---\n",
    "        impact_type = entry.impact_type if entry.impact_type in VALID_IMPACT_TYPES else 'direct'\n",
    "        impact_modelling = entry.impact_modelling if entry.impact_modelling in VALID_CALCULATION_TYPES else 'observed'\n",
    "        impact_metric = entry.impact_metric if entry.impact_metric in VALID_IMPACT_METRICS else 'asset_loss'\n",
    "        loss_type = entry.loss_type if entry.loss_type in VALID_LOSS_TYPES else 'ground_up'\n",
    "        loss_approach = entry.loss_approach if entry.loss_approach in VALID_FUNCTION_APPROACHES else 'empirical'\n",
    "        loss_freq = entry.loss_frequency_type if entry.loss_frequency_type in VALID_ANALYSIS_TYPES else 'empirical'\n",
    "\n",
    "        # --- Group 2 re-validation at build time ---\n",
    "        metric_constraint = IMPACT_METRIC_CONSTRAINTS.get(impact_metric)\n",
    "        if metric_constraint:\n",
    "            expected_qty, allowed_types = metric_constraint\n",
    "            quantity_kind = expected_qty  # enforce correct quantity_kind\n",
    "            if impact_type not in allowed_types:\n",
    "                impact_type = 'direct' if 'direct' in allowed_types else sorted(allowed_types)[0]\n",
    "        else:\n",
    "            quantity_kind = entry.quantity_kind or 'count'\n",
    "\n",
    "        # Build impact_and_losses sub-object (7 required fields)\n",
    "        impact_and_losses = {\n",
    "            'impact_type': impact_type,\n",
    "            'impact_modelling': impact_modelling,\n",
    "            'impact_metric': impact_metric,\n",
    "            'quantity_kind': quantity_kind,\n",
    "            'loss_type': loss_type,\n",
    "            'loss_approach': loss_approach,\n",
    "            'loss_frequency_type': loss_freq,\n",
    "        }\n",
    "\n",
    "        # Optional: currency (only when quantity_kind is monetary)\n",
    "        if entry.currency and entry.currency in VALID_CURRENCIES and quantity_kind == 'monetary':\n",
    "            impact_and_losses['currency'] = entry.currency\n",
    "\n",
    "        # --- Group 1 re-validation: asset_dimension for asset_category ---\n",
    "        asset_category = entry.asset_category if entry.asset_category in VALID_EXPOSURE_CATEGORIES else None\n",
    "        asset_dimension = entry.asset_dimension if entry.asset_dimension in VALID_METRIC_DIMENSIONS else None\n",
    "\n",
    "        if asset_category and asset_dimension:\n",
    "            allowed_dims = VALID_ASSET_TRIPLETS.get(asset_category, [])\n",
    "            if allowed_dims and asset_dimension not in allowed_dims:\n",
    "                asset_dimension = allowed_dims[0]\n",
    "\n",
    "        # Build the loss entry (5 required top-level fields)\n",
    "        loss_entry = {\n",
    "            'id': f\"loss_{dataset_id[:8]}_{idx + 1}\",\n",
    "            'hazard_type': entry.hazard_type if entry.hazard_type in VALID_HAZARD_TYPES else None,\n",
    "            'asset_category': asset_category,\n",
    "            'asset_dimension': asset_dimension,\n",
    "            'impact_and_losses': impact_and_losses,\n",
    "        }\n",
    "\n",
    "        # P1+P2 fix: Skip loss entries with no determinable hazard or asset\n",
    "        if loss_entry['hazard_type'] is None or asset_category is None or asset_dimension is None:\n",
    "            continue\n",
    "\n",
    "        # Optional: hazard_process\n",
    "        if entry.hazard_process and entry.hazard_process in VALID_PROCESS_TYPES:\n",
    "            loss_entry['hazard_process'] = entry.hazard_process\n",
    "\n",
    "        # Optional: lineage\n",
    "        lineage = {}\n",
    "        if entry.hazard_dataset:\n",
    "            lineage['hazard_dataset'] = entry.hazard_dataset\n",
    "        if entry.exposure_dataset:\n",
    "            lineage['exposure_dataset'] = entry.exposure_dataset\n",
    "        if entry.vulnerability_dataset:\n",
    "            lineage['vulnerability_dataset'] = entry.vulnerability_dataset\n",
    "        if lineage:\n",
    "            loss_entry['lineage'] = lineage\n",
    "\n",
    "        # Optional: description\n",
    "        if entry.description:\n",
    "            loss_entry['description'] = entry.description\n",
    "\n",
    "        losses_list.append(loss_entry)\n",
    "\n",
    "    return {'losses': losses_list} if losses_list else None\n",
    "\n",
    "\n",
    "print(\"Loss block builder defined (constraint-validated).\")\n",
    "print(\"  - Group 1: asset_dimension validated for asset_category\")\n",
    "print(\"  - Group 2: quantity_kind + impact_type validated for impact_metric\")\n",
    "print(\"  - All entries validated against schema codelists\")\n",
    "print(\"  - impact_and_losses includes all 7 required fields\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_test_md_07",
   "metadata": {},
   "source": [
    "### 8.4 Test Loss Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "loss_test_samples_08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 unique Loss test samples across 7 categories.\n",
      "\n",
      "Samples per category:\n",
      "  human_loss: 7\n",
      "  displacement: 12\n",
      "  structural_damage: 9\n",
      "  economic_loss: 0\n",
      "  agricultural_loss: 0\n",
      "  general_loss: 10\n",
      "  edge_cases: 9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.4 Load Loss Test Samples\n",
    "\n",
    "Organized by expected loss signal type to stress-test extraction.\n",
    "\"\"\"\n",
    "\n",
    "LOSS_TEST_SAMPLES = {\n",
    "    'human_loss': [\n",
    "        ('casualty', 'Casualty/fatality data'),\n",
    "        ('mortality', 'Mortality data'),\n",
    "        ('fatality', 'Fatality records'),\n",
    "    ],\n",
    "    'displacement': [\n",
    "        ('displaced', 'Displacement data'),\n",
    "        ('evacuation', 'Evacuation data'),\n",
    "        ('idp', 'IDP data'),\n",
    "    ],\n",
    "    'structural_damage': [\n",
    "        ('damage-assessment', 'Damage assessment'),\n",
    "        ('building-damage', 'Building damage'),\n",
    "        ('infrastructure-damage', 'Infrastructure damage'),\n",
    "    ],\n",
    "    'economic_loss': [\n",
    "        ('economic-loss', 'Economic loss data'),\n",
    "        ('insurance', 'Insurance claim data'),\n",
    "    ],\n",
    "    'agricultural_loss': [\n",
    "        ('crop-loss', 'Crop loss data'),\n",
    "        ('livestock-loss', 'Livestock loss data'),\n",
    "    ],\n",
    "    'general_loss': [\n",
    "        ('disaster-loss', 'Disaster loss records'),\n",
    "        ('disaster-incidents', 'Disaster incidents'),\n",
    "        ('post-disaster', 'Post disaster assessment'),\n",
    "        ('pdna', 'Post-Disaster Needs Assessment'),\n",
    "    ],\n",
    "    'edge_cases': [\n",
    "        ('flood', 'Flood datasets (may have loss signals)'),\n",
    "        ('earthquake', 'Earthquake datasets (may have loss signals)'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Load samples by searching filenames\n",
    "loss_sample_records = []\n",
    "loss_sample_meta = []\n",
    "loss_loaded_ids = set()\n",
    "\n",
    "for category, keyword_list in LOSS_TEST_SAMPLES.items():\n",
    "    for keyword, note in keyword_list:\n",
    "        files = sorted(DATASET_METADATA_DIR.glob(f'*{keyword}*.json'))[:5]\n",
    "        for fp in files:\n",
    "            try:\n",
    "                with open(fp, 'r', encoding='utf-8') as f:\n",
    "                    record = json.load(f)\n",
    "                rid = record.get('id', fp.stem)\n",
    "                if rid not in loss_loaded_ids:\n",
    "                    loss_loaded_ids.add(rid)\n",
    "                    loss_sample_records.append(record)\n",
    "                    loss_sample_meta.append({\n",
    "                        'category': category,\n",
    "                        'note': note,\n",
    "                        'filename': fp.name,\n",
    "                    })\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(f\"Loaded {len(loss_sample_records)} unique Loss test samples across {len(LOSS_TEST_SAMPLES)} categories.\")\n",
    "print(f\"\\nSamples per category:\")\n",
    "for cat in LOSS_TEST_SAMPLES:\n",
    "    count = sum(1 for m in loss_sample_meta if m['category'] == cat)\n",
    "    print(f\"  {cat}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "loss_test_run_09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "LOSS EXTRACTION TEST RESULTS\n",
      "Testing 47 samples\n",
      "==========================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[human_loss] Ethiopia-Infant Mortality Rate\n",
      "  LOSS: signal=human_loss | hazard=None | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[human_loss] Perú: Covid19 Mortality Rate in Lima\n",
      "  LOSS: signal=human_loss | hazard=None | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[human_loss] Infant Mortality / Mortalité infantile -- Under 5 Mortality / Mortalité des\n",
      "  LOSS: signal=human_loss | hazard=None | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[human_loss] Financial Times - Excess mortality during COVID-19 pandemic\n",
      "  LOSS: signal=human_loss | hazard=None | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[human_loss] Somalia - Mortality Estimation 2014-2018\n",
      "  LOSS: signal=human_loss | hazard=None | asset=population/population | metric=casualty_count | loss_type=count | approach=judgement | freq=empirical | conf=0.60\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=judgement | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[human_loss] Peace and Security Pillar: Uniformed Personnel Fatality Rate in Active Peac\n",
      "  LOSS: signal=human_loss | hazard=None | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[human_loss] Peace and Security Pillar: Uniformed Personnel Fatality Rate in Peace Opera\n",
      "  LOSS: signal=human_loss | hazard=None | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Internally Displaced Persons Shelters in Daraa and Quneitra Governorate\n",
      "  LOSS: signal=displacement | hazard=flood | asset=population/population | metric=displaced_count | loss_type=count | approach=judgement | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Kenya -  Tally of Internaly displaced persons resulting from natural disast\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "  LOSS: signal=affected_population | hazard=None | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Lake Chad Basin Crisis Displaced Persons\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Somalia: Internally Displaced Persons (IDPs)\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Internally Displaced Persons Shelters in Maungdaw Township of Rakhine State\n",
      "  LOSS: signal=displacement | hazard=flood | asset=infrastructure/structure | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Typhoon Ruby (Hagupit) Evacuation Centers\n",
      "  LOSS: signal=displacement | hazard=convective_storm | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Fiji Evacuation Tracking & Monitoring Cycle 2 - Site Assessment data\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Uzbekistan - Internal Displacements (New Displacements) – IDPs\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Nicaragua - Internal Displacements (New Displacements) – IDPs\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Isle of Man - Internal Displacements (New Displacements) – IDPs\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] South Sudan Displacement - [IDPs, Returnees] - Baseline Assessment [IOM DTM\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=judgement | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[displacement] Paraguay - Internal Displacements (New Displacements) – IDPs\n",
      "  LOSS: signal=displacement | hazard=None | asset=population/population | metric=displaced_count | loss_type=count | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Damage assessment in Saint Martin’s Island Cox’Bazar District, Chittagong D\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Damage Assessment in Ambae Island, Penama Province, Vanuatu\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Haiti - Damage Assessment Overview of Dame Marie and Anse d'Hainault Areas,\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=infrastructure/structure | metric=damage_ratio | loss_type=ground_up | approach=hybrid | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Damage Assessment in Al Bab, Syria as of 11 February 2023\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Hurricane Melissa: Building Damage Assessment in Jamaica\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Sri Lanka Floods: Building Damage Assessment in Colombo\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Satellite detected water & building damage assessment over  Belet Weyne Tow\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Building Damage Assessment in Bellavista village, Bojayá Municipality, Choc\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[structural_damage] Tornado Arkansas - Building Damage Assessment\n",
      "  LOSS: signal=structural_damage | hazard=convective_storm | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Disaster loss and damage dataset for Albania\n",
      "  LOSS: signal=structural_damage | hazard=None | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "  LOSS: signal=general_loss | hazard=None | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Disaster loss and damage dataset for Sri Lanka\n",
      "  LOSS: signal=structural_damage | hazard=None | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "  LOSS: signal=general_loss | hazard=None | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Disaster loss and damage data for Cambodia\n",
      "  LOSS: signal=structural_damage | hazard=None | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "  LOSS: signal=general_loss | hazard=None | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Disaster loss and damage dataset for Senegal\n",
      "  LOSS: signal=structural_damage | hazard=None | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "  LOSS: signal=general_loss | hazard=None | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Disaster loss and damage dataset for Niger\n",
      "  LOSS: signal=structural_damage | hazard=None | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "  LOSS: signal=general_loss | hazard=None | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.60\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Afghanistan - Natural Disaster Incidents in 2018\n",
      "  LOSS: signal=affected_population | hazard=flood | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=general_loss | hazard=flood | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Natural disaster incidents from 1 January to 31 December 2012\n",
      "  LOSS: signal=human_loss | hazard=flood | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=affected_population | hazard=flood | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=population/population | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=general_loss | hazard=flood | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Afghanistan - Natural Disaster Incidents\n",
      "  LOSS: signal=affected_population | hazard=flood | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=general_loss | hazard=flood | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Afghanistan - Natural Disaster Incidents in 2024\n",
      "  LOSS: signal=affected_population | hazard=flood | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=general_loss | hazard=flood | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[general_loss] Natural disaster incidents from 1 January to 31 December 2013\n",
      "  LOSS: signal=human_loss | hazard=flood | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=affected_population | hazard=flood | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=structural_damage | hazard=flood | asset=population/population | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=general_loss | hazard=flood | asset=population/population | metric=asset_loss | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Türkiye and Syria Earthquake - Key Figures\n",
      "  LOSS: signal=human_loss | hazard=earthquake | asset=population/population | metric=casualty_count | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=structural_damage | hazard=earthquake | asset=buildings/structure | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Vanuatu: Earthquake - 6.1M - Sep 2023\n",
      "  LOSS: signal=affected_population | hazard=earthquake | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Afghanistan -  Post-Earthquake Shelter Evaluation\n",
      "  LOSS: signal=affected_population | hazard=earthquake | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "  LOSS: signal=structural_damage | hazard=earthquake | asset=population/population | metric=damage_ratio | loss_type=ground_up | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "[edge_cases] Population Affected in the Nepal Earthquake (Estimate)\n",
      "  LOSS: signal=affected_population | hazard=earthquake | asset=population/population | metric=exposure_to_hazard | loss_type=count | approach=empirical | freq=empirical | conf=0.80\n",
      "\n",
      "==========================================================================================\n",
      "LOSS TEST SUMMARY\n",
      "==========================================================================================\n",
      "  Total samples: 47\n",
      "  With loss signal: 42 (89.4%)\n",
      "  Total loss entries: 60\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.5 Run Loss Extraction on Test Samples\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"LOSS EXTRACTION TEST RESULTS\")\n",
    "print(f\"Testing {len(loss_sample_records)} samples\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "loss_test_results = []\n",
    "loss_entry_count = 0\n",
    "\n",
    "for record, meta in zip(loss_sample_records, loss_sample_meta):\n",
    "    extraction = loss_extractor.extract(record)\n",
    "\n",
    "    loss_test_results.append({\n",
    "        'id': record.get('id'),\n",
    "        'title': record.get('title', '')[:70],\n",
    "        'category': meta['category'],\n",
    "        'extraction': extraction,\n",
    "    })\n",
    "\n",
    "    if extraction.has_any_signal():\n",
    "        loss_entry_count += len(extraction.losses)\n",
    "\n",
    "        print(f\"\\n{'â”€' * 90}\")\n",
    "        print(f\"[{meta['category']}] {record.get('title', '')[:75]}\")\n",
    "\n",
    "        for entry in extraction.losses:\n",
    "            print(f\"  LOSS: signal={entry.loss_signal_type} | hazard={entry.hazard_type} | \"\n",
    "                  f\"asset={entry.asset_category}/{entry.asset_dimension} | \"\n",
    "                  f\"metric={entry.impact_metric} | loss_type={entry.loss_type} | \"\n",
    "                  f\"approach={entry.loss_approach} | freq={entry.loss_frequency_type} | \"\n",
    "                  f\"conf={entry.confidence:.2f}\")\n",
    "            if entry.currency:\n",
    "                print(f\"         currency={entry.currency}\")\n",
    "\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"LOSS TEST SUMMARY\")\n",
    "print(f\"{'=' * 90}\")\n",
    "total = len(loss_test_results)\n",
    "with_signal = sum(1 for r in loss_test_results if r['extraction'].has_any_signal())\n",
    "print(f\"  Total samples: {total}\")\n",
    "print(f\"  With loss signal: {with_signal} ({with_signal/total*100:.1f}%)\")\n",
    "print(f\"  Total loss entries: {loss_entry_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "loss_compliance_10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "LOSS STRUCTURAL COMPLIANCE VERIFICATION\n",
      "==========================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Loss block preview: Internally Displaced Persons Shelters in Daraa and Quneitra Governorat\n",
      "{\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"id\": \"loss_1b24eb18_1\",\n",
      "      \"hazard_type\": \"flood\",\n",
      "      \"asset_category\": \"population\",\n",
      "      \"asset_dimension\": \"population\",\n",
      "      \"impact_and_losses\": {\n",
      "        \"impact_type\": \"direct\",\n",
      "        \"impact_modelling\": \"inferred\",\n",
      "        \"impact_metric\": \"displaced_count\",\n",
      "        \"quantity_kind\": \"count\",\n",
      "        \"loss_type\": \"count\",\n",
      "        \"loss_approach\": \"judgement\",\n",
      "        \"loss_frequency_type\": \"empirical\"\n",
      "      },\n",
      "      \"description\": \"Loss data from HDX dataset: Internally Displaced Persons Shelters in Daraa and Quneitra Governorate\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Loss block preview: Internally Displaced Persons Shelters in Maungdaw Township of Rakhine \n",
      "{\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"id\": \"loss_5067f403_1\",\n",
      "      \"hazard_type\": \"flood\",\n",
      "      \"asset_category\": \"infrastructure\",\n",
      "      \"asset_dimension\": \"structure\",\n",
      "      \"impact_and_losses\": {\n",
      "        \"impact_type\": \"direct\",\n",
      "        \"impact_modelling\": \"observed\",\n",
      "        \"impact_metric\": \"displaced_count\",\n",
      "        \"quantity_kind\": \"count\",\n",
      "        \"loss_type\": \"count\",\n",
      "        \"loss_approach\": \"empirical\",\n",
      "        \"loss_frequency_type\": \"empirical\"\n",
      "      },\n",
      "      \"description\": \"Loss data from HDX dataset: Internally Displaced Persons Shelters in Maungdaw Township of Rakhine State in Myanmar\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Loss block preview: Typhoon Ruby (Hagupit) Evacuation Centers\n",
      "{\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"id\": \"loss_763f6604_1\",\n",
      "      \"hazard_type\": \"convective_storm\",\n",
      "      \"asset_category\": \"population\",\n",
      "      \"asset_dimension\": \"population\",\n",
      "      \"impact_and_losses\": {\n",
      "        \"impact_type\": \"direct\",\n",
      "        \"impact_modelling\": \"observed\",\n",
      "        \"impact_metric\": \"displaced_count\",\n",
      "        \"quantity_kind\": \"count\",\n",
      "        \"loss_type\": \"count\",\n",
      "        \"loss_approach\": \"empirical\",\n",
      "        \"loss_frequency_type\": \"empirical\"\n",
      "      },\n",
      "      \"hazard_process\": \"tropical_cyclone\",\n",
      "      \"description\": \"Loss data from HDX dataset: Typhoon Ruby (Hagupit) Evacuation Centers\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "==========================================================================================\n",
      "LOSS COMPLIANCE REPORT\n",
      "==========================================================================================\n",
      "  Total blocks built:           21\n",
      "  Total loss entries:            32\n",
      "\n",
      "  Top-level required fields:     PASS\n",
      "  impact_and_losses required:    PASS\n",
      "  Codelist compliance:           PASS\n",
      "  ID uniqueness:                 PASS\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.6 Build Loss Blocks and Verify Structural Compliance\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"LOSS STRUCTURAL COMPLIANCE VERIFICATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "loss_total_blocks = 0\n",
    "loss_total_entries = 0\n",
    "\n",
    "# Compliance trackers\n",
    "loss_field_violations = []\n",
    "loss_impact_violations = []\n",
    "loss_codelist_violations = []\n",
    "\n",
    "LOSS_TOP_REQUIRED = ['id', 'hazard_type', 'asset_category', 'asset_dimension', 'impact_and_losses']\n",
    "IMPACT_LOSSES_REQUIRED = [\n",
    "    'impact_type', 'impact_modelling', 'impact_metric', 'quantity_kind',\n",
    "    'loss_type', 'loss_approach', 'loss_frequency_type'\n",
    "]\n",
    "\n",
    "for result in loss_test_results:\n",
    "    extraction = result['extraction']\n",
    "    if not extraction.has_any_signal():\n",
    "        continue\n",
    "\n",
    "    block = build_loss_block(extraction, result['id'])\n",
    "    if not block:\n",
    "        continue\n",
    "\n",
    "    loss_total_blocks += 1\n",
    "\n",
    "    for entry in block.get('losses', []):\n",
    "        loss_total_entries += 1\n",
    "\n",
    "        # Check top-level required fields\n",
    "        for fld in LOSS_TOP_REQUIRED:\n",
    "            if fld not in entry or not entry[fld]:\n",
    "                loss_field_violations.append(f\"{result['id'][:8]}: missing {fld}\")\n",
    "\n",
    "        # Check impact_and_losses required fields\n",
    "        ial = entry.get('impact_and_losses', {})\n",
    "        for fld in IMPACT_LOSSES_REQUIRED:\n",
    "            if fld not in ial or not ial[fld]:\n",
    "                loss_impact_violations.append(f\"{result['id'][:8]}: impact_and_losses missing {fld}\")\n",
    "\n",
    "        # Check codelist values\n",
    "        if entry.get('hazard_type') and entry['hazard_type'] not in VALID_HAZARD_TYPES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: hazard_type='{entry['hazard_type']}'\")\n",
    "        if entry.get('asset_category') and entry['asset_category'] not in VALID_EXPOSURE_CATEGORIES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: asset_category='{entry['asset_category']}'\")\n",
    "        if entry.get('asset_dimension') and entry['asset_dimension'] not in VALID_METRIC_DIMENSIONS:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: asset_dimension='{entry['asset_dimension']}'\")\n",
    "        if entry.get('hazard_process') and entry['hazard_process'] not in VALID_PROCESS_TYPES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: hazard_process='{entry['hazard_process']}'\")\n",
    "\n",
    "        # Check impact_and_losses codelist values\n",
    "        if ial.get('impact_type') and ial['impact_type'] not in VALID_IMPACT_TYPES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: impact_type='{ial['impact_type']}'\")\n",
    "        if ial.get('impact_modelling') and ial['impact_modelling'] not in VALID_CALCULATION_TYPES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: impact_modelling='{ial['impact_modelling']}'\")\n",
    "        if ial.get('impact_metric') and ial['impact_metric'] not in VALID_IMPACT_METRICS:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: impact_metric='{ial['impact_metric']}'\")\n",
    "        if ial.get('loss_type') and ial['loss_type'] not in VALID_LOSS_TYPES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: loss_type='{ial['loss_type']}'\")\n",
    "        if ial.get('loss_approach') and ial['loss_approach'] not in VALID_FUNCTION_APPROACHES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: loss_approach='{ial['loss_approach']}'\")\n",
    "        if ial.get('loss_frequency_type') and ial['loss_frequency_type'] not in VALID_ANALYSIS_TYPES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: loss_frequency_type='{ial['loss_frequency_type']}'\")\n",
    "        if ial.get('currency') and ial['currency'] not in VALID_CURRENCIES:\n",
    "            loss_codelist_violations.append(f\"{result['id'][:8]}: currency='{ial['currency']}'\")\n",
    "\n",
    "    # Show first 3 block previews\n",
    "    if loss_total_blocks <= 3:\n",
    "        print(f\"\\n{'â”€' * 90}\")\n",
    "        print(f\"Loss block preview: {result['title']}\")\n",
    "        print(json.dumps(block, indent=2)[:2000])\n",
    "\n",
    "# --- Compliance Report ---\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"LOSS COMPLIANCE REPORT\")\n",
    "print(f\"{'=' * 90}\")\n",
    "print(f\"  Total blocks built:           {loss_total_blocks}\")\n",
    "print(f\"  Total loss entries:            {loss_total_entries}\")\n",
    "print()\n",
    "print(f\"  Top-level required fields:     {'PASS' if not loss_field_violations else f'FAIL ({len(loss_field_violations)} violations)'}\")\n",
    "for v in loss_field_violations[:5]:\n",
    "    print(f\"    - {v}\")\n",
    "print(f\"  impact_and_losses required:    {'PASS' if not loss_impact_violations else f'FAIL ({len(loss_impact_violations)} violations)'}\")\n",
    "for v in loss_impact_violations[:5]:\n",
    "    print(f\"    - {v}\")\n",
    "print(f\"  Codelist compliance:           {'PASS' if not loss_codelist_violations else f'FAIL ({len(loss_codelist_violations)} violations)'}\")\n",
    "for v in loss_codelist_violations[:5]:\n",
    "    print(f\"    - {v}\")\n",
    "\n",
    "# ID uniqueness check\n",
    "all_loss_ids = []\n",
    "for result in loss_test_results:\n",
    "    extraction = result['extraction']\n",
    "    if not extraction.has_any_signal():\n",
    "        continue\n",
    "    block = build_loss_block(extraction, result['id'])\n",
    "    if not block:\n",
    "        continue\n",
    "    for entry in block.get('losses', []):\n",
    "        all_loss_ids.append(entry['id'])\n",
    "\n",
    "dup_loss_ids = [id for id in all_loss_ids if all_loss_ids.count(id) > 1]\n",
    "print(f\"  ID uniqueness:                 {'PASS' if not dup_loss_ids else f'FAIL ({len(set(dup_loss_ids))} duplicates)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_batch_md_11",
   "metadata": {},
   "source": [
    "### 8.7 Loss Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "loss_batch_12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all records for Loss extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0b053f8def4d948bf0488b07ba20ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting loss:   0%|          | 0/26246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.7 Process Full Corpus for Loss Extraction\n",
    "\"\"\"\n",
    "\n",
    "def process_loss_extraction(\n",
    "    metadata_dir: Path,\n",
    "    extractor: LossExtractor,\n",
    "    limit: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process all records for loss extraction.\"\"\"\n",
    "    json_files = sorted(metadata_dir.glob('*.json'))\n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "\n",
    "    results = []\n",
    "    iterator = tqdm(json_files, desc=\"Extracting loss\") if HAS_TQDM else json_files\n",
    "\n",
    "    for filepath in iterator:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                record = json.load(f)\n",
    "\n",
    "            extraction = extractor.extract(record)\n",
    "\n",
    "            loss_signal_types = [e.loss_signal_type for e in extraction.losses]\n",
    "            hazard_types = list(set(e.hazard_type for e in extraction.losses if e.hazard_type))\n",
    "            asset_categories = list(set(e.asset_category for e in extraction.losses))\n",
    "\n",
    "            results.append({\n",
    "                'id': record.get('id'),\n",
    "                'title': record.get('title'),\n",
    "                'organization': record.get('organization'),\n",
    "                'has_loss': extraction.has_any_signal(),\n",
    "                'loss_count': len(extraction.losses),\n",
    "                'loss_signal_types': loss_signal_types,\n",
    "                'hazard_types': hazard_types,\n",
    "                'asset_categories': asset_categories,\n",
    "                'overall_confidence': extraction.overall_confidence,\n",
    "                'extraction': extraction,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({'id': filepath.stem, 'error': str(e), 'has_loss': False})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "LOSS_PROCESS_LIMIT = None  # Set to e.g. 2000 for testing, None for full corpus\n",
    "\n",
    "print(f\"Processing {'all' if LOSS_PROCESS_LIMIT is None else LOSS_PROCESS_LIMIT} records for Loss extraction...\")\n",
    "df_loss = process_loss_extraction(DATASET_METADATA_DIR, loss_extractor, limit=LOSS_PROCESS_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "loss_stats_13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOSS EXTRACTION STATISTICS\n",
      "============================================================\n",
      "\n",
      "Total records processed: 26,246\n",
      "  With any loss signal: 5,771 (22.0%)\n",
      "  Average loss entries per record: 1.2\n",
      "\n",
      "Loss Signal Type Distribution:\n",
      "  human_loss: 3,093\n",
      "  displacement: 1,908\n",
      "  affected_population: 1,179\n",
      "  structural_damage: 482\n",
      "  general_loss: 39\n",
      "  agricultural_loss: 7\n",
      "  economic_loss: 2\n",
      "  catastrophe_model: 1\n",
      "\n",
      "Hazard Types in Loss Records:\n",
      "  flood: 637\n",
      "  earthquake: 73\n",
      "  convective_storm: 72\n",
      "  drought: 33\n",
      "  strong_wind: 2\n",
      "  volcanic: 2\n",
      "  coastal_flood: 1\n",
      "  landslide: 1\n",
      "\n",
      "Asset Category Distribution:\n",
      "  population: 2,371\n",
      "  infrastructure: 2,002\n",
      "  buildings: 1,144\n",
      "  economic_indicator: 245\n",
      "  agriculture: 27\n",
      "  natural_environment: 1\n",
      "\n",
      "Confidence Distribution:\n",
      "  Mean: 0.63\n",
      "  Median: 0.60\n",
      "  High (>=0.7): 821\n",
      "  Medium (0.5-0.7): 4,950\n",
      "  Low (<0.5): 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.8 Loss Extraction Statistics\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOSS EXTRACTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_loss)\n",
    "with_loss = df_loss['has_loss'].sum()\n",
    "avg_entries = df_loss[df_loss['has_loss']]['loss_count'].mean() if with_loss > 0 else 0\n",
    "\n",
    "print(f\"\\nTotal records processed: {total:,}\")\n",
    "print(f\"  With any loss signal: {with_loss:,} ({with_loss/total*100:.1f}%)\")\n",
    "print(f\"  Average loss entries per record: {avg_entries:.1f}\")\n",
    "\n",
    "# Loss signal type distribution\n",
    "signal_counts = Counter()\n",
    "for signals in df_loss['loss_signal_types'].dropna():\n",
    "    if isinstance(signals, list):\n",
    "        signal_counts.update(signals)\n",
    "\n",
    "if signal_counts:\n",
    "    print(f\"\\nLoss Signal Type Distribution:\")\n",
    "    for sig, count in signal_counts.most_common():\n",
    "        print(f\"  {sig}: {count:,}\")\n",
    "\n",
    "# Hazard type distribution among loss records\n",
    "hazard_counts = Counter()\n",
    "for htypes in df_loss['hazard_types'].dropna():\n",
    "    if isinstance(htypes, list):\n",
    "        hazard_counts.update(htypes)\n",
    "\n",
    "if hazard_counts:\n",
    "    print(f\"\\nHazard Types in Loss Records:\")\n",
    "    for ht, count in hazard_counts.most_common(10):\n",
    "        print(f\"  {ht}: {count:,}\")\n",
    "\n",
    "# Asset category distribution\n",
    "asset_counts = Counter()\n",
    "for cats in df_loss['asset_categories'].dropna():\n",
    "    if isinstance(cats, list):\n",
    "        asset_counts.update(cats)\n",
    "\n",
    "if asset_counts:\n",
    "    print(f\"\\nAsset Category Distribution:\")\n",
    "    for ac, count in asset_counts.most_common():\n",
    "        print(f\"  {ac}: {count:,}\")\n",
    "\n",
    "# Confidence distribution\n",
    "conf = df_loss[df_loss['has_loss']]['overall_confidence']\n",
    "if len(conf) > 0:\n",
    "    print(f\"\\nConfidence Distribution:\")\n",
    "    print(f\"  Mean: {conf.mean():.2f}\")\n",
    "    print(f\"  Median: {conf.median():.2f}\")\n",
    "    print(f\"  High (>=0.7): {(conf >= 0.7).sum():,}\")\n",
    "    print(f\"  Medium (0.5-0.7): {((conf >= 0.5) & (conf < 0.7)).sum():,}\")\n",
    "    print(f\"  Low (<0.5): {(conf < 0.5).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_export_md_14",
   "metadata": {},
   "source": [
    "### 8.9 Loss Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "loss_export_15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted/loss_extraction_results.csv\n",
      "Saved: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted/loss_detected_records.csv (5771 records)\n",
      "\n",
      "Generating RDLS loss block JSONs for 5,771 datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d44cdd0f1f144a9a97021a9385bf35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building loss JSONs:   0%|          | 0/5771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n",
      "  Generated: 821 loss block JSONs\n",
      "  Skipped (no valid block): 4,950\n",
      "  Output: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.9 Export Loss Results and Generate RDLS Loss Block JSONs\n",
    "\"\"\"\n",
    "\n",
    "# Prepare export DataFrame\n",
    "loss_export_df = df_loss[[\n",
    "    'id', 'title', 'organization', 'has_loss', 'loss_count',\n",
    "    'loss_signal_types', 'hazard_types', 'asset_categories', 'overall_confidence'\n",
    "]].copy()\n",
    "\n",
    "# Convert lists to pipe-separated for CSV\n",
    "for col in ['loss_signal_types', 'hazard_types', 'asset_categories']:\n",
    "    loss_export_df[col] = loss_export_df[col].apply(\n",
    "        lambda x: '|'.join(str(v) for v in x) if isinstance(x, list) else ''\n",
    "    )\n",
    "\n",
    "# Save full results\n",
    "loss_output_file = OUTPUT_DIR / 'loss_extraction_results.csv'\n",
    "loss_export_df.to_csv(loss_output_file, index=False)\n",
    "print(f\"Saved: {loss_output_file}\")\n",
    "\n",
    "# Save records with loss signals\n",
    "loss_records = loss_export_df[loss_export_df['has_loss']]\n",
    "loss_detected_file = OUTPUT_DIR / 'loss_detected_records.csv'\n",
    "loss_records.to_csv(loss_detected_file, index=False)\n",
    "print(f\"Saved: {loss_detected_file} ({len(loss_records)} records)\")\n",
    "\n",
    "# --- Generate RDLS loss block JSONs for ALL flagged datasets ---\n",
    "all_loss = df_loss[\n",
    "    df_loss['has_loss'] &\n",
    "    (df_loss['overall_confidence'] >= 0.5)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nGenerating RDLS loss block JSONs for {len(all_loss):,} datasets...\")\n",
    "\n",
    "generated = 0\n",
    "skipped = 0\n",
    "\n",
    "iterator = tqdm(all_loss.iterrows(), total=len(all_loss), desc=\"Building loss JSONs\") if HAS_TQDM else all_loss.iterrows()\n",
    "\n",
    "for idx, row in iterator:\n",
    "    extraction = row['extraction']\n",
    "    loss_block = build_loss_block(extraction, row['id'])\n",
    "\n",
    "    if loss_block:\n",
    "        rdls_record = {\n",
    "            'datasets': [{\n",
    "                'id': f\"rdls_lss-hdx_{row['id'][:8]}\",\n",
    "                'title': row['title'],\n",
    "                'risk_data_type': ['loss'],\n",
    "                'loss': loss_block,\n",
    "                'links': [{\n",
    "                    'href': 'https://docs.riskdatalibrary.org/en/0__3__0/rdls_schema.json',\n",
    "                    'rel': 'describedby'\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        output_path = OUTPUT_DIR / f\"rdls_lss-hdx_{row['id'][:8]}.json\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rdls_record, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        generated += 1\n",
    "    else:\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"\\nDone.\")\n",
    "print(f\"  Generated: {generated:,} loss block JSONs\")\n",
    "print(f\"  Skipped (no valid block): {skipped:,}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "loss_complete_16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss extraction complete: 2026-02-11T18:06:04.028728\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoss extraction complete: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss_footer_17",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "This notebook produces both Vulnerability and Loss extraction results that feed into:\n",
    "\n",
    "1. **Notebook 12**: HEVL Integration â€” merges all four components with general metadata\n",
    "2. **Notebook 13**: Validation QA â€” validates complete RDLS records\n",
    "\n",
    "### CSV Backward Compatibility\n",
    "The CSV outputs are compatible with Notebook 12:\n",
    "- `vulnerability_extraction_results.csv` â€” Vulnerability signals\n",
    "- `vulnerability_detected_records.csv` â€” Records with vulnerability\n",
    "- `loss_extraction_results.csv` â€” Loss signals\n",
    "- `loss_detected_records.csv` â€” Records with loss\n",
    "\n",
    "New columns are additive and will be consumed when Notebook 12 is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b918842-af33-4404-9db7-e29058230ebe",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
