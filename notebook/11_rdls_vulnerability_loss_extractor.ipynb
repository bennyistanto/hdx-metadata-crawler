{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 11: RDLS Vulnerability & Loss Block Extractor\n",
    "\n",
    "**Purpose**: Extract and populate RDLS v0.3 Vulnerability and Loss component blocks from HDX metadata.\n",
    "\n",
    "**Note**: Vulnerability and Loss data are less common in HDX. This notebook focuses on:\n",
    "1. Detecting V/L signals where they exist\n",
    "2. Building partial blocks with available information\n",
    "3. Flagging records that may benefit from manual enrichment\n",
    "\n",
    "**RDLS Vulnerability Block Structure**:\n",
    "```json\n",
    "\"vulnerability\": {\n",
    "  \"functions\": {\n",
    "    \"vulnerability\": [...],\n",
    "    \"fragility\": [...],\n",
    "    \"damage_to_loss\": [...]\n",
    "  },\n",
    "  \"socio_economic\": [...]\n",
    "}\n",
    "```\n",
    "\n",
    "**RDLS Loss Block Structure**:\n",
    "```json\n",
    "\"loss\": {\n",
    "  \"losses\": [{\n",
    "    \"id\": \"...\",\n",
    "    \"hazard_type\": \"...\",\n",
    "    \"asset_category\": \"...\",\n",
    "    \"asset_dimension\": \"...\",\n",
    "    \"impact_and_losses\": {...}\n",
    "  }]\n",
    "}\n",
    "```\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Paths and Configuration\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "DATASET_METADATA_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'dataset_metadata'\n",
    "SIGNAL_DICT_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'config' / 'signal_dictionary.yaml'\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'extracted'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(SIGNAL_DICT_PATH, 'r', encoding='utf-8') as f:\n",
    "    SIGNAL_DICT = yaml.safe_load(f)\n",
    "\n",
    "with open(RDLS_SCHEMA_PATH, 'r', encoding='utf-8') as f:\n",
    "    RDLS_SCHEMA = json.load(f)\n",
    "\n",
    "print(f\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vulnerability & Loss Detection Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Define Detection Patterns\n",
    "\n",
    "Vulnerability and Loss data are specialized. These patterns detect\n",
    "the presence of such data rather than extracting detailed parameters.\n",
    "\"\"\"\n",
    "\n",
    "# Vulnerability function indicators\n",
    "VULNERABILITY_FUNCTION_PATTERNS = {\n",
    "    'vulnerability_curve': [\n",
    "        r'\\b(vulnerability.?curve|vulnerability.?function|damage.?curve)\\b',\n",
    "        r'\\b(mean.?damage.?ratio|mdr)\\b',\n",
    "        r'\\b(damage.?state|ds\\d)\\b',\n",
    "    ],\n",
    "    'fragility_curve': [\n",
    "        r'\\b(fragility.?curve|fragility.?function)\\b',\n",
    "        r'\\b(probability.?of.?damage|failure.?probability)\\b',\n",
    "        r'\\b(capacity.?spectrum|pushover)\\b',\n",
    "    ],\n",
    "    'damage_to_loss': [\n",
    "        r'\\b(damage.?to.?loss|loss.?function|consequence.?function)\\b',\n",
    "        r'\\b(repair.?cost|replacement.?cost)\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Socio-economic vulnerability indicators\n",
    "SOCIOECONOMIC_PATTERNS = [\n",
    "    r'\\b(social.?vulnerability|sovi|svi)\\b',\n",
    "    r'\\b(socio.?economic.?vulnerability|socioeconomic)\\b',\n",
    "    r'\\b(coping.?capacity|adaptive.?capacity)\\b',\n",
    "    r'\\b(resilience.?index|vulnerability.?index)\\b',\n",
    "    r'\\b(livelihood|poverty|deprivation)\\b',\n",
    "]\n",
    "\n",
    "# Loss data indicators\n",
    "LOSS_PATTERNS = {\n",
    "    'economic_loss': [\n",
    "        r'\\b(economic.?loss|financial.?loss|monetary.?loss)\\b',\n",
    "        r'\\b(damage.?cost|loss.?cost|loss.?estimate)\\b',\n",
    "        r'\\b(aal|average.?annual.?loss)\\b',\n",
    "        r'\\b(probable.?maximum.?loss|pml)\\b',\n",
    "    ],\n",
    "    'human_loss': [\n",
    "        r'\\b(casualty|fatality|mortality|death)\\b',\n",
    "        r'\\b(injury|injured|wounded)\\b',\n",
    "        r'\\b(displaced|homeless|evacuated)\\b',\n",
    "        r'\\b(affected.?population|people.?affected)\\b',\n",
    "    ],\n",
    "    'physical_damage': [\n",
    "        r'\\b(building.?damage|structural.?damage)\\b',\n",
    "        r'\\b(damage.?assessment|damage.?survey)\\b',\n",
    "        r'\\b(destroyed|collapsed|damaged)\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Impact type patterns\n",
    "IMPACT_TYPE_PATTERNS = {\n",
    "    'direct': [r'\\b(direct.?loss|direct.?damage|direct.?impact)\\b'],\n",
    "    'indirect': [r'\\b(indirect.?loss|business.?interruption|downtime)\\b'],\n",
    "    'total': [r'\\b(total.?loss|combined.?loss|aggregate)\\b'],\n",
    "}\n",
    "\n",
    "print(\"Vulnerability/Loss patterns defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Data Classes\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class VulnerabilityExtraction:\n",
    "    \"\"\"Vulnerability extraction results.\"\"\"\n",
    "    has_vulnerability_functions: bool = False\n",
    "    function_types: List[str] = field(default_factory=list)\n",
    "    has_socioeconomic: bool = False\n",
    "    socioeconomic_hints: List[str] = field(default_factory=list)\n",
    "    confidence: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class LossExtraction:\n",
    "    \"\"\"Loss extraction results.\"\"\"\n",
    "    has_loss_data: bool = False\n",
    "    loss_types: List[str] = field(default_factory=list)\n",
    "    impact_type: Optional[str] = None\n",
    "    hazard_type: Optional[str] = None\n",
    "    asset_category: Optional[str] = None\n",
    "    confidence: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class VulnLossExtraction:\n",
    "    \"\"\"Combined V/L extraction.\"\"\"\n",
    "    vulnerability: VulnerabilityExtraction = field(default_factory=VulnerabilityExtraction)\n",
    "    loss: LossExtraction = field(default_factory=LossExtraction)\n",
    "    overall_confidence: float = 0.0\n",
    "    \n",
    "    def has_any_signal(self) -> bool:\n",
    "        return (self.vulnerability.has_vulnerability_functions or \n",
    "                self.vulnerability.has_socioeconomic or\n",
    "                self.loss.has_loss_data)\n",
    "\n",
    "print(\"Data classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.3 Vulnerability/Loss Extractor Class\n",
    "\"\"\"\n",
    "\n",
    "class VulnLossExtractor:\n",
    "    \"\"\"\n",
    "    Extracts RDLS Vulnerability and Loss signals from HDX metadata.\n",
    "    \n",
    "    Due to the specialized nature of V/L data, this extractor focuses on:\n",
    "    - Detecting presence of V/L information\n",
    "    - Identifying type of V/L data\n",
    "    - Linking to hazard/exposure context where available\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, signal_dict: Dict[str, Any]):\n",
    "        self.signal_dict = signal_dict\n",
    "        self._compile_patterns()\n",
    "    \n",
    "    def _compile_patterns(self) -> None:\n",
    "        \"\"\"Compile regex patterns.\"\"\"\n",
    "        self.vuln_func_patterns = {}\n",
    "        for func_type, patterns in VULNERABILITY_FUNCTION_PATTERNS.items():\n",
    "            self.vuln_func_patterns[func_type] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in patterns\n",
    "            ]\n",
    "        \n",
    "        self.socioeconomic_patterns = [\n",
    "            re.compile(p, re.IGNORECASE) for p in SOCIOECONOMIC_PATTERNS\n",
    "        ]\n",
    "        \n",
    "        self.loss_patterns = {}\n",
    "        for loss_type, patterns in LOSS_PATTERNS.items():\n",
    "            self.loss_patterns[loss_type] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in patterns\n",
    "            ]\n",
    "        \n",
    "        self.impact_patterns = {}\n",
    "        for impact_type, patterns in IMPACT_TYPE_PATTERNS.items():\n",
    "            self.impact_patterns[impact_type] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in patterns\n",
    "            ]\n",
    "        \n",
    "        # Compile hazard patterns from signal dict for context\n",
    "        self.hazard_patterns = {}\n",
    "        for hazard, config in self.signal_dict.get('hazard_type', {}).items():\n",
    "            self.hazard_patterns[hazard] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in config.get('patterns', [])\n",
    "            ]\n",
    "        \n",
    "        # Compile exposure patterns for context\n",
    "        self.exposure_patterns = {}\n",
    "        for cat, config in self.signal_dict.get('exposure_category', {}).items():\n",
    "            self.exposure_patterns[cat] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in config.get('patterns', [])\n",
    "            ]\n",
    "    \n",
    "    def _get_all_text(self, record: Dict[str, Any]) -> str:\n",
    "        \"\"\"Concatenate all text fields.\"\"\"\n",
    "        parts = [\n",
    "            record.get('title', ''),\n",
    "            record.get('name', ''),\n",
    "            record.get('notes', ''),\n",
    "            ' '.join(record.get('tags', [])),\n",
    "            record.get('methodology_other', '') or '',\n",
    "        ]\n",
    "        for r in record.get('resources', []):\n",
    "            parts.append(r.get('name', ''))\n",
    "            parts.append(r.get('description', ''))\n",
    "        \n",
    "        return ' '.join(filter(None, parts)).lower()\n",
    "    \n",
    "    def _extract_vulnerability(self, text: str) -> VulnerabilityExtraction:\n",
    "        \"\"\"Extract vulnerability signals.\"\"\"\n",
    "        function_types = []\n",
    "        socioeconomic_hints = []\n",
    "        \n",
    "        # Check vulnerability function patterns\n",
    "        for func_type, patterns in self.vuln_func_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    function_types.append(func_type)\n",
    "                    break\n",
    "        \n",
    "        # Check socioeconomic patterns\n",
    "        for p in self.socioeconomic_patterns:\n",
    "            match = p.search(text)\n",
    "            if match:\n",
    "                socioeconomic_hints.append(match.group(0))\n",
    "        \n",
    "        has_functions = len(function_types) > 0\n",
    "        has_socio = len(socioeconomic_hints) > 0\n",
    "        \n",
    "        confidence = 0.0\n",
    "        if has_functions:\n",
    "            confidence = 0.8\n",
    "        elif has_socio:\n",
    "            confidence = 0.6\n",
    "        \n",
    "        return VulnerabilityExtraction(\n",
    "            has_vulnerability_functions=has_functions,\n",
    "            function_types=list(set(function_types)),\n",
    "            has_socioeconomic=has_socio,\n",
    "            socioeconomic_hints=list(set(socioeconomic_hints))[:5],\n",
    "            confidence=confidence\n",
    "        )\n",
    "    \n",
    "    def _extract_loss(self, text: str) -> LossExtraction:\n",
    "        \"\"\"Extract loss signals.\"\"\"\n",
    "        loss_types = []\n",
    "        \n",
    "        for loss_type, patterns in self.loss_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    loss_types.append(loss_type)\n",
    "                    break\n",
    "        \n",
    "        # Detect impact type\n",
    "        impact_type = None\n",
    "        for itype, patterns in self.impact_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    impact_type = itype\n",
    "                    break\n",
    "            if impact_type:\n",
    "                break\n",
    "        \n",
    "        # Detect hazard context\n",
    "        hazard_type = None\n",
    "        for hazard, patterns in self.hazard_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    hazard_type = hazard\n",
    "                    break\n",
    "            if hazard_type:\n",
    "                break\n",
    "        \n",
    "        # Detect asset category context\n",
    "        asset_category = None\n",
    "        for cat, patterns in self.exposure_patterns.items():\n",
    "            for p in patterns:\n",
    "                if p.search(text):\n",
    "                    asset_category = cat\n",
    "                    break\n",
    "            if asset_category:\n",
    "                break\n",
    "        \n",
    "        has_loss = len(loss_types) > 0\n",
    "        confidence = 0.7 if has_loss else 0.0\n",
    "        if has_loss and hazard_type:\n",
    "            confidence = 0.85\n",
    "        \n",
    "        return LossExtraction(\n",
    "            has_loss_data=has_loss,\n",
    "            loss_types=list(set(loss_types)),\n",
    "            impact_type=impact_type or 'direct',\n",
    "            hazard_type=hazard_type,\n",
    "            asset_category=asset_category,\n",
    "            confidence=confidence\n",
    "        )\n",
    "    \n",
    "    def extract(self, record: Dict[str, Any]) -> VulnLossExtraction:\n",
    "        \"\"\"Extract V/L information from record.\"\"\"\n",
    "        text = self._get_all_text(record)\n",
    "        \n",
    "        vuln = self._extract_vulnerability(text)\n",
    "        loss = self._extract_loss(text)\n",
    "        \n",
    "        overall = max(vuln.confidence, loss.confidence)\n",
    "        \n",
    "        return VulnLossExtraction(\n",
    "            vulnerability=vuln,\n",
    "            loss=loss,\n",
    "            overall_confidence=overall\n",
    "        )\n",
    "\n",
    "# Initialize\n",
    "vl_extractor = VulnLossExtractor(SIGNAL_DICT)\n",
    "print(\"VulnLossExtractor initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RDLS Block Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Build RDLS Vulnerability Block\n",
    "\"\"\"\n",
    "\n",
    "def build_vulnerability_block(\n",
    "    extraction: VulnerabilityExtraction,\n",
    "    dataset_id: str\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS vulnerability block.\n",
    "    \n",
    "    Note: Full vulnerability functions require many parameters that\n",
    "    are typically not available in HDX metadata. This builds a\n",
    "    minimal/placeholder structure.\n",
    "    \"\"\"\n",
    "    if not extraction.has_vulnerability_functions and not extraction.has_socioeconomic:\n",
    "        return None\n",
    "    \n",
    "    block = {'functions': {}}\n",
    "    \n",
    "    # Add function type placeholders\n",
    "    if extraction.has_vulnerability_functions:\n",
    "        if 'vulnerability_curve' in extraction.function_types:\n",
    "            block['functions']['vulnerability'] = []\n",
    "        if 'fragility_curve' in extraction.function_types:\n",
    "            block['functions']['fragility'] = []\n",
    "        if 'damage_to_loss' in extraction.function_types:\n",
    "            block['functions']['damage_to_loss'] = []\n",
    "    \n",
    "    # Add socioeconomic if detected\n",
    "    if extraction.has_socioeconomic:\n",
    "        block['socio_economic'] = [{\n",
    "            'id': f\"socio_{dataset_id[:8]}\",\n",
    "            'indicators_detected': extraction.socioeconomic_hints\n",
    "        }]\n",
    "    \n",
    "    return block if block.get('functions') or block.get('socio_economic') else None\n",
    "\n",
    "print(\"Vulnerability block builder defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.2 Build RDLS Loss Block\n",
    "\"\"\"\n",
    "\n",
    "def build_loss_block(\n",
    "    extraction: LossExtraction,\n",
    "    dataset_id: str\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build RDLS loss block.\n",
    "    \"\"\"\n",
    "    if not extraction.has_loss_data:\n",
    "        return None\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i, loss_type in enumerate(extraction.loss_types):\n",
    "        loss_entry = {\n",
    "            'id': f\"loss_{dataset_id[:8]}_{i+1}\",\n",
    "        }\n",
    "        \n",
    "        # Add hazard type if detected\n",
    "        if extraction.hazard_type:\n",
    "            loss_entry['hazard_type'] = extraction.hazard_type\n",
    "        \n",
    "        # Add asset category if detected\n",
    "        if extraction.asset_category:\n",
    "            loss_entry['asset_category'] = extraction.asset_category\n",
    "        else:\n",
    "            # Default based on loss type\n",
    "            if loss_type == 'human_loss':\n",
    "                loss_entry['asset_category'] = 'population'\n",
    "            elif loss_type == 'physical_damage':\n",
    "                loss_entry['asset_category'] = 'buildings'\n",
    "        \n",
    "        # Add asset dimension\n",
    "        if loss_type == 'human_loss':\n",
    "            loss_entry['asset_dimension'] = 'Other'\n",
    "        elif loss_type == 'economic_loss':\n",
    "            loss_entry['asset_dimension'] = 'Structure'\n",
    "        else:\n",
    "            loss_entry['asset_dimension'] = 'Structure'\n",
    "        \n",
    "        # Impact and losses\n",
    "        loss_entry['impact_and_losses'] = {\n",
    "            'impact_type': extraction.impact_type or 'direct',\n",
    "            'loss_type_detected': loss_type\n",
    "        }\n",
    "        \n",
    "        losses.append(loss_entry)\n",
    "    \n",
    "    return {'losses': losses} if losses else None\n",
    "\n",
    "print(\"Loss block builder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Find and Load V/L Candidate Records\n",
    "\"\"\"\n",
    "\n",
    "# Search for files with V/L keywords\n",
    "vl_keywords = ['vulnerability', 'loss', 'damage', 'impact', 'casualty', 'fatality']\n",
    "\n",
    "vl_files = []\n",
    "for kw in vl_keywords:\n",
    "    vl_files.extend(list(DATASET_METADATA_DIR.glob(f'*{kw}*.json'))[:5])\n",
    "\n",
    "# Also check for risk assessment files\n",
    "vl_files.extend(list(DATASET_METADATA_DIR.glob('*risk*.json'))[:10])\n",
    "\n",
    "# Deduplicate\n",
    "vl_files = list(set(vl_files))\n",
    "\n",
    "sample_records = []\n",
    "for filepath in vl_files[:30]:\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            sample_records.append(json.load(f))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Loaded {len(sample_records)} potential V/L records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 Run Extraction\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VULNERABILITY/LOSS EXTRACTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "vl_found = 0\n",
    "\n",
    "for record in sample_records:\n",
    "    extraction = vl_extractor.extract(record)\n",
    "    \n",
    "    if extraction.has_any_signal():\n",
    "        vl_found += 1\n",
    "        print(f\"\\n{'â”€' * 80}\")\n",
    "        print(f\"Title: {record.get('title', '')[:70]}\")\n",
    "        print(f\"Overall Confidence: {extraction.overall_confidence:.2f}\")\n",
    "        \n",
    "        if extraction.vulnerability.has_vulnerability_functions:\n",
    "            print(f\"Vulnerability Functions: {extraction.vulnerability.function_types}\")\n",
    "        \n",
    "        if extraction.vulnerability.has_socioeconomic:\n",
    "            print(f\"Socioeconomic: {extraction.vulnerability.socioeconomic_hints}\")\n",
    "        \n",
    "        if extraction.loss.has_loss_data:\n",
    "            print(f\"Loss Types: {extraction.loss.loss_types}\")\n",
    "            print(f\"  Hazard context: {extraction.loss.hazard_type}\")\n",
    "            print(f\"  Asset category: {extraction.loss.asset_category}\")\n",
    "\n",
    "print(f\"\\n\\nSummary: {vl_found}/{len(sample_records)} records have V/L signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Process Full Corpus\n",
    "\"\"\"\n",
    "\n",
    "def process_vl_extraction(\n",
    "    metadata_dir: Path,\n",
    "    extractor: VulnLossExtractor,\n",
    "    limit: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process records for V/L extraction.\"\"\"\n",
    "    json_files = list(metadata_dir.glob('*.json'))\n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "    \n",
    "    results = []\n",
    "    iterator = tqdm(json_files, desc=\"Extracting V/L\") if HAS_TQDM else json_files\n",
    "    \n",
    "    for filepath in iterator:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                record = json.load(f)\n",
    "            \n",
    "            extraction = extractor.extract(record)\n",
    "            \n",
    "            results.append({\n",
    "                'id': record.get('id'),\n",
    "                'title': record.get('title'),\n",
    "                'organization': record.get('organization'),\n",
    "                'has_vulnerability': extraction.vulnerability.has_vulnerability_functions,\n",
    "                'has_socioeconomic': extraction.vulnerability.has_socioeconomic,\n",
    "                'vuln_function_types': extraction.vulnerability.function_types,\n",
    "                'has_loss': extraction.loss.has_loss_data,\n",
    "                'loss_types': extraction.loss.loss_types,\n",
    "                'loss_hazard': extraction.loss.hazard_type,\n",
    "                'loss_asset': extraction.loss.asset_category,\n",
    "                'overall_confidence': extraction.overall_confidence,\n",
    "                'has_any_vl': extraction.has_any_signal(),\n",
    "                'extraction': extraction\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({'id': filepath.stem, 'error': str(e)})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "PROCESS_LIMIT = 2000  # V/L is rarer, process more\n",
    "\n",
    "print(f\"Processing {PROCESS_LIMIT or 'all'} records...\")\n",
    "df_vl = process_vl_extraction(DATASET_METADATA_DIR, vl_extractor, limit=PROCESS_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Statistics\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VULNERABILITY/LOSS EXTRACTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_vl)\n",
    "\n",
    "print(f\"\\nTotal records: {total:,}\")\n",
    "print(f\"\\nVulnerability:\")\n",
    "print(f\"  With vulnerability functions: {df_vl['has_vulnerability'].sum()} ({df_vl['has_vulnerability'].mean()*100:.2f}%)\")\n",
    "print(f\"  With socioeconomic indicators: {df_vl['has_socioeconomic'].sum()} ({df_vl['has_socioeconomic'].mean()*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nLoss:\")\n",
    "print(f\"  With loss data: {df_vl['has_loss'].sum()} ({df_vl['has_loss'].mean()*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCombined:\")\n",
    "print(f\"  Any V/L signal: {df_vl['has_any_vl'].sum()} ({df_vl['has_any_vl'].mean()*100:.2f}%)\")\n",
    "\n",
    "# Loss type distribution\n",
    "loss_types = Counter()\n",
    "for types in df_vl['loss_types'].dropna():\n",
    "    if isinstance(types, list):\n",
    "        loss_types.update(types)\n",
    "\n",
    "print(f\"\\nLoss Type Distribution:\")\n",
    "for lt, count in loss_types.most_common():\n",
    "    print(f\"  {lt}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Export Results\n",
    "\"\"\"\n",
    "\n",
    "export_df = df_vl[[\n",
    "    'id', 'title', 'organization', 'has_vulnerability', 'has_socioeconomic',\n",
    "    'vuln_function_types', 'has_loss', 'loss_types', 'loss_hazard',\n",
    "    'loss_asset', 'overall_confidence', 'has_any_vl'\n",
    "]].copy()\n",
    "\n",
    "for col in ['vuln_function_types', 'loss_types']:\n",
    "    export_df[col] = export_df[col].apply(\n",
    "        lambda x: '|'.join(x) if isinstance(x, list) else ''\n",
    "    )\n",
    "\n",
    "output_file = OUTPUT_DIR / 'vuln_loss_extraction_results.csv'\n",
    "export_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved: {output_file}\")\n",
    "\n",
    "# Records with V/L signals\n",
    "vl_records = export_df[export_df['has_any_vl']]\n",
    "vl_file = OUTPUT_DIR / 'vuln_loss_detected_records.csv'\n",
    "vl_records.to_csv(vl_file, index=False)\n",
    "print(f\"Saved: {vl_file} ({len(vl_records)} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.2 Generate Sample RDLS Records\n",
    "\"\"\"\n",
    "\n",
    "# Get records with highest confidence V/L signals\n",
    "top_vl = df_vl[\n",
    "    df_vl['has_any_vl'] & \n",
    "    (df_vl['overall_confidence'] >= 0.6)\n",
    "].nlargest(10, 'overall_confidence')\n",
    "\n",
    "print(f\"\\nGenerating {len(top_vl)} sample RDLS V/L records...\")\n",
    "\n",
    "for idx, row in top_vl.iterrows():\n",
    "    extraction = row['extraction']\n",
    "    \n",
    "    rdls_dataset = {\n",
    "        'id': f\"rdls_vln-hdx_{row['id'][:8]}\",\n",
    "        'title': row['title'],\n",
    "        'risk_data_type': [],\n",
    "        'links': [{\n",
    "            'href': 'https://docs.riskdatalibrary.org/en/0__3__0/rdls_schema.json',\n",
    "            'rel': 'describedby'\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    # Add vulnerability block\n",
    "    if extraction.vulnerability.has_vulnerability_functions or extraction.vulnerability.has_socioeconomic:\n",
    "        vuln_block = build_vulnerability_block(extraction.vulnerability, row['id'])\n",
    "        if vuln_block:\n",
    "            rdls_dataset['vulnerability'] = vuln_block\n",
    "            rdls_dataset['risk_data_type'].append('vulnerability')\n",
    "    \n",
    "    # Add loss block\n",
    "    if extraction.loss.has_loss_data:\n",
    "        loss_block = build_loss_block(extraction.loss, row['id'])\n",
    "        if loss_block:\n",
    "            rdls_dataset['loss'] = loss_block\n",
    "            rdls_dataset['risk_data_type'].append('loss')\n",
    "    \n",
    "    if rdls_dataset['risk_data_type']:\n",
    "        rdls_record = {'datasets': [rdls_dataset]}\n",
    "        \n",
    "        output_path = OUTPUT_DIR / f\"rdls_vln-hdx_{row['id'][:8]}.json\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rdls_record, f, indent=2)\n",
    "        \n",
    "        print(f\"  Created: {output_path.name}\")\n",
    "\n",
    "print(f\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
