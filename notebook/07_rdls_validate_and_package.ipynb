{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Step 7 - Validate RDLS JSON and Package for Delivery\n",
    "\n",
    "**Purpose:** Validate RDLS JSON records produced in Step 6, generate QA reports, and package the deliverable bundle.\n",
    "\n",
    "**Process:**\n",
    "1. Load and validate all RDLS JSON records against the schema\n",
    "2. Detect missing required fields and duplicates\n",
    "3. Generate human-readable validation summary\n",
    "4. Package records, index, and reports into a distributable ZIP\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- `rdls/records/*.json` — RDLS records from Step 6\n",
    "- `rdls/index/rdls_index.jsonl` — Index file from Step 6\n",
    "- `rdls/schema/rdls_schema_v0.3.json` — RDLS JSON Schema\n",
    "\n",
    "## Outputs\n",
    "- `rdls/reports/rdls_validation_summary.md` — Human-readable summary\n",
    "- `rdls/reports/rdls_missing_fields.csv` — Records with missing required fields\n",
    "- `rdls/reports/rdls_duplicates.csv` — Duplicate ID and content detection\n",
    "- `rdls/dist/rdls_metadata_bundle.zip` — Deliverable package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm available: True\n",
      "Configuration:\n",
      "  DUMP_DIR: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump\n",
      "  RDLS_DIR: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls\n",
      "  OUTPUT_MODE: in_place\n",
      "  RDLS_RUN_DIR: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls\n",
      "  RECORDS_DIR: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/records\n",
      "  INDEX_JSONL: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/index/rdls_index.jsonl\n",
      "  SCHEMA_JSON: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/schema/rdls_schema_v0.3.json\n",
      "  DIST_DIR: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/dist\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup: Import libraries and configure paths.\n",
    "\n",
    "Configuration Options:\n",
    "    OUTPUT_MODE: 'in_place' or 'run_folder' to match Step 6\n",
    "    RUN_ID: Specific run ID for run_folder mode (None for auto-detect)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import hashlib\n",
    "import json\n",
    "import zipfile\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- tqdm with graceful fallback ---\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "    def tqdm(iterable, **kwargs):\n",
    "        \"\"\"Fallback: return iterable unchanged if tqdm not installed.\"\"\"\n",
    "        return iterable\n",
    "\n",
    "print(f\"tqdm available: {TQDM_AVAILABLE}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for RDLS validation and packaging.\n",
    "    \n",
    "    Attributes:\n",
    "        dump_dir: Root directory for HDX metadata dump\n",
    "        output_mode: 'in_place' or 'run_folder' (must match Step 6)\n",
    "        run_id: Specific run ID for run_folder mode (None for auto-detect)\n",
    "    \"\"\"\n",
    "    dump_dir: Path = field(default_factory=lambda: (Path(\"..\") / \"hdx_dataset_metadata_dump\").resolve())\n",
    "    output_mode: str = \"in_place\"  # \"in_place\" | \"run_folder\"\n",
    "    run_id: Optional[str] = None\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "config = ValidationConfig()\n",
    "\n",
    "# --- Resolve paths ---\n",
    "DUMP_DIR = config.dump_dir\n",
    "RDLS_DIR = (DUMP_DIR / \"rdls\").resolve()\n",
    "\n",
    "\n",
    "def resolve_rdls_run_dir(rdls_dir: Path, output_mode: str, run_id: Optional[str]) -> Path:\n",
    "    \"\"\"\n",
    "    Resolve the RDLS run directory based on output mode.\n",
    "    \n",
    "    Parameters:\n",
    "        rdls_dir: Base RDLS directory\n",
    "        output_mode: 'in_place' or 'run_folder'\n",
    "        run_id: Specific run ID (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Resolved path to run directory\n",
    "    \"\"\"\n",
    "    rdls_dir = rdls_dir.resolve()\n",
    "    \n",
    "    if output_mode == \"in_place\":\n",
    "        return rdls_dir\n",
    "    \n",
    "    if output_mode != \"run_folder\":\n",
    "        raise ValueError(f\"Unknown OUTPUT_MODE: {output_mode}\")\n",
    "    \n",
    "    runs_dir = rdls_dir / \"runs\"\n",
    "    latest_ptr = runs_dir / \"_latest.txt\"\n",
    "    \n",
    "    # 1) Explicit RUN_ID\n",
    "    if run_id:\n",
    "        candidate = (runs_dir / run_id).resolve()\n",
    "        if not candidate.exists():\n",
    "            raise FileNotFoundError(f\"RUN_ID folder not found: {candidate}\")\n",
    "        return candidate\n",
    "    \n",
    "    # 2) _latest.txt pointer\n",
    "    if latest_ptr.exists():\n",
    "        rid = latest_ptr.read_text(encoding=\"utf-8\").strip()\n",
    "        if rid:\n",
    "            candidate = (runs_dir / rid).resolve()\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "    \n",
    "    # 3) Newest run folder fallback\n",
    "    if not runs_dir.exists():\n",
    "        raise FileNotFoundError(f\"Runs folder not found: {runs_dir}\")\n",
    "    \n",
    "    candidates = [p for p in runs_dir.iterdir() if p.is_dir() and not p.name.startswith(\"_\")]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No run folders found under: {runs_dir}. \"\n",
    "            \"Either set OUTPUT_MODE='in_place' or provide RUN_ID.\"\n",
    "        )\n",
    "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return candidates[0].resolve()\n",
    "\n",
    "\n",
    "RDLS_RUN_DIR = resolve_rdls_run_dir(RDLS_DIR, config.output_mode, config.run_id)\n",
    "\n",
    "# Input paths\n",
    "RECORDS_DIR = RDLS_RUN_DIR / \"records\"\n",
    "INDEX_JSONL = RDLS_RUN_DIR / \"index\" / \"rdls_index.jsonl\"\n",
    "SCHEMA_JSON = RDLS_DIR / \"schema\" / \"rdls_schema_v0.3.json\"  # Schema lives at root\n",
    "\n",
    "# Output paths\n",
    "REPORTS_DIR = RDLS_RUN_DIR / \"reports\"\n",
    "DIST_DIR = RDLS_RUN_DIR / \"dist\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  DUMP_DIR: {DUMP_DIR}\")\n",
    "print(f\"  RDLS_DIR: {RDLS_DIR}\")\n",
    "print(f\"  OUTPUT_MODE: {config.output_mode}\")\n",
    "print(f\"  RDLS_RUN_DIR: {RDLS_RUN_DIR}\")\n",
    "print(f\"  RECORDS_DIR: {RECORDS_DIR}\")\n",
    "print(f\"  INDEX_JSONL: {INDEX_JSONL}\")\n",
    "print(f\"  SCHEMA_JSON: {SCHEMA_JSON}\")\n",
    "print(f\"  DIST_DIR: {DIST_DIR}\")\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "CLEANUP_MODE = \"replace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1-setup-clean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 07 Validation Reports]:\n",
      "  schema_validation_full.csv              : 1 files\n",
      "  rdls_missing_fields.csv                 : 1 files\n",
      "  rdls_duplicates.csv                     : 1 files\n",
      "  rdls_nested_field_warnings.csv          : 1 files\n",
      "  rdls_validation_summary.md              : 1 files\n",
      "  Cleaned 5 files. Ready for fresh output.\n",
      "\n",
      "Output cleanup [NB 07 Distribution Bundle]:\n",
      "  rdls_metadata_bundle.zip                : 1 files\n",
      "  Cleaned 1 files. Ready for fresh output.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deleted': 1, 'skipped': False}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Clean Previous Outputs\n",
    "\n",
    "Remove stale output files from previous runs (controlled by CLEANUP_MODE).\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice, defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "# ── Run cleanup ────────────────────────────────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    REPORTS_DIR,\n",
    "    patterns=[\n",
    "        \"schema_validation_full.csv\",\n",
    "        \"rdls_missing_fields.csv\",\n",
    "        \"rdls_duplicates.csv\",\n",
    "        \"rdls_nested_field_warnings.csv\",\n",
    "        \"rdls_validation_summary.md\",\n",
    "    ],\n",
    "    label=\"NB 07 Validation Reports\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n",
    "\n",
    "clean_previous_outputs(\n",
    "    DIST_DIR,\n",
    "    patterns=[\"rdls_metadata_bundle.zip\"],\n",
    "    label=\"NB 07 Distribution Bundle\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Load RDLS Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonschema validation enabled (Draft2020-12)\n",
      "Required fields (schema): ['id', 'title', 'risk_data_type', 'attributions', 'spatial', 'license', 'resources']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load RDLS schema and initialize JSON Schema validator.\n",
    "\"\"\"\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file with UTF-8 encoding.\"\"\"\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "# Load schema\n",
    "if not SCHEMA_JSON.exists():\n",
    "    raise FileNotFoundError(f\"RDLS schema not found: {SCHEMA_JSON}\")\n",
    "\n",
    "rdls_schema = safe_load_json(SCHEMA_JSON)\n",
    "\n",
    "# Initialize JSON Schema validator\n",
    "def try_import_jsonschema():\n",
    "    \"\"\"Try to import jsonschema library.\"\"\"\n",
    "    try:\n",
    "        import jsonschema\n",
    "        return jsonschema\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "_jsonschema = try_import_jsonschema()\n",
    "validator = None\n",
    "\n",
    "if _jsonschema is not None:\n",
    "    try:\n",
    "        validator = _jsonschema.Draft202012Validator(rdls_schema)\n",
    "        print(\"jsonschema validation enabled (Draft2020-12)\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: jsonschema init failed: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: jsonschema not installed; validation will be skipped\")\n",
    "\n",
    "# Required fields from schema\n",
    "REQUIRED_FIELDS = rdls_schema.get(\"required\", [])\n",
    "print(f\"Required fields (schema): {REQUIRED_FIELDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3-validators",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation functions loaded.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validation helper functions.\n",
    "\"\"\"\n",
    "\n",
    "def validate_dataset_obj(dataset_obj: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate RDLS dataset object against schema.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_obj: The RDLS dataset object (not the wrapper)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if validator is None:\n",
    "        return True, \"\"\n",
    "    \n",
    "    errors = sorted(validator.iter_errors(dataset_obj), key=lambda e: e.path)\n",
    "    if not errors:\n",
    "        return True, \"\"\n",
    "    \n",
    "    msgs = [f\"{'.'.join(str(p) for p in e.path)}: {e.message}\" for e in errors[:10]]\n",
    "    return False, \" | \".join(msgs)\n",
    "\n",
    "\n",
    "def sha256_file(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Compute SHA-256 hash of a file.\n",
    "    \n",
    "    Parameters:\n",
    "        path: Path to file\n",
    "        \n",
    "    Returns:\n",
    "        Hex digest of SHA-256 hash\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def iter_record_files(folder: Path) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Get sorted list of JSON record files.\n",
    "    \n",
    "    Parameters:\n",
    "        folder: Directory containing records\n",
    "        \n",
    "    Returns:\n",
    "        Sorted list of JSON file paths\n",
    "    \"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Records folder not found: {folder}\")\n",
    "    return sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "print(\"Validation functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Validate All Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-4-validate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record files found: 13,152\n",
      "\n",
      "Validating records...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895fc157fe534a789c586a964ad2d6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/13152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VALIDATION COMPLETE\n",
      "==================================================\n",
      "Valid: 0\n",
      "Invalid: 13,152\n",
      "Records with missing required fields: 0\n",
      "Duplicates detected: 0\n",
      "Nested field warnings: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validate all RDLS records and detect duplicates.\n",
    "\n",
    "Checks:\n",
    "    - JSON Schema validation\n",
    "    - Missing required fields\n",
    "    - Duplicate IDs\n",
    "    - Duplicate content (hash-based)\n",
    "\"\"\"\n",
    "\n",
    "# Get record files\n",
    "record_files = iter_record_files(RECORDS_DIR)\n",
    "print(f\"Record files found: {len(record_files):,}\")\n",
    "\n",
    "# Tracking structures\n",
    "rows_validation: List[Dict[str, Any]] = []\n",
    "rows_missing: List[Dict[str, Any]] = []\n",
    "rows_duplicates: List[Dict[str, Any]] = []\n",
    "\n",
    "seen_ids: Dict[str, str] = {}      # rdls_id -> filename\n",
    "seen_hash: Dict[str, str] = {}     # sha256 -> filename\n",
    "\n",
    "# Counters\n",
    "valid_ok = 0\n",
    "invalid = 0\n",
    "\n",
    "print(f\"\\nValidating records...\")\n",
    "\n",
    "for fp in tqdm(record_files, desc=\"Validating\"):\n",
    "    # Try to load JSON\n",
    "    try:\n",
    "        rec = safe_load_json(fp)\n",
    "    except Exception as e:\n",
    "        invalid += 1\n",
    "        rows_validation.append({\n",
    "            \"filename\": fp.name,\n",
    "            \"rdls_id\": \"\",\n",
    "            \"valid\": False,\n",
    "            \"message\": f\"json_parse_error: {e}\",\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Extract dataset object (support wrapper or raw)\n",
    "    if isinstance(rec, dict) and \"datasets\" in rec and isinstance(rec[\"datasets\"], list) and rec[\"datasets\"]:\n",
    "        ds = rec[\"datasets\"][0]\n",
    "    else:\n",
    "        ds = rec\n",
    "    \n",
    "    rdls_id = str(ds.get(\"id\", \"\")).strip()\n",
    "    \n",
    "    # Validate against schema\n",
    "    ok, msg = validate_dataset_obj(ds)\n",
    "    rows_validation.append({\n",
    "        \"filename\": fp.name,\n",
    "        \"rdls_id\": rdls_id,\n",
    "        \"valid\": ok,\n",
    "        \"message\": msg,\n",
    "    })\n",
    "    \n",
    "    if ok:\n",
    "        valid_ok += 1\n",
    "    else:\n",
    "        invalid += 1\n",
    "    \n",
    "    # Check missing required fields\n",
    "    missing_fields = []\n",
    "    for k in REQUIRED_FIELDS:\n",
    "        v = ds.get(k, None)\n",
    "        if v is None:\n",
    "            missing_fields.append(k)\n",
    "        elif isinstance(v, str) and not v.strip():\n",
    "            missing_fields.append(k)\n",
    "        elif isinstance(v, (list, dict)) and len(v) == 0:\n",
    "            missing_fields.append(k)\n",
    "    \n",
    "    if missing_fields:\n",
    "        rows_missing.append({\n",
    "            \"filename\": fp.name,\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"missing_fields\": \";\".join(missing_fields),\n",
    "        })\n",
    "    \n",
    "    # Check for duplicate IDs\n",
    "    if rdls_id:\n",
    "        if rdls_id in seen_ids:\n",
    "            rows_duplicates.append({\n",
    "                \"type\": \"duplicate_id\",\n",
    "                \"rdls_id\": rdls_id,\n",
    "                \"filename_a\": seen_ids[rdls_id],\n",
    "                \"filename_b\": fp.name,\n",
    "            })\n",
    "        else:\n",
    "            seen_ids[rdls_id] = fp.name\n",
    "    \n",
    "    # Check for duplicate content (hash-based)\n",
    "    file_hash = sha256_file(fp)\n",
    "    if file_hash in seen_hash:\n",
    "        rows_duplicates.append({\n",
    "            \"type\": \"duplicate_content_hash\",\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"filename_a\": seen_hash[file_hash],\n",
    "            \"filename_b\": fp.name,\n",
    "        })\n",
    "    else:\n",
    "        seen_hash[file_hash] = fp.name\n",
    "\n",
    "# --- Nested required fields validation (M8) ---\n",
    "REQUIRED_ATTRIBUTION_ROLES = {\"publisher\", \"creator\", \"contact_point\"}\n",
    "REQUIRED_RESOURCE_FIELDS = {\"id\", \"title\", \"data_format\", \"access_modality\"}\n",
    "\n",
    "rows_nested_warnings: List[Dict[str, Any]] = []\n",
    "\n",
    "for fp in record_files:\n",
    "    try:\n",
    "        rec = safe_load_json(fp)\n",
    "    except Exception:\n",
    "        continue\n",
    "    if isinstance(rec, dict) and \"datasets\" in rec and isinstance(rec[\"datasets\"], list) and rec[\"datasets\"]:\n",
    "        ds = rec[\"datasets\"][0]\n",
    "    else:\n",
    "        ds = rec\n",
    "    rdls_id = str(ds.get(\"id\", \"\")).strip()\n",
    "\n",
    "    # Check attributions: must have all 3 required roles\n",
    "    attributions = ds.get(\"attributions\", [])\n",
    "    attr_roles = {a.get(\"role\") for a in attributions if isinstance(a, dict)}\n",
    "    missing_roles = REQUIRED_ATTRIBUTION_ROLES - attr_roles\n",
    "    if missing_roles:\n",
    "        rows_nested_warnings.append({\n",
    "            \"filename\": fp.name,\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"check\": \"attribution_roles\",\n",
    "            \"detail\": f\"Missing roles: {sorted(missing_roles)}\",\n",
    "            \"severity\": \"warning\",\n",
    "        })\n",
    "\n",
    "    # Check resources: each must have required sub-fields\n",
    "    resources = ds.get(\"resources\", [])\n",
    "    for ri, r in enumerate(resources):\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        missing_res = {k for k in REQUIRED_RESOURCE_FIELDS if not r.get(k)}\n",
    "        if missing_res:\n",
    "            rows_nested_warnings.append({\n",
    "                \"filename\": fp.name,\n",
    "                \"rdls_id\": rdls_id,\n",
    "                \"check\": f\"resource[{ri}]_fields\",\n",
    "                \"detail\": f\"Missing fields: {sorted(missing_res)}\",\n",
    "                \"severity\": \"warning\",\n",
    "            })\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"VALIDATION COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Valid: {valid_ok:,}\")\n",
    "print(f\"Invalid: {invalid:,}\")\n",
    "print(f\"Records with missing required fields: {len(rows_missing):,}\")\n",
    "print(f\"Duplicates detected: {len(rows_duplicates):,}\")\n",
    "print(f\"Nested field warnings: {len(rows_nested_warnings):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Save Validation Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-5-reports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/schema_validation_full.csv\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/rdls_missing_fields.csv\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/rdls_duplicates.csv\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/rdls_nested_field_warnings.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save validation results to CSV reports.\n",
    "\"\"\"\n",
    "\n",
    "# Create DataFrames\n",
    "df_val = pd.DataFrame(rows_validation)\n",
    "df_missing = pd.DataFrame(rows_missing)\n",
    "df_dups = pd.DataFrame(rows_duplicates)\n",
    "\n",
    "# Output paths\n",
    "OUT_VALIDATION = REPORTS_DIR / \"schema_validation_full.csv\"\n",
    "OUT_MISSING = REPORTS_DIR / \"rdls_missing_fields.csv\"\n",
    "OUT_DUPS = REPORTS_DIR / \"rdls_duplicates.csv\"\n",
    "\n",
    "# Save reports\n",
    "df_val.to_csv(OUT_VALIDATION, index=False)\n",
    "print(f\"Wrote: {OUT_VALIDATION}\")\n",
    "\n",
    "df_missing.to_csv(OUT_MISSING, index=False)\n",
    "print(f\"Wrote: {OUT_MISSING}\")\n",
    "\n",
    "df_dups.to_csv(OUT_DUPS, index=False)\n",
    "print(f\"Wrote: {OUT_DUPS}\")\n",
    "\n",
    "# Save nested field warnings report\n",
    "OUT_NESTED = REPORTS_DIR / \"rdls_nested_field_warnings.csv\"\n",
    "df_nested = pd.DataFrame(rows_nested_warnings)\n",
    "df_nested.to_csv(OUT_NESTED, index=False)\n",
    "print(f\"Wrote: {OUT_NESTED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Generate Human-Readable Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-6-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/rdls_validation_summary.md\n",
      "\n",
      "==================================================\n",
      "VALIDATION SUMMARY\n",
      "==================================================\n",
      "# RDLS Validation Summary\n",
      "- Run timestamp: 2026-02-10T23:30:53.137910+00:00\n",
      "- Records folder: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/records\n",
      "- Total JSON files: 13,152\n",
      "- Schema valid: 0\n",
      "- Schema invalid: 13,152\n",
      "- Records missing required fields: 0\n",
      "- Duplicates detected: 0\n",
      "- Nested field warnings: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate Markdown summary of validation results.\n",
    "\"\"\"\n",
    "\n",
    "summary_lines: List[str] = []\n",
    "summary_lines.append(\"# RDLS Validation Summary\")\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(f\"- **Run timestamp:** {datetime.now(timezone.utc).isoformat()}\")\n",
    "summary_lines.append(f\"- **Records folder:** `{RECORDS_DIR}`\")\n",
    "summary_lines.append(f\"- **Total JSON files:** **{len(record_files):,}**\")\n",
    "summary_lines.append(f\"- **Schema valid:** **{int((df_val['valid'] == True).sum()):,}**\")\n",
    "summary_lines.append(f\"- **Schema invalid:** **{int((df_val['valid'] == False).sum()):,}**\")\n",
    "summary_lines.append(f\"- **Records missing required fields:** **{len(df_missing):,}**\")\n",
    "summary_lines.append(f\"- **Duplicates detected:** **{len(df_dups):,}**\")\n",
    "summary_lines.append(f\"- **Nested field warnings:** **{len(rows_nested_warnings):,}**\")\n",
    "summary_lines.append(\"\")\n",
    "\n",
    "# Missing fields breakdown\n",
    "if not df_missing.empty:\n",
    "    summary_lines.append(\"## Top Missing Required Fields\")\n",
    "    summary_lines.append(\"\")\n",
    "    tmp = df_missing.copy()\n",
    "    tmp[\"missing_fields\"] = tmp[\"missing_fields\"].fillna(\"\").astype(str)\n",
    "    exploded = tmp[\"missing_fields\"].str.split(\";\").explode()\n",
    "    vc = exploded[exploded != \"\"].value_counts().head(20)\n",
    "    for k, v in vc.items():\n",
    "        summary_lines.append(f\"- `{k}`: {int(v):,}\")\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "# Duplicates breakdown\n",
    "if not df_dups.empty:\n",
    "    summary_lines.append(\"## Duplicate Signals\")\n",
    "    summary_lines.append(\"\")\n",
    "    vc = df_dups[\"type\"].value_counts()\n",
    "    for k, v in vc.items():\n",
    "        summary_lines.append(f\"- `{k}`: {int(v):,}\")\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "# Write summary\n",
    "OUT_MD = REPORTS_DIR / \"rdls_validation_summary.md\"\n",
    "OUT_MD.write_text(\"\\n\".join(summary_lines) + \"\\n\", encoding=\"utf-8\")\n",
    "print(f\"Wrote: {OUT_MD}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for line in summary_lines[:15]:\n",
    "    if line.strip():\n",
    "        print(line.replace(\"**\", \"\").replace(\"`\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. Package Deliverable Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-7-package",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating deliverable bundle...\n",
      "  Added 13,152 record files\n",
      "  Added index file\n",
      "  Added 11 report files\n",
      "\n",
      "==================================================\n",
      "PACKAGING COMPLETE\n",
      "==================================================\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/dist/rdls_metadata_bundle.zip\n",
      "Bundle size: 45.19 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create distributable ZIP bundle containing:\n",
    "    - records/*.json\n",
    "    - index/rdls_index.jsonl\n",
    "    - reports/*.csv, *.md\n",
    "\"\"\"\n",
    "\n",
    "OUT_ZIP = DIST_DIR / \"rdls_metadata_bundle.zip\"\n",
    "\n",
    "\n",
    "def add_folder_to_zip(z: zipfile.ZipFile, folder: Path, arc_prefix: str) -> int:\n",
    "    \"\"\"\n",
    "    Add all files from a folder to a ZIP archive.\n",
    "    \n",
    "    Parameters:\n",
    "        z: ZipFile object\n",
    "        folder: Source folder\n",
    "        arc_prefix: Prefix for archive paths\n",
    "        \n",
    "    Returns:\n",
    "        Number of files added\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for p in sorted(folder.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            z.write(p, arcname=str(Path(arc_prefix) / p.relative_to(folder)))\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "print(\"Creating deliverable bundle...\")\n",
    "\n",
    "with zipfile.ZipFile(OUT_ZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    # Add records\n",
    "    records_count = add_folder_to_zip(z, RECORDS_DIR, \"records\")\n",
    "    print(f\"  Added {records_count:,} record files\")\n",
    "    \n",
    "    # Add index\n",
    "    if INDEX_JSONL.exists():\n",
    "        z.write(INDEX_JSONL, arcname=\"index/rdls_index.jsonl\")\n",
    "        print(f\"  Added index file\")\n",
    "    \n",
    "    # Add reports\n",
    "    reports_count = add_folder_to_zip(z, REPORTS_DIR, \"reports\")\n",
    "    print(f\"  Added {reports_count:,} report files\")\n",
    "\n",
    "# Report size\n",
    "zip_size_mb = OUT_ZIP.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"PACKAGING COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Wrote: {OUT_ZIP}\")\n",
    "print(f\"Bundle size: {zip_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Final QA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-8-qa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HDX-RDLS PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Validation Results:\n",
      "  - Total records: 13,152\n",
      "  - Schema valid: 0\n",
      "  - Schema invalid: 13,152\n",
      "  - Missing required fields: 0\n",
      "  - Duplicates: 0\n",
      "  - Nested field warnings: 0\n",
      "\n",
      "Output Files:\n",
      "  - Validation report: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/schema_validation_full.csv\n",
      "  - Missing fields report: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/rdls_missing_fields.csv\n",
      "  - Duplicates report: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/rdls_duplicates.csv\n",
      "  - Summary (Markdown): /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/reports/rdls_validation_summary.md\n",
      "  - Deliverable bundle: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/dist/rdls_metadata_bundle.zip\n",
      "\n",
      "============================================================\n",
      "QUALITY GATE: REVIEW NEEDED\n",
      "  - 13,152 records failed schema validation\n",
      "Review the reports above for details.\n",
      "============================================================\n",
      "\n",
      "Pipeline execution complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Final QA summary and pipeline completion status.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HDX-RDLS PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  - Total records: {len(record_files):,}\")\n",
    "print(f\"  - Schema valid: {valid_ok:,}\")\n",
    "print(f\"  - Schema invalid: {invalid:,}\")\n",
    "print(f\"  - Missing required fields: {len(rows_missing):,}\")\n",
    "print(f\"  - Duplicates: {len(rows_duplicates):,}\")\n",
    "print(f\"  - Nested field warnings: {len(rows_nested_warnings):,}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - Validation report: {OUT_VALIDATION}\")\n",
    "print(f\"  - Missing fields report: {OUT_MISSING}\")\n",
    "print(f\"  - Duplicates report: {OUT_DUPS}\")\n",
    "print(f\"  - Summary (Markdown): {OUT_MD}\")\n",
    "print(f\"  - Deliverable bundle: {OUT_ZIP}\")\n",
    "\n",
    "# Quality gate\n",
    "if invalid == 0 and len(rows_missing) == 0 and len(rows_nested_warnings) == 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QUALITY GATE: PASSED\")\n",
    "    print(f\"All {valid_ok:,} records are schema-valid with no missing required fields or nested field warnings.\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QUALITY GATE: REVIEW NEEDED\")\n",
    "    if invalid > 0:\n",
    "        print(f\"  - {invalid:,} records failed schema validation\")\n",
    "    if len(rows_missing) > 0:\n",
    "        print(f\"  - {len(rows_missing):,} records have missing required fields\")\n",
    "    if len(rows_nested_warnings) > 0:\n",
    "        print(f\"  - {len(rows_nested_warnings):,} records have nested field warnings (attribution roles / resource sub-fields)\")\n",
    "    print(\"Review the reports above for details.\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nPipeline execution complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52dbf58-619b-48dc-840b-bc7b2a248163",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
