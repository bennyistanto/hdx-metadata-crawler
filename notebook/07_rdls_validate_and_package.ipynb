{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Step 7 - Validate RDLS JSON and Package for Delivery\n",
    "\n",
    "**Purpose:** Validate RDLS JSON records produced in Step 6, generate QA reports, and package the deliverable bundle.\n",
    "\n",
    "**Process:**\n",
    "1. Load and validate all RDLS JSON records against the schema\n",
    "2. Detect missing required fields and duplicates\n",
    "3. Generate human-readable validation summary\n",
    "4. Package records, index, and reports into a distributable ZIP\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- `rdls/records/*.json` — RDLS records from Step 6\n",
    "- `rdls/index/rdls_index.jsonl` — Index file from Step 6\n",
    "- `rdls/schema/rdls_schema_v0.3.json` — RDLS JSON Schema\n",
    "\n",
    "## Outputs\n",
    "- `rdls/reports/rdls_validation_summary.md` — Human-readable summary\n",
    "- `rdls/reports/rdls_missing_fields.csv` — Records with missing required fields\n",
    "- `rdls/reports/rdls_duplicates.csv` — Duplicate ID and content detection\n",
    "- `rdls/dist/rdls_metadata_bundle.zip` — Deliverable package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup: Import libraries and configure paths.\n",
    "\n",
    "Configuration Options:\n",
    "    OUTPUT_MODE: 'in_place' or 'run_folder' to match Step 6\n",
    "    RUN_ID: Specific run ID for run_folder mode (None for auto-detect)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import hashlib\n",
    "import json\n",
    "import zipfile\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- tqdm with graceful fallback ---\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    TQDM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TQDM_AVAILABLE = False\n",
    "    def tqdm(iterable, **kwargs):\n",
    "        \"\"\"Fallback: return iterable unchanged if tqdm not installed.\"\"\"\n",
    "        return iterable\n",
    "\n",
    "print(f\"tqdm available: {TQDM_AVAILABLE}\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationConfig:\n",
    "    \"\"\"\n",
    "    Configuration for RDLS validation and packaging.\n",
    "    \n",
    "    Attributes:\n",
    "        dump_dir: Root directory for HDX metadata dump\n",
    "        output_mode: 'in_place' or 'run_folder' (must match Step 6)\n",
    "        run_id: Specific run ID for run_folder mode (None for auto-detect)\n",
    "    \"\"\"\n",
    "    dump_dir: Path = field(default_factory=lambda: (Path(\"..\") / \"hdx_dataset_metadata_dump\").resolve())\n",
    "    output_mode: str = \"in_place\"  # \"in_place\" | \"run_folder\"\n",
    "    run_id: Optional[str] = None\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "config = ValidationConfig()\n",
    "\n",
    "# --- Resolve paths ---\n",
    "DUMP_DIR = config.dump_dir\n",
    "RDLS_DIR = (DUMP_DIR / \"rdls\").resolve()\n",
    "\n",
    "\n",
    "def resolve_rdls_run_dir(rdls_dir: Path, output_mode: str, run_id: Optional[str]) -> Path:\n",
    "    \"\"\"\n",
    "    Resolve the RDLS run directory based on output mode.\n",
    "    \n",
    "    Parameters:\n",
    "        rdls_dir: Base RDLS directory\n",
    "        output_mode: 'in_place' or 'run_folder'\n",
    "        run_id: Specific run ID (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Resolved path to run directory\n",
    "    \"\"\"\n",
    "    rdls_dir = rdls_dir.resolve()\n",
    "    \n",
    "    if output_mode == \"in_place\":\n",
    "        return rdls_dir\n",
    "    \n",
    "    if output_mode != \"run_folder\":\n",
    "        raise ValueError(f\"Unknown OUTPUT_MODE: {output_mode}\")\n",
    "    \n",
    "    runs_dir = rdls_dir / \"runs\"\n",
    "    latest_ptr = runs_dir / \"_latest.txt\"\n",
    "    \n",
    "    # 1) Explicit RUN_ID\n",
    "    if run_id:\n",
    "        candidate = (runs_dir / run_id).resolve()\n",
    "        if not candidate.exists():\n",
    "            raise FileNotFoundError(f\"RUN_ID folder not found: {candidate}\")\n",
    "        return candidate\n",
    "    \n",
    "    # 2) _latest.txt pointer\n",
    "    if latest_ptr.exists():\n",
    "        rid = latest_ptr.read_text(encoding=\"utf-8\").strip()\n",
    "        if rid:\n",
    "            candidate = (runs_dir / rid).resolve()\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "    \n",
    "    # 3) Newest run folder fallback\n",
    "    if not runs_dir.exists():\n",
    "        raise FileNotFoundError(f\"Runs folder not found: {runs_dir}\")\n",
    "    \n",
    "    candidates = [p for p in runs_dir.iterdir() if p.is_dir() and not p.name.startswith(\"_\")]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No run folders found under: {runs_dir}. \"\n",
    "            \"Either set OUTPUT_MODE='in_place' or provide RUN_ID.\"\n",
    "        )\n",
    "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return candidates[0].resolve()\n",
    "\n",
    "\n",
    "RDLS_RUN_DIR = resolve_rdls_run_dir(RDLS_DIR, config.output_mode, config.run_id)\n",
    "\n",
    "# Input paths\n",
    "RECORDS_DIR = RDLS_RUN_DIR / \"records\"\n",
    "INDEX_JSONL = RDLS_RUN_DIR / \"index\" / \"rdls_index.jsonl\"\n",
    "SCHEMA_JSON = RDLS_DIR / \"schema\" / \"rdls_schema_v0.3.json\"  # Schema lives at root\n",
    "\n",
    "# Output paths\n",
    "REPORTS_DIR = RDLS_RUN_DIR / \"reports\"\n",
    "DIST_DIR = RDLS_RUN_DIR / \"dist\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  DUMP_DIR: {DUMP_DIR}\")\n",
    "print(f\"  RDLS_DIR: {RDLS_DIR}\")\n",
    "print(f\"  OUTPUT_MODE: {config.output_mode}\")\n",
    "print(f\"  RDLS_RUN_DIR: {RDLS_RUN_DIR}\")\n",
    "print(f\"  RECORDS_DIR: {RECORDS_DIR}\")\n",
    "print(f\"  INDEX_JSONL: {INDEX_JSONL}\")\n",
    "print(f\"  SCHEMA_JSON: {SCHEMA_JSON}\")\n",
    "print(f\"  DIST_DIR: {DIST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "## 2. Load RDLS Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load RDLS schema and initialize JSON Schema validator.\n",
    "\"\"\"\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file with UTF-8 encoding.\"\"\"\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "# Load schema\n",
    "if not SCHEMA_JSON.exists():\n",
    "    raise FileNotFoundError(f\"RDLS schema not found: {SCHEMA_JSON}\")\n",
    "\n",
    "rdls_schema = safe_load_json(SCHEMA_JSON)\n",
    "\n",
    "# Initialize JSON Schema validator\n",
    "def try_import_jsonschema():\n",
    "    \"\"\"Try to import jsonschema library.\"\"\"\n",
    "    try:\n",
    "        import jsonschema\n",
    "        return jsonschema\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "_jsonschema = try_import_jsonschema()\n",
    "validator = None\n",
    "\n",
    "if _jsonschema is not None:\n",
    "    try:\n",
    "        validator = _jsonschema.Draft202012Validator(rdls_schema)\n",
    "        print(\"jsonschema validation enabled (Draft2020-12)\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: jsonschema init failed: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: jsonschema not installed; validation will be skipped\")\n",
    "\n",
    "# Required fields from schema\n",
    "REQUIRED_FIELDS = rdls_schema.get(\"required\", [])\n",
    "print(f\"Required fields (schema): {REQUIRED_FIELDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "## 3. Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-validators",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation helper functions.\n",
    "\"\"\"\n",
    "\n",
    "def validate_dataset_obj(dataset_obj: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate RDLS dataset object against schema.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_obj: The RDLS dataset object (not the wrapper)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if validator is None:\n",
    "        return True, \"\"\n",
    "    \n",
    "    errors = sorted(validator.iter_errors(dataset_obj), key=lambda e: e.path)\n",
    "    if not errors:\n",
    "        return True, \"\"\n",
    "    \n",
    "    msgs = [f\"{'.'.join(str(p) for p in e.path)}: {e.message}\" for e in errors[:10]]\n",
    "    return False, \" | \".join(msgs)\n",
    "\n",
    "\n",
    "def sha256_file(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Compute SHA-256 hash of a file.\n",
    "    \n",
    "    Parameters:\n",
    "        path: Path to file\n",
    "        \n",
    "    Returns:\n",
    "        Hex digest of SHA-256 hash\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def iter_record_files(folder: Path) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Get sorted list of JSON record files.\n",
    "    \n",
    "    Parameters:\n",
    "        folder: Directory containing records\n",
    "        \n",
    "    Returns:\n",
    "        Sorted list of JSON file paths\n",
    "    \"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Records folder not found: {folder}\")\n",
    "    return sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "print(\"Validation functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "## 4. Validate All Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-validate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate all RDLS records and detect duplicates.\n",
    "\n",
    "Checks:\n",
    "    - JSON Schema validation\n",
    "    - Missing required fields\n",
    "    - Duplicate IDs\n",
    "    - Duplicate content (hash-based)\n",
    "\"\"\"\n",
    "\n",
    "# Get record files\n",
    "record_files = iter_record_files(RECORDS_DIR)\n",
    "print(f\"Record files found: {len(record_files):,}\")\n",
    "\n",
    "# Tracking structures\n",
    "rows_validation: List[Dict[str, Any]] = []\n",
    "rows_missing: List[Dict[str, Any]] = []\n",
    "rows_duplicates: List[Dict[str, Any]] = []\n",
    "\n",
    "seen_ids: Dict[str, str] = {}      # rdls_id -> filename\n",
    "seen_hash: Dict[str, str] = {}     # sha256 -> filename\n",
    "\n",
    "# Counters\n",
    "valid_ok = 0\n",
    "invalid = 0\n",
    "\n",
    "print(f\"\\nValidating records...\")\n",
    "\n",
    "for fp in tqdm(record_files, desc=\"Validating\"):\n",
    "    # Try to load JSON\n",
    "    try:\n",
    "        rec = safe_load_json(fp)\n",
    "    except Exception as e:\n",
    "        invalid += 1\n",
    "        rows_validation.append({\n",
    "            \"filename\": fp.name,\n",
    "            \"rdls_id\": \"\",\n",
    "            \"valid\": False,\n",
    "            \"message\": f\"json_parse_error: {e}\",\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Extract dataset object (support wrapper or raw)\n",
    "    if isinstance(rec, dict) and \"datasets\" in rec and isinstance(rec[\"datasets\"], list) and rec[\"datasets\"]:\n",
    "        ds = rec[\"datasets\"][0]\n",
    "    else:\n",
    "        ds = rec\n",
    "    \n",
    "    rdls_id = str(ds.get(\"id\", \"\")).strip()\n",
    "    \n",
    "    # Validate against schema\n",
    "    ok, msg = validate_dataset_obj(ds)\n",
    "    rows_validation.append({\n",
    "        \"filename\": fp.name,\n",
    "        \"rdls_id\": rdls_id,\n",
    "        \"valid\": ok,\n",
    "        \"message\": msg,\n",
    "    })\n",
    "    \n",
    "    if ok:\n",
    "        valid_ok += 1\n",
    "    else:\n",
    "        invalid += 1\n",
    "    \n",
    "    # Check missing required fields\n",
    "    missing_fields = []\n",
    "    for k in REQUIRED_FIELDS:\n",
    "        v = ds.get(k, None)\n",
    "        if v is None:\n",
    "            missing_fields.append(k)\n",
    "        elif isinstance(v, str) and not v.strip():\n",
    "            missing_fields.append(k)\n",
    "        elif isinstance(v, (list, dict)) and len(v) == 0:\n",
    "            missing_fields.append(k)\n",
    "    \n",
    "    if missing_fields:\n",
    "        rows_missing.append({\n",
    "            \"filename\": fp.name,\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"missing_fields\": \";\".join(missing_fields),\n",
    "        })\n",
    "    \n",
    "    # Check for duplicate IDs\n",
    "    if rdls_id:\n",
    "        if rdls_id in seen_ids:\n",
    "            rows_duplicates.append({\n",
    "                \"type\": \"duplicate_id\",\n",
    "                \"rdls_id\": rdls_id,\n",
    "                \"filename_a\": seen_ids[rdls_id],\n",
    "                \"filename_b\": fp.name,\n",
    "            })\n",
    "        else:\n",
    "            seen_ids[rdls_id] = fp.name\n",
    "    \n",
    "    # Check for duplicate content (hash-based)\n",
    "    file_hash = sha256_file(fp)\n",
    "    if file_hash in seen_hash:\n",
    "        rows_duplicates.append({\n",
    "            \"type\": \"duplicate_content_hash\",\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"filename_a\": seen_hash[file_hash],\n",
    "            \"filename_b\": fp.name,\n",
    "        })\n",
    "    else:\n",
    "        seen_hash[file_hash] = fp.name\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"VALIDATION COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Valid: {valid_ok:,}\")\n",
    "print(f\"Invalid: {invalid:,}\")\n",
    "print(f\"Records with missing required fields: {len(rows_missing):,}\")\n",
    "print(f\"Duplicates detected: {len(rows_duplicates):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-header",
   "metadata": {},
   "source": [
    "## 5. Save Validation Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-reports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save validation results to CSV reports.\n",
    "\"\"\"\n",
    "\n",
    "# Create DataFrames\n",
    "df_val = pd.DataFrame(rows_validation)\n",
    "df_missing = pd.DataFrame(rows_missing)\n",
    "df_dups = pd.DataFrame(rows_duplicates)\n",
    "\n",
    "# Output paths\n",
    "OUT_VALIDATION = REPORTS_DIR / \"schema_validation_full.csv\"\n",
    "OUT_MISSING = REPORTS_DIR / \"rdls_missing_fields.csv\"\n",
    "OUT_DUPS = REPORTS_DIR / \"rdls_duplicates.csv\"\n",
    "\n",
    "# Save reports\n",
    "df_val.to_csv(OUT_VALIDATION, index=False)\n",
    "print(f\"Wrote: {OUT_VALIDATION}\")\n",
    "\n",
    "df_missing.to_csv(OUT_MISSING, index=False)\n",
    "print(f\"Wrote: {OUT_MISSING}\")\n",
    "\n",
    "df_dups.to_csv(OUT_DUPS, index=False)\n",
    "print(f\"Wrote: {OUT_DUPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6-header",
   "metadata": {},
   "source": [
    "## 6. Generate Human-Readable Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate Markdown summary of validation results.\n",
    "\"\"\"\n",
    "\n",
    "summary_lines: List[str] = []\n",
    "summary_lines.append(\"# RDLS Validation Summary\")\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(f\"- **Run timestamp:** {datetime.now(timezone.utc).isoformat()}\")\n",
    "summary_lines.append(f\"- **Records folder:** `{RECORDS_DIR}`\")\n",
    "summary_lines.append(f\"- **Total JSON files:** **{len(record_files):,}**\")\n",
    "summary_lines.append(f\"- **Schema valid:** **{int((df_val['valid'] == True).sum()):,}**\")\n",
    "summary_lines.append(f\"- **Schema invalid:** **{int((df_val['valid'] == False).sum()):,}**\")\n",
    "summary_lines.append(f\"- **Records missing required fields:** **{len(df_missing):,}**\")\n",
    "summary_lines.append(f\"- **Duplicates detected:** **{len(df_dups):,}**\")\n",
    "summary_lines.append(\"\")\n",
    "\n",
    "# Missing fields breakdown\n",
    "if not df_missing.empty:\n",
    "    summary_lines.append(\"## Top Missing Required Fields\")\n",
    "    summary_lines.append(\"\")\n",
    "    tmp = df_missing.copy()\n",
    "    tmp[\"missing_fields\"] = tmp[\"missing_fields\"].fillna(\"\").astype(str)\n",
    "    exploded = tmp[\"missing_fields\"].str.split(\";\").explode()\n",
    "    vc = exploded[exploded != \"\"].value_counts().head(20)\n",
    "    for k, v in vc.items():\n",
    "        summary_lines.append(f\"- `{k}`: {int(v):,}\")\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "# Duplicates breakdown\n",
    "if not df_dups.empty:\n",
    "    summary_lines.append(\"## Duplicate Signals\")\n",
    "    summary_lines.append(\"\")\n",
    "    vc = df_dups[\"type\"].value_counts()\n",
    "    for k, v in vc.items():\n",
    "        summary_lines.append(f\"- `{k}`: {int(v):,}\")\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "# Write summary\n",
    "OUT_MD = REPORTS_DIR / \"rdls_validation_summary.md\"\n",
    "OUT_MD.write_text(\"\\n\".join(summary_lines) + \"\\n\", encoding=\"utf-8\")\n",
    "print(f\"Wrote: {OUT_MD}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for line in summary_lines[:15]:\n",
    "    if line.strip():\n",
    "        print(line.replace(\"**\", \"\").replace(\"`\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-header",
   "metadata": {},
   "source": [
    "## 7. Package Deliverable Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create distributable ZIP bundle containing:\n",
    "    - records/*.json\n",
    "    - index/rdls_index.jsonl\n",
    "    - reports/*.csv, *.md\n",
    "\"\"\"\n",
    "\n",
    "OUT_ZIP = DIST_DIR / \"rdls_metadata_bundle.zip\"\n",
    "\n",
    "\n",
    "def add_folder_to_zip(z: zipfile.ZipFile, folder: Path, arc_prefix: str) -> int:\n",
    "    \"\"\"\n",
    "    Add all files from a folder to a ZIP archive.\n",
    "    \n",
    "    Parameters:\n",
    "        z: ZipFile object\n",
    "        folder: Source folder\n",
    "        arc_prefix: Prefix for archive paths\n",
    "        \n",
    "    Returns:\n",
    "        Number of files added\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for p in sorted(folder.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            z.write(p, arcname=str(Path(arc_prefix) / p.relative_to(folder)))\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "print(\"Creating deliverable bundle...\")\n",
    "\n",
    "with zipfile.ZipFile(OUT_ZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    # Add records\n",
    "    records_count = add_folder_to_zip(z, RECORDS_DIR, \"records\")\n",
    "    print(f\"  Added {records_count:,} record files\")\n",
    "    \n",
    "    # Add index\n",
    "    if INDEX_JSONL.exists():\n",
    "        z.write(INDEX_JSONL, arcname=\"index/rdls_index.jsonl\")\n",
    "        print(f\"  Added index file\")\n",
    "    \n",
    "    # Add reports\n",
    "    reports_count = add_folder_to_zip(z, REPORTS_DIR, \"reports\")\n",
    "    print(f\"  Added {reports_count:,} report files\")\n",
    "\n",
    "# Report size\n",
    "zip_size_mb = OUT_ZIP.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"PACKAGING COMPLETE\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Wrote: {OUT_ZIP}\")\n",
    "print(f\"Bundle size: {zip_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8-header",
   "metadata": {},
   "source": [
    "## 8. Final QA Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8-qa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Final QA summary and pipeline completion status.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HDX-RDLS PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  - Total records: {len(record_files):,}\")\n",
    "print(f\"  - Schema valid: {valid_ok:,}\")\n",
    "print(f\"  - Schema invalid: {invalid:,}\")\n",
    "print(f\"  - Missing required fields: {len(rows_missing):,}\")\n",
    "print(f\"  - Duplicates: {len(rows_duplicates):,}\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  - Validation report: {OUT_VALIDATION}\")\n",
    "print(f\"  - Missing fields report: {OUT_MISSING}\")\n",
    "print(f\"  - Duplicates report: {OUT_DUPS}\")\n",
    "print(f\"  - Summary (Markdown): {OUT_MD}\")\n",
    "print(f\"  - Deliverable bundle: {OUT_ZIP}\")\n",
    "\n",
    "# Quality gate\n",
    "if invalid == 0 and len(rows_missing) == 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QUALITY GATE: PASSED\")\n",
    "    print(f\"All {valid_ok:,} records are schema-valid with no missing required fields.\")\n",
    "    print(f\"{'='*60}\")\n",
    "else:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"QUALITY GATE: REVIEW NEEDED\")\n",
    "    if invalid > 0:\n",
    "        print(f\"  - {invalid:,} records failed schema validation\")\n",
    "    if len(rows_missing) > 0:\n",
    "        print(f\"  - {len(rows_missing):,} records have missing required fields\")\n",
    "    print(\"Review the reports above for details.\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nPipeline execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
