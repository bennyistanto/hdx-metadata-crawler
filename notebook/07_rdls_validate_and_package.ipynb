{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a25e6e",
   "metadata": {},
   "source": [
    "# 07 â€” Validate RDLS JSON and package for delivery\n",
    "\n",
    "This notebook validates the RDLS JSON records produced in **Step 06**, produces QA reports, and packages the deliverable bundle.\n",
    "\n",
    "**Outputs**\n",
    "- `rdls/reports/rdls_validation_summary.md`\n",
    "- `rdls/reports/rdls_missing_fields.csv`\n",
    "- `rdls/reports/rdls_duplicates.csv`\n",
    "- `rdls/dist/rdls_metadata_bundle.zip`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23aefdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMP_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\n",
      "RDLS_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\n",
      "OUTPUT_MODE: in_place\n",
      "RDLS_RUN_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\n",
      "RECORDS_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\records\n",
      "INDEX_JSONL: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\index\\rdls_index.jsonl\n",
      "SCHEMA_JSON: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\schema\\rdls_schema_v0.3.json\n",
      "DIST_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\dist\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Config (edit if needed)\n",
    "# ======================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import hashlib\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "# If you run from /notebooks, keep the default:\n",
    "DUMP_DIR = Path(\"../hdx_dataset_metadata_dump\").resolve()\n",
    "\n",
    "# RDLS root (created in Notebook 06)\n",
    "RDLS_DIR = (DUMP_DIR / \"rdls\").resolve()\n",
    "\n",
    "# -------------------------\n",
    "# Output mode compatibility\n",
    "# -------------------------\n",
    "# - \"in_place\"  : validate/package rdls/{records,index,reports}\n",
    "# - \"run_folder\": validate/package rdls/runs/<RUN_ID>/{records,index,reports}\n",
    "OUTPUT_MODE = \"in_place\"  # \"in_place\" | \"run_folder\"\n",
    "\n",
    "# Only used when OUTPUT_MODE=\"run_folder\".\n",
    "# If None/blank, the notebook will try:\n",
    "#  1) rdls/runs/_latest.txt  (if present)\n",
    "#  2) most recently modified directory under rdls/runs/\n",
    "RUN_ID: Optional[str] = None\n",
    "\n",
    "def resolve_rdls_run_dir(rdls_dir: Path, output_mode: str, run_id: Optional[str]) -> Path:\n",
    "    rdls_dir = rdls_dir.resolve()\n",
    "    if output_mode == \"in_place\":\n",
    "        return rdls_dir\n",
    "\n",
    "    if output_mode != \"run_folder\":\n",
    "        raise ValueError(f\"Unknown OUTPUT_MODE: {output_mode}\")\n",
    "\n",
    "    runs_dir = rdls_dir / \"runs\"\n",
    "    latest_ptr = runs_dir / \"_latest.txt\"\n",
    "\n",
    "    # 1) explicit RUN_ID\n",
    "    if run_id:\n",
    "        candidate = (runs_dir / run_id).resolve()\n",
    "        if not candidate.exists():\n",
    "            raise FileNotFoundError(f\"RUN_ID folder not found: {candidate}\")\n",
    "        return candidate\n",
    "\n",
    "    # 2) _latest.txt pointer\n",
    "    if latest_ptr.exists():\n",
    "        rid = latest_ptr.read_text(encoding=\"utf-8\").strip()\n",
    "        if rid:\n",
    "            candidate = (runs_dir / rid).resolve()\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "\n",
    "    # 3) newest run folder fallback\n",
    "    if not runs_dir.exists():\n",
    "        raise FileNotFoundError(f\"Runs folder not found: {runs_dir}\")\n",
    "\n",
    "    candidates = [p for p in runs_dir.iterdir() if p.is_dir() and not p.name.startswith(\"_\")]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No run folders found under: {runs_dir}. \"\n",
    "            \"Either set OUTPUT_MODE='in_place' or provide RUN_ID.\"\n",
    "        )\n",
    "    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return candidates[0].resolve()\n",
    "\n",
    "RDLS_RUN_DIR = resolve_rdls_run_dir(RDLS_DIR, OUTPUT_MODE, RUN_ID)\n",
    "\n",
    "# Inputs\n",
    "RECORDS_DIR = RDLS_RUN_DIR / \"records\"\n",
    "INDEX_JSONL = RDLS_RUN_DIR / \"index\" / \"rdls_index.jsonl\"\n",
    "SCHEMA_JSON = RDLS_DIR / \"schema\" / \"rdls_schema_v0.3.json\"  # schema lives at root\n",
    "\n",
    "# Outputs (package + reports are written within the selected run dir)\n",
    "REPORTS_DIR = RDLS_RUN_DIR / \"reports\"\n",
    "DIST_DIR = RDLS_RUN_DIR / \"dist\"\n",
    "\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DUMP_DIR:\", DUMP_DIR)\n",
    "print(\"RDLS_DIR:\", RDLS_DIR)\n",
    "print(\"OUTPUT_MODE:\", OUTPUT_MODE)\n",
    "print(\"RDLS_RUN_DIR:\", RDLS_RUN_DIR)\n",
    "print(\"RECORDS_DIR:\", RECORDS_DIR)\n",
    "print(\"INDEX_JSONL:\", INDEX_JSONL)\n",
    "print(\"SCHEMA_JSON:\", SCHEMA_JSON)\n",
    "print(\"DIST_DIR:\", DIST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4daea790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jsonschema validation enabled (Draft2020-12).\n",
      "Required fields (schema): ['id', 'title', 'risk_data_type', 'attributions', 'spatial', 'license', 'resources']\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Load schema + optional jsonschema validator\n",
    "# ======================\n",
    "import pandas as pd\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "rdls_schema = safe_load_json(SCHEMA_JSON)\n",
    "\n",
    "def try_import_jsonschema():\n",
    "    try:\n",
    "        import jsonschema  # type: ignore\n",
    "        return jsonschema\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "_jsonschema = try_import_jsonschema()\n",
    "validator = None\n",
    "if _jsonschema is not None:\n",
    "    try:\n",
    "        validator = _jsonschema.Draft202012Validator(rdls_schema)  # type: ignore\n",
    "        print(\"jsonschema validation enabled (Draft2020-12).\")\n",
    "    except Exception as e:\n",
    "        print(\"WARNING: jsonschema available but validator init failed:\", e)\n",
    "        validator = None\n",
    "else:\n",
    "    print(\"WARNING: jsonschema not installed; schema validation will be skipped.\")\n",
    "\n",
    "def validate_dataset_obj(dataset_obj: Dict[str, Any]) -> Tuple[bool, str]:\n",
    "    \"\"\"Validate the RDLS *dataset object* (not the outer wrapper).\"\"\"\n",
    "    if validator is None:\n",
    "        return True, \"\"\n",
    "    errors = sorted(validator.iter_errors(dataset_obj), key=lambda e: e.path)\n",
    "    if not errors:\n",
    "        return True, \"\"\n",
    "    msgs = []\n",
    "    for e in errors[:10]:\n",
    "        path = \".\".join([str(p) for p in e.path])\n",
    "        msgs.append(f\"{path}: {e.message}\")\n",
    "    return False, \" | \".join(msgs)\n",
    "\n",
    "# Determine required fields from schema\n",
    "REQUIRED_FIELDS = rdls_schema.get(\"required\", [])\n",
    "print(\"Required fields (schema):\", REQUIRED_FIELDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e77074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record files: 50\n",
      "Valid: 50 Invalid: 0\n",
      "Files with missing required fields: 0\n",
      "Duplicates found: 0\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\\schema_validation_full.csv\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\\rdls_missing_fields.csv\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\\rdls_duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Validate all records + compute duplicates\n",
    "# ======================\n",
    "import zipfile\n",
    "\n",
    "def iter_record_files(folder: Path) -> List[Path]:\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Records folder not found: {folder}\")\n",
    "    return sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "def sha256_file(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "record_files = iter_record_files(RECORDS_DIR)\n",
    "print(\"Record files:\", len(record_files))\n",
    "\n",
    "rows_validation: List[Dict[str, Any]] = []\n",
    "rows_missing: List[Dict[str, Any]] = []\n",
    "rows_duplicates: List[Dict[str, Any]] = []\n",
    "\n",
    "seen_ids: Dict[str, str] = {}         # rdls id -> filename\n",
    "seen_hash: Dict[str, str] = {}        # sha256 -> filename\n",
    "\n",
    "valid_ok = 0\n",
    "invalid = 0\n",
    "\n",
    "for fp in record_files:\n",
    "    try:\n",
    "        rec = safe_load_json(fp)\n",
    "    except Exception as e:\n",
    "        invalid += 1\n",
    "        rows_validation.append({\n",
    "            \"filename\": fp.name,\n",
    "            \"rdls_id\": \"\",\n",
    "            \"valid\": False,\n",
    "            \"message\": f\"json_parse_error: {e}\",\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Support either outer wrapper {'datasets':[...]} or raw dataset object\n",
    "    if isinstance(rec, dict) and \"datasets\" in rec and isinstance(rec[\"datasets\"], list) and rec[\"datasets\"]:\n",
    "        ds = rec[\"datasets\"][0]\n",
    "    else:\n",
    "        ds = rec\n",
    "\n",
    "    rdls_id = str(ds.get(\"id\", \"\")).strip()\n",
    "\n",
    "    ok, msg = validate_dataset_obj(ds)\n",
    "    rows_validation.append({\n",
    "        \"filename\": fp.name,\n",
    "        \"rdls_id\": rdls_id,\n",
    "        \"valid\": ok,\n",
    "        \"message\": msg,\n",
    "    })\n",
    "    if ok:\n",
    "        valid_ok += 1\n",
    "    else:\n",
    "        invalid += 1\n",
    "\n",
    "    # Missing required fields (treat empty string / empty list / empty dict as missing too)\n",
    "    missing_fields = []\n",
    "    for k in REQUIRED_FIELDS:\n",
    "        v = ds.get(k, None)\n",
    "        if v is None:\n",
    "            missing_fields.append(k)\n",
    "        elif isinstance(v, str) and not v.strip():\n",
    "            missing_fields.append(k)\n",
    "        elif isinstance(v, (list, dict)) and len(v) == 0:\n",
    "            missing_fields.append(k)\n",
    "    if missing_fields:\n",
    "        rows_missing.append({\n",
    "            \"filename\": fp.name,\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"missing_fields\": \";\".join(missing_fields),\n",
    "        })\n",
    "\n",
    "    # Duplicates\n",
    "    if rdls_id:\n",
    "        if rdls_id in seen_ids:\n",
    "            rows_duplicates.append({\n",
    "                \"type\": \"duplicate_id\",\n",
    "                \"rdls_id\": rdls_id,\n",
    "                \"filename_a\": seen_ids[rdls_id],\n",
    "                \"filename_b\": fp.name,\n",
    "            })\n",
    "        else:\n",
    "            seen_ids[rdls_id] = fp.name\n",
    "\n",
    "    # Identical-content duplicates (hash)\n",
    "    file_hash = sha256_file(fp)\n",
    "    if file_hash in seen_hash:\n",
    "        rows_duplicates.append({\n",
    "            \"type\": \"duplicate_content_hash\",\n",
    "            \"rdls_id\": rdls_id,\n",
    "            \"filename_a\": seen_hash[file_hash],\n",
    "            \"filename_b\": fp.name,\n",
    "        })\n",
    "    else:\n",
    "        seen_hash[file_hash] = fp.name\n",
    "\n",
    "df_val = pd.DataFrame(rows_validation)\n",
    "df_missing = pd.DataFrame(rows_missing)\n",
    "df_dups = pd.DataFrame(rows_duplicates)\n",
    "\n",
    "print(\"Valid:\", valid_ok, \"Invalid:\", invalid)\n",
    "print(\"Files with missing required fields:\", len(df_missing))\n",
    "print(\"Duplicates found:\", len(df_dups))\n",
    "\n",
    "OUT_VALIDATION = REPORTS_DIR / \"schema_validation_full.csv\"\n",
    "OUT_MISSING = REPORTS_DIR / \"rdls_missing_fields.csv\"\n",
    "OUT_DUPS = REPORTS_DIR / \"rdls_duplicates.csv\"\n",
    "\n",
    "df_val.to_csv(OUT_VALIDATION, index=False)\n",
    "df_missing.to_csv(OUT_MISSING, index=False)\n",
    "df_dups.to_csv(OUT_DUPS, index=False)\n",
    "\n",
    "print(\"Wrote:\", OUT_VALIDATION)\n",
    "print(\"Wrote:\", OUT_MISSING)\n",
    "print(\"Wrote:\", OUT_DUPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "824290d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\reports\\rdls_validation_summary.md\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Create a human-readable summary (Markdown)\n",
    "# ======================\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "summary_lines: List[str] = []\n",
    "summary_lines.append(\"# RDLS Validation Summary\")\n",
    "summary_lines.append(\"\")\n",
    "summary_lines.append(f\"- Run timestamp: {datetime.now(timezone.utc).isoformat()}\")\n",
    "summary_lines.append(f\"- Records folder: `{RECORDS_DIR}`\")\n",
    "summary_lines.append(f\"- Total JSON files: **{len(record_files)}**\")\n",
    "summary_lines.append(f\"- Schema valid: **{int((df_val['valid'] == True).sum())}**\")\n",
    "summary_lines.append(f\"- Schema invalid: **{int((df_val['valid'] == False).sum())}**\")\n",
    "summary_lines.append(f\"- Records missing required fields: **{len(df_missing)}**\")\n",
    "summary_lines.append(f\"- Duplicates detected: **{len(df_dups)}**\")\n",
    "summary_lines.append(\"\")\n",
    "\n",
    "if not df_missing.empty:\n",
    "    summary_lines.append(\"## Top missing required fields\")\n",
    "    # explode missing_fields\n",
    "    tmp = df_missing.copy()\n",
    "    tmp[\"missing_fields\"] = tmp[\"missing_fields\"].fillna(\"\").astype(str)\n",
    "    exploded = tmp[\"missing_fields\"].str.split(\";\").explode()\n",
    "    vc = exploded[exploded != \"\"].value_counts().head(20)\n",
    "    for k, v in vc.items():\n",
    "        summary_lines.append(f\"- `{k}`: {int(v)}\")\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "if not df_dups.empty:\n",
    "    summary_lines.append(\"## Duplicate signals\")\n",
    "    vc = df_dups[\"type\"].value_counts()\n",
    "    for k, v in vc.items():\n",
    "        summary_lines.append(f\"- `{k}`: {int(v)}\")\n",
    "    summary_lines.append(\"\")\n",
    "\n",
    "OUT_MD = REPORTS_DIR / \"rdls_validation_summary.md\"\n",
    "OUT_MD.write_text(\"\\n\".join(summary_lines) + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"Wrote:\", OUT_MD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e32debe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\rdls\\dist\\rdls_metadata_bundle.zip\n",
      "Zip size (MB): 0.14\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Package deliverable bundle (zip)\n",
    "# ======================\n",
    "import zipfile\n",
    "\n",
    "OUT_ZIP = DIST_DIR / \"rdls_metadata_bundle.zip\"\n",
    "\n",
    "def add_folder_to_zip(z: zipfile.ZipFile, folder: Path, arc_prefix: str) -> None:\n",
    "    for p in sorted(folder.rglob(\"*\")):\n",
    "        if p.is_file():\n",
    "            z.write(p, arcname=str(Path(arc_prefix) / p.relative_to(folder)))\n",
    "\n",
    "with zipfile.ZipFile(OUT_ZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "    # records + index + reports\n",
    "    add_folder_to_zip(z, RECORDS_DIR, \"records\")\n",
    "    if INDEX_JSONL.exists():\n",
    "        z.write(INDEX_JSONL, arcname=\"index/rdls_index.jsonl\")\n",
    "    add_folder_to_zip(z, REPORTS_DIR, \"reports\")\n",
    "\n",
    "print(\"Wrote:\", OUT_ZIP)\n",
    "print(\"Zip size (MB):\", round(OUT_ZIP.stat().st_size / (1024 * 1024), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfcf47-f4ea-4be5-8c2b-cd8ea4f4b722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
