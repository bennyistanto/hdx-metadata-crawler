{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 12: RDLS HEVL Integration\n",
    "\n",
    "**Purpose**: Integrate HEVL extractions (from Notebooks 09-11) with general metadata records (from Notebook 06) to produce complete RDLS v0.3 records.\n",
    "\n",
    "**Approach**:\n",
    "1. Load NB 06 general metadata JSONs as the base (authoritative general metadata)\n",
    "2. Load HEVL extraction CSVs (from NB 09-11) to determine component flags per dataset\n",
    "3. Load HEVL extraction JSONs (from NB 09-11) with detailed HEVL blocks\n",
    "4. Merge HEVL blocks into base records, update `risk_data_type`\n",
    "5. Generate final integrated RDLS JSON files\n",
    "\n",
    "**Key principle**: NB 12 does NOT rebuild general metadata or HEVL blocks. It merges\n",
    "pre-built outputs from upstream notebooks (NB 06 for general, NB 09-11 for HEVL).\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started: 2026-02-11T18:08:41.937336\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler\n",
      "NB 06 records: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/records\n",
      "NB 06 index:   /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/index/rdls_index.jsonl  (exists=True)\n",
      "Extracted:     /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/extracted\n",
      "Output:        /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/integrated\n",
      "Cleanup mode:  replace\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths and Output Settings\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "# Controls what happens to old output files when this notebook is re-run.\n",
    "#   \"replace\" - Auto-delete old outputs and continue (default)\n",
    "#   \"prompt\"  - Show what will be deleted, ask user to confirm\n",
    "#   \"skip\"    - Keep old files, write new on top (may leave orphans)\n",
    "#   \"abort\"   - Stop if old outputs exist (for CI/automated runs)\n",
    "CLEANUP_MODE = \"replace\"\n",
    "\n",
    "# NB 06 output: general metadata records (the BASE for integration)\n",
    "RECORDS_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'records'\n",
    "\n",
    "# NB 06 output: index JSONL mapping HDX UUIDs to NB 06 filenames\n",
    "INDEX_JSONL = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'index' / 'rdls_index.jsonl'\n",
    "\n",
    "# NB 09-11 output: HEVL extraction CSVs and JSONs\n",
    "EXTRACTED_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'extracted'\n",
    "\n",
    "# Extraction CSVs (separate files from revised NB 09-11)\n",
    "HAZARD_CSV = EXTRACTED_DIR / 'hazard_extraction_results.csv'\n",
    "EXPOSURE_CSV = EXTRACTED_DIR / 'exposure_extraction_results.csv'\n",
    "VULNERABILITY_CSV = EXTRACTED_DIR / 'vulnerability_extraction_results.csv'\n",
    "LOSS_CSV = EXTRACTED_DIR / 'loss_extraction_results.csv'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'integrated'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base: {BASE_DIR}\")\n",
    "print(f\"NB 06 records: {RECORDS_DIR}\")\n",
    "print(f\"NB 06 index:   {INDEX_JSONL}  (exists={INDEX_JSONL.exists()})\")\n",
    "print(f\"Extracted:     {EXTRACTED_DIR}\")\n",
    "print(f\"Output:        {OUTPUT_DIR}\")\n",
    "print(f\"Cleanup mode:  {CLEANUP_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NB 06 index: 13,152 entries\n",
      "  Columns: ['dataset_id', 'rdls_id', 'filename', 'risk_data_type', 'spatial_scale', 'countries_count', 'license_raw', 'orgtoken', 'hazard_suffix', 'organization_token', 'iso3', 'hazard_types', 'blocked', 'blocked_reasons']\n",
      "Loaded 13,152 NB 06 records indexed by HDX UUID\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Load NB 06 Index and General Metadata Records\n",
    "\n",
    "The NB 06 index JSONL is the BRIDGE between:\n",
    "  - HDX dataset UUIDs (used in HEVL extraction CSVs)\n",
    "  - NB 06 record filenames (slug-based, e.g., rdls_exp-hdx_3is_eth_...)\n",
    "\n",
    "We load the index first to build a UUID→filename mapping,\n",
    "then load the actual NB 06 JSON records.\n",
    "\"\"\"\n",
    "\n",
    "def load_nb06_index(index_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load rdls_index.jsonl produced by NB 06.\n",
    "    \n",
    "    Each line contains: dataset_id (HDX UUID), rdls_id, filename, etc.\n",
    "    \"\"\"\n",
    "    if not index_path.exists():\n",
    "        print(f\"  ERROR: Index not found at {index_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    records = []\n",
    "    with open(index_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                try:\n",
    "                    records.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Loaded NB 06 index: {len(df):,} entries\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_nb06_records(records_dir: Path, index_df: pd.DataFrame) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Load NB 06 RDLS JSON records, indexed by full HDX dataset UUID.\n",
    "    \n",
    "    Uses the index JSONL to map UUID -> filename, then loads each JSON.\n",
    "    \"\"\"\n",
    "    records = {}\n",
    "    missing = 0\n",
    "    \n",
    "    for _, row in index_df.iterrows():\n",
    "        hdx_uuid = row.get('dataset_id', '')\n",
    "        filename = row.get('filename', '')\n",
    "        \n",
    "        if not hdx_uuid or not filename:\n",
    "            continue\n",
    "        \n",
    "        filepath = records_dir / filename\n",
    "        if not filepath.exists():\n",
    "            missing += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            ds = data.get('datasets', [{}])[0]\n",
    "            \n",
    "            records[hdx_uuid] = {\n",
    "                'filepath': filepath,\n",
    "                'data': data,\n",
    "                'rdls_id': ds.get('id', ''),\n",
    "                'title': ds.get('title', ''),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(records):,} NB 06 records indexed by HDX UUID\")\n",
    "    if missing:\n",
    "        print(f\"  ({missing} index entries had no matching JSON file)\")\n",
    "    return records\n",
    "\n",
    "\n",
    "# Load index, then records\n",
    "df_nb06_index = load_nb06_index(INDEX_JSONL)\n",
    "nb06_records = load_nb06_records(RECORDS_DIR, df_nb06_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load HEVL Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HEVL extraction CSVs...\n",
      "  Hazard (NB 09): 26,246 records, columns=['id', 'title', 'organization', 'hazard_types', 'process_types', 'analysis_type', 'return_periods', 'intensity_measures', 'calculation_method', 'overall_confidence', 'has_hazard']\n",
      "  Exposure (NB 10): 26,246 records, columns=['id', 'title', 'organization', 'categories', 'category_count', 'taxonomy', 'overall_confidence', 'has_exposure']\n",
      "  Vulnerability (NB 11): 26,246 records, columns=['id', 'title', 'organization', 'has_functions', 'function_types', 'has_socio_economic', 'socio_indicators', 'overall_confidence', 'has_vulnerability']\n",
      "  Loss (NB 11): 26,246 records, columns=['id', 'title', 'organization', 'has_loss', 'loss_count', 'loss_signal_types', 'hazard_types', 'asset_categories', 'overall_confidence']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 Load HEVL Extraction CSVs\n",
    "\n",
    "These CSVs tell us WHICH datasets have WHICH components.\n",
    "- hazard_extraction_results.csv  (NB 09): has_hazard flag + hazard_types\n",
    "- exposure_extraction_results.csv (NB 10): has_exposure flag + categories\n",
    "- vulnerability_extraction_results.csv (NB 11): has_vulnerability flag\n",
    "- loss_extraction_results.csv (NB 11): has_loss flag\n",
    "\"\"\"\n",
    "\n",
    "def load_csv_safe(path: Path, label: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load CSV if it exists, return None otherwise.\"\"\"\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "        print(f\"  {label}: {len(df):,} records, columns={list(df.columns)}\")\n",
    "        return df\n",
    "    print(f\"  {label}: NOT FOUND at {path.name}\")\n",
    "    return None\n",
    "\n",
    "print(\"Loading HEVL extraction CSVs...\")\n",
    "df_hazard = load_csv_safe(HAZARD_CSV, \"Hazard (NB 09)\")\n",
    "df_exposure = load_csv_safe(EXPOSURE_CSV, \"Exposure (NB 10)\")\n",
    "df_vuln = load_csv_safe(VULNERABILITY_CSV, \"Vulnerability (NB 11)\")\n",
    "df_loss = load_csv_safe(LOSS_CSV, \"Loss (NB 11)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged flags: 26,246 unique datasets\n",
      "With any HEVL signal: 22,473\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Build Merged HEVL Flags DataFrame\n",
    "\n",
    "Merge all four CSVs on 'id' to get a single row per dataset\n",
    "with boolean has_hazard / has_exposure / has_vulnerability / has_loss flags.\n",
    "\"\"\"\n",
    "\n",
    "def merge_hevl_flags(\n",
    "    haz_df: Optional[pd.DataFrame],\n",
    "    exp_df: Optional[pd.DataFrame],\n",
    "    vul_df: Optional[pd.DataFrame],\n",
    "    los_df: Optional[pd.DataFrame],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge HEVL detection flags into one DataFrame keyed by HDX dataset id.\"\"\"\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    if haz_df is not None and 'id' in haz_df.columns:\n",
    "        h = haz_df[['id', 'has_hazard']].copy()\n",
    "        h['has_hazard'] = h['has_hazard'].astype(bool)\n",
    "        frames.append(h)\n",
    "    \n",
    "    if exp_df is not None and 'id' in exp_df.columns:\n",
    "        e = exp_df[['id', 'has_exposure']].copy()\n",
    "        e['has_exposure'] = e['has_exposure'].astype(bool)\n",
    "        frames.append(e)\n",
    "    \n",
    "    if vul_df is not None and 'id' in vul_df.columns:\n",
    "        v = vul_df[['id', 'has_vulnerability']].copy()\n",
    "        v['has_vulnerability'] = v['has_vulnerability'].astype(bool)\n",
    "        frames.append(v)\n",
    "    \n",
    "    if los_df is not None and 'id' in los_df.columns:\n",
    "        l = los_df[['id', 'has_loss']].copy()\n",
    "        l['has_loss'] = l['has_loss'].astype(bool)\n",
    "        frames.append(l)\n",
    "    \n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=['id', 'has_hazard', 'has_exposure', 'has_vulnerability', 'has_loss'])\n",
    "    \n",
    "    merged = frames[0]\n",
    "    for f in frames[1:]:\n",
    "        merged = merged.merge(f, on='id', how='outer')\n",
    "    \n",
    "    # Fill missing flags with False\n",
    "    for col in ['has_hazard', 'has_exposure', 'has_vulnerability', 'has_loss']:\n",
    "        if col not in merged.columns:\n",
    "            merged[col] = False\n",
    "        merged[col] = merged[col].fillna(False).astype(bool)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "df_flags = merge_hevl_flags(df_hazard, df_exposure, df_vuln, df_loss)\n",
    "print(f\"\\nMerged flags: {len(df_flags):,} unique datasets\")\n",
    "\n",
    "# Filter to datasets with at least one HEVL signal\n",
    "any_hevl = df_flags['has_hazard'] | df_flags['has_exposure'] | df_flags['has_vulnerability'] | df_flags['has_loss']\n",
    "df_hevl = df_flags[any_hevl].copy()\n",
    "print(f\"With any HEVL signal: {len(df_hevl):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HEVL EXTRACTION COVERAGE\n",
      "============================================================\n",
      "\n",
      "Total datasets scanned: 26,246\n",
      "\n",
      "Component Detection:\n",
      "  Hazard (H):         3,208 ( 12.2%)\n",
      "  Exposure (E):      21,631 ( 82.4%)\n",
      "  Vulnerability (V):  5,327 ( 20.3%)\n",
      "  Loss (L):           5,771 ( 22.0%)\n",
      "\n",
      "HEVL Combinations (top 15):\n",
      "  -E--: 11,091 ( 42.3%)\n",
      "  ----:  3,773 ( 14.4%)\n",
      "  -EV-:  3,164 ( 12.1%)\n",
      "  -E-L:  2,709 ( 10.3%)\n",
      "  -EVL:  1,952 (  7.4%)\n",
      "  HE--:  1,800 (  6.9%)\n",
      "  HE-L:    764 (  2.9%)\n",
      "  H---:    473 (  1.8%)\n",
      "  ---L:    289 (  1.1%)\n",
      "  HEV-:    114 (  0.4%)\n",
      "  --V-:     60 (  0.2%)\n",
      "  HEVL:     37 (  0.1%)\n",
      "  H--L:     20 (  0.1%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.3 Analyze HEVL Coverage\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HEVL EXTRACTION COVERAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_flags)\n",
    "h_count = df_flags['has_hazard'].sum()\n",
    "e_count = df_flags['has_exposure'].sum()\n",
    "v_count = df_flags['has_vulnerability'].sum()\n",
    "l_count = df_flags['has_loss'].sum()\n",
    "\n",
    "print(f\"\\nTotal datasets scanned: {total:,}\")\n",
    "print(f\"\\nComponent Detection:\")\n",
    "print(f\"  Hazard (H):        {h_count:>6,} ({h_count/total*100:>5.1f}%)\")\n",
    "print(f\"  Exposure (E):      {e_count:>6,} ({e_count/total*100:>5.1f}%)\")\n",
    "print(f\"  Vulnerability (V): {v_count:>6,} ({v_count/total*100:>5.1f}%)\")\n",
    "print(f\"  Loss (L):          {l_count:>6,} ({l_count/total*100:>5.1f}%)\")\n",
    "\n",
    "# HEVL combination analysis\n",
    "df_flags['hevl_combo'] = (\n",
    "    df_flags['has_hazard'].apply(lambda x: 'H' if x else '-') +\n",
    "    df_flags['has_exposure'].apply(lambda x: 'E' if x else '-') +\n",
    "    df_flags['has_vulnerability'].apply(lambda x: 'V' if x else '-') +\n",
    "    df_flags['has_loss'].apply(lambda x: 'L' if x else '-')\n",
    ")\n",
    "\n",
    "print(f\"\\nHEVL Combinations (top 15):\")\n",
    "for combo, count in df_flags['hevl_combo'].value_counts().head(15).items():\n",
    "    pct = count / total * 100\n",
    "    print(f\"  {combo}: {count:>6,} ({pct:>5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load HEVL Extraction JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID8→full mapping: 13,152 entries\n",
      "Loaded HEVL JSON blocks for 12594 datasets:\n",
      "  hazard: 2791 blocks\n",
      "  exposure: 11522 blocks\n",
      "  vulnerability: 3442 blocks\n",
      "  loss: 705 blocks\n",
      "  (11570 HEVL JSONs could not be resolved to full UUID)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.1 Load HEVL JSON Files by Component Type\n",
    "\n",
    "NB 09-11 each produce sample RDLS JSONs with detailed HEVL blocks:\n",
    "  - rdls_hzd-hdx_*.json  (hazard blocks from NB 09)\n",
    "  - rdls_exp-hdx_*.json  (exposure blocks from NB 10)\n",
    "  - rdls_vln-hdx_*.json  (vulnerability blocks from NB 11)\n",
    "  - rdls_lss-hdx_*.json  (loss blocks from NB 11)\n",
    "\n",
    "Each JSON has structure: {\"datasets\": [{\"hazard\": {...}, ...}]}\n",
    "We extract the HEVL-specific keys and index by FULL HDX UUID.\n",
    "\n",
    "NOTE: HEVL JSON filenames use 8-char UUID prefixes (e.g., rdls_hzd-hdx_0013fbe3.json).\n",
    "We use the NB 06 index to resolve these to full 36-char UUIDs.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "def build_uuid8_to_full(index_df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"Build mapping from 8-char UUID prefix to full 36-char UUID.\"\"\"\n",
    "    mapping = {}\n",
    "    if 'dataset_id' in index_df.columns:\n",
    "        for uuid_full in index_df['dataset_id'].dropna():\n",
    "            uuid8 = uuid_full[:8]\n",
    "            mapping[uuid8] = uuid_full\n",
    "    print(f\"UUID8→full mapping: {len(mapping):,} entries\")\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def load_hevl_jsons(\n",
    "    extracted_dir: Path,\n",
    "    uuid8_to_full: Dict[str, str],\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all HEVL JSONs from extracted_dir, indexed by FULL HDX UUID.\n",
    "    \n",
    "    Returns dict: full_hdx_uuid -> {\n",
    "        'hazard': {...} or absent,\n",
    "        'exposure': [...] or absent,\n",
    "        'vulnerability': {...} or absent,\n",
    "        'loss': {...} or absent,\n",
    "    }\n",
    "    \"\"\"\n",
    "    hevl_blocks = {}  # full_uuid -> component blocks\n",
    "    unresolved = 0\n",
    "    \n",
    "    # Component key per file prefix\n",
    "    prefix_to_key = {\n",
    "        'rdls_hzd': 'hazard',\n",
    "        'rdls_exp': 'exposure',\n",
    "        'rdls_vln': 'vulnerability',\n",
    "        'rdls_lss': 'loss',\n",
    "    }\n",
    "    \n",
    "    for filepath in sorted(extracted_dir.glob('rdls_*.json')):\n",
    "        try:\n",
    "            # Determine component type from filename prefix\n",
    "            stem = filepath.stem\n",
    "            component_key = None\n",
    "            for prefix, key in prefix_to_key.items():\n",
    "                if stem.startswith(prefix):\n",
    "                    component_key = key\n",
    "                    break\n",
    "            \n",
    "            if component_key is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract 8-char UUID from filename: rdls_hzd-hdx_0013fbe3 -> 0013fbe3\n",
    "            match = re.search(r'hdx_([a-f0-9]{8})$', stem)\n",
    "            if not match:\n",
    "                continue\n",
    "            \n",
    "            uuid8 = match.group(1)\n",
    "            full_uuid = uuid8_to_full.get(uuid8)\n",
    "            \n",
    "            if not full_uuid:\n",
    "                unresolved += 1\n",
    "                continue\n",
    "            \n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            ds = data.get('datasets', [{}])[0]\n",
    "            \n",
    "            if full_uuid not in hevl_blocks:\n",
    "                hevl_blocks[full_uuid] = {}\n",
    "            \n",
    "            # Extract the component-specific block\n",
    "            block = ds.get(component_key)\n",
    "            if block is not None:\n",
    "                hevl_blocks[full_uuid][component_key] = block\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading {filepath.name}: {e}\")\n",
    "    \n",
    "    # Count by component\n",
    "    counts = {'hazard': 0, 'exposure': 0, 'vulnerability': 0, 'loss': 0}\n",
    "    for uuid_full, blocks in hevl_blocks.items():\n",
    "        for key in counts:\n",
    "            if key in blocks:\n",
    "                counts[key] += 1\n",
    "    \n",
    "    print(f\"Loaded HEVL JSON blocks for {len(hevl_blocks)} datasets:\")\n",
    "    for key, count in counts.items():\n",
    "        print(f\"  {key}: {count} blocks\")\n",
    "    if unresolved:\n",
    "        print(f\"  ({unresolved} HEVL JSONs could not be resolved to full UUID)\")\n",
    "    \n",
    "    return hevl_blocks\n",
    "\n",
    "uuid8_to_full = build_uuid8_to_full(df_nb06_index)\n",
    "hevl_json_blocks = load_hevl_jsons(EXTRACTED_DIR, uuid8_to_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB 06 records indexed by full UUID: 13,152\n",
      "\n",
      "Coverage check:\n",
      "  Datasets with HEVL signals:   22,473\n",
      "  NB 06 records available:      13,152\n",
      "  Overlap (will integrate):     12,843\n",
      "  HEVL but no NB 06 record:      9,630\n",
      "  NB 06 but no HEVL signal:        309\n",
      "\n",
      "  Sample HEVL-only UUIDs (first 5): ['1cfe7b03-4a3a-4947-bf72-c0337a36a9ef', '6628359d-c7c9-40aa-a07f-22e22755d59d', 'e362a580-5075-4525-84f0-ec9500f6e09d', 'da611749-4214-4945-bd97-7f97ffa9ac62', 'f7ad9387-0fd0-4cbb-a871-48ff25bf1de2']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.2 Verify NB 06 Record Lookup\n",
    "\n",
    "NB 06 records are already indexed by full HDX UUID (loaded in cell 4).\n",
    "This cell verifies the lookup is working and shows coverage stats.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"NB 06 records indexed by full UUID: {len(nb06_records):,}\")\n",
    "\n",
    "# Check overlap with HEVL-flagged datasets\n",
    "hevl_uuids = set(df_hevl['id'].values)\n",
    "nb06_uuids = set(nb06_records.keys())\n",
    "\n",
    "overlap = hevl_uuids & nb06_uuids\n",
    "hevl_only = hevl_uuids - nb06_uuids\n",
    "nb06_only = nb06_uuids - hevl_uuids\n",
    "\n",
    "print(f\"\\nCoverage check:\")\n",
    "print(f\"  Datasets with HEVL signals: {len(hevl_uuids):>8,}\")\n",
    "print(f\"  NB 06 records available:    {len(nb06_uuids):>8,}\")\n",
    "print(f\"  Overlap (will integrate):   {len(overlap):>8,}\")\n",
    "print(f\"  HEVL but no NB 06 record:   {len(hevl_only):>8,}\")\n",
    "print(f\"  NB 06 but no HEVL signal:   {len(nb06_only):>8,}\")\n",
    "\n",
    "if hevl_only:\n",
    "    print(f\"\\n  Sample HEVL-only UUIDs (first 5): {list(hevl_only)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integration Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration helpers defined. All 15 combinations verified ✓\n",
      "  Prefix priority:  loss > vulnerability > exposure > hazard\n",
      "  Standalone guard:  REQUIRE_HE_FOR_VL = True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.1 Determine risk_data_type and Filename Prefix\n",
    "\n",
    "Naming convention priority (highest → lowest):\n",
    "  loss > vulnerability > exposure > hazard\n",
    "\n",
    "Examples:\n",
    "  H only              → rdls_hzd\n",
    "  E only              → rdls_exp\n",
    "  H + E               → rdls_exp   (exposure > hazard)\n",
    "  H + L               → rdls_lss   (loss > hazard)\n",
    "  E + L               → rdls_lss   (loss > exposure)\n",
    "  E + V               → rdls_vln   (vulnerability > exposure)\n",
    "  H + V               → rdls_vln   (vulnerability > hazard)\n",
    "  V + L               → rdls_lss   (loss > vulnerability)\n",
    "  H + E + V           → rdls_vln   (vulnerability > exposure > hazard)\n",
    "  H + E + L           → rdls_lss   (loss > exposure > hazard)\n",
    "  E + V + L           → rdls_lss   (loss > vulnerability > exposure)\n",
    "  H + V + L           → rdls_lss   (loss > vulnerability > hazard)\n",
    "  H + E + V + L       → rdls_lss   (loss > vulnerability > exposure > hazard)\n",
    "\n",
    "Standalone constraint (configurable):\n",
    "  V only (no H or E)  → INVALID — vulnerability needs hazard or exposure context\n",
    "  L only (no H or E)  → INVALID — loss needs hazard or exposure context\n",
    "  V + L (no H or E)   → INVALID — same reason\n",
    "  Set REQUIRE_HE_FOR_VL = False to disable this constraint.\n",
    "\"\"\"\n",
    "\n",
    "# Order for building risk_data_type list (canonical HEVL order)\n",
    "COMPONENT_ORDER = ['hazard', 'exposure', 'vulnerability', 'loss']\n",
    "\n",
    "# Priority order for filename prefix: loss > vulnerability > exposure > hazard\n",
    "PREFIX_PRIORITY = ['loss', 'vulnerability', 'exposure', 'hazard']\n",
    "\n",
    "PREFIX_MAP = {\n",
    "    'hazard': 'rdls_hzd',\n",
    "    'exposure': 'rdls_exp',\n",
    "    'vulnerability': 'rdls_vln',\n",
    "    'loss': 'rdls_lss',\n",
    "}\n",
    "\n",
    "# ── Standalone constraint (modular toggle) ────────────────────────────\n",
    "# When True: V and L cannot stand alone — they must be paired with H or E.\n",
    "# Combinations like V-only, L-only, or V+L (without H or E) are invalid.\n",
    "# Set to False if the team decides standalone V or L is acceptable.\n",
    "REQUIRE_HE_FOR_VL = True\n",
    "\n",
    "\n",
    "def determine_risk_data_types(flags: Dict[str, bool]) -> List[str]:\n",
    "    \"\"\"Determine risk_data_type list from boolean flags (canonical HEVL order).\"\"\"\n",
    "    types = []\n",
    "    for comp in COMPONENT_ORDER:\n",
    "        if flags.get(f'has_{comp}', False):\n",
    "            types.append(comp)\n",
    "    return types\n",
    "\n",
    "\n",
    "def validate_component_combination(risk_types: List[str]) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate whether a component combination is allowed.\n",
    "\n",
    "    Returns (is_valid, reason).\n",
    "    When REQUIRE_HE_FOR_VL is True, vulnerability and loss cannot appear\n",
    "    without at least one of hazard or exposure.\n",
    "    \"\"\"\n",
    "    if not REQUIRE_HE_FOR_VL:\n",
    "        return True, 'ok'\n",
    "\n",
    "    has_h_or_e = ('hazard' in risk_types) or ('exposure' in risk_types)\n",
    "    has_v_or_l = ('vulnerability' in risk_types) or ('loss' in risk_types)\n",
    "\n",
    "    if has_v_or_l and not has_h_or_e:\n",
    "        present = [c for c in risk_types]\n",
    "        return False, f\"standalone_vl_invalid({'+'.join(present)})\"\n",
    "\n",
    "    return True, 'ok'\n",
    "\n",
    "\n",
    "def determine_filename_prefix(risk_types: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Determine filename prefix based on naming convention priority.\n",
    "\n",
    "    Priority: loss > vulnerability > exposure > hazard\n",
    "    The prefix reflects the HIGHEST-priority component present.\n",
    "    \"\"\"\n",
    "    for comp in PREFIX_PRIORITY:\n",
    "        if comp in risk_types:\n",
    "            return PREFIX_MAP[comp]\n",
    "    return 'rdls_unk'\n",
    "\n",
    "\n",
    "# ── Verify all 15 possible HEVL combinations ─────────────────────────\n",
    "_test_cases = [\n",
    "    # Single component\n",
    "    (['hazard'],                                      'rdls_hzd', True),\n",
    "    (['exposure'],                                    'rdls_exp', True),\n",
    "    (['vulnerability'],                               'rdls_vln', False),  # standalone V invalid\n",
    "    (['loss'],                                        'rdls_lss', False),  # standalone L invalid\n",
    "    # Two components\n",
    "    (['hazard', 'exposure'],                          'rdls_exp', True),\n",
    "    (['hazard', 'vulnerability'],                     'rdls_vln', True),\n",
    "    (['hazard', 'loss'],                              'rdls_lss', True),\n",
    "    (['exposure', 'vulnerability'],                   'rdls_vln', True),\n",
    "    (['exposure', 'loss'],                            'rdls_lss', True),\n",
    "    (['vulnerability', 'loss'],                       'rdls_lss', False),  # standalone V+L invalid\n",
    "    # Three components\n",
    "    (['hazard', 'exposure', 'vulnerability'],         'rdls_vln', True),\n",
    "    (['hazard', 'exposure', 'loss'],                  'rdls_lss', True),\n",
    "    (['hazard', 'vulnerability', 'loss'],             'rdls_lss', True),\n",
    "    (['exposure', 'vulnerability', 'loss'],           'rdls_lss', True),\n",
    "    # All four\n",
    "    (['hazard', 'exposure', 'vulnerability', 'loss'], 'rdls_lss', True),\n",
    "]\n",
    "\n",
    "_all_pass = True\n",
    "for _types, _expected_prefix, _expected_valid in _test_cases:\n",
    "    _actual_prefix = determine_filename_prefix(_types)\n",
    "    _actual_valid, _ = validate_component_combination(_types)\n",
    "    if _actual_prefix != _expected_prefix:\n",
    "        print(f\"  PREFIX FAIL: {_types} → {_actual_prefix} (expected {_expected_prefix})\")\n",
    "        _all_pass = False\n",
    "    if _actual_valid != _expected_valid:\n",
    "        print(f\"  VALID  FAIL: {_types} → valid={_actual_valid} (expected {_expected_valid})\")\n",
    "        _all_pass = False\n",
    "\n",
    "if _all_pass:\n",
    "    print(f\"Integration helpers defined. All {len(_test_cases)} combinations verified ✓\")\n",
    "    print(f\"  Prefix priority:  loss > vulnerability > exposure > hazard\")\n",
    "    print(f\"  Standalone guard:  REQUIRE_HE_FOR_VL = {REQUIRE_HE_FOR_VL}\")\n",
    "else:\n",
    "    print(\"WARNING: Some test cases failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integration function defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.2 Integrate: Merge HEVL Blocks into NB 06 Base Records\n",
    "\n",
    "For each dataset with HEVL signals:\n",
    "1. Look up the NB 06 base record by full HDX UUID\n",
    "2. Deep-copy the base record\n",
    "3. Determine risk_data_type and validate the combination\n",
    "4. Update the RDLS id and filename prefix (loss > vulnerability > exposure > hazard)\n",
    "5. Insert HEVL blocks from NB 09-11 JSONs (if available)\n",
    "6. RECONCILE risk_data_type with actual blocks present\n",
    "7. Re-derive filename prefix from reconciled types\n",
    "8. Append HDX provenance note to description\n",
    "\"\"\"\n",
    "\n",
    "HDX_PROVENANCE_NOTE = (\n",
    "    \"[Source: This metadata record was automatically extracted from the \"\n",
    "    \"Humanitarian Data Exchange (HDX) at https://data.humdata.org]\"\n",
    ")\n",
    "\n",
    "def integrate_record(\n",
    "    hdx_uuid: str,\n",
    "    flags: Dict[str, bool],\n",
    "    nb06_entry: Optional[Dict],\n",
    "    hevl_blocks: Optional[Dict[str, Any]],\n",
    ") -> Tuple[Optional[Dict], str]:\n",
    "    \"\"\"\n",
    "    Produce an integrated RDLS record.\n",
    "\n",
    "    Args:\n",
    "        hdx_uuid: Full 36-char HDX dataset UUID\n",
    "        flags: Boolean HEVL detection flags\n",
    "        nb06_entry: NB 06 record entry (dict with 'data', 'rdls_id', etc.)\n",
    "        hevl_blocks: HEVL JSON blocks (dict with 'hazard', 'exposure', etc.)\n",
    "\n",
    "    Returns (rdls_record, status_message).\n",
    "    \"\"\"\n",
    "    risk_types = determine_risk_data_types(flags)\n",
    "\n",
    "    if not risk_types:\n",
    "        return None, 'no_hevl_signals'\n",
    "\n",
    "    # --- Validate component combination ---\n",
    "    is_valid, reason = validate_component_combination(risk_types)\n",
    "    if not is_valid:\n",
    "        return None, reason\n",
    "\n",
    "    # --- Base record from NB 06 ---\n",
    "    if nb06_entry is not None:\n",
    "        record = deepcopy(nb06_entry['data'])\n",
    "    else:\n",
    "        # No NB 06 record: skip (NB 06 is authoritative for general metadata)\n",
    "        return None, 'no_nb06_record'\n",
    "\n",
    "    ds = record['datasets'][0]\n",
    "\n",
    "    # --- Update risk_data_type ---\n",
    "    ds['risk_data_type'] = risk_types\n",
    "\n",
    "    # --- Update RDLS id prefix to reflect primary component ---\n",
    "    # Priority: loss > vulnerability > exposure > hazard\n",
    "    old_id = ds.get('id', '')\n",
    "    new_prefix = determine_filename_prefix(risk_types)\n",
    "\n",
    "    # Replace the existing prefix pattern: rdls_XXX-hdx_ -> new_prefix-hdx_\n",
    "    new_id = re.sub(r'^rdls_\\w+-hdx_', f'{new_prefix}-hdx_', old_id)\n",
    "    if new_id == old_id and not old_id.startswith(new_prefix):\n",
    "        # Fallback: just prepend new prefix with UUID\n",
    "        uuid8 = hdx_uuid[:8]\n",
    "        new_id = f\"{new_prefix}-hdx_{uuid8}\"\n",
    "    ds['id'] = new_id\n",
    "\n",
    "    # --- Move \"links\" to end (insert HEVL blocks before it) ---\n",
    "    links_value = ds.pop('links', None)\n",
    "\n",
    "    # --- Insert HEVL blocks from JSON files ---\n",
    "    if hevl_blocks:\n",
    "        for comp in COMPONENT_ORDER:\n",
    "            if comp in hevl_blocks:\n",
    "                ds[comp] = deepcopy(hevl_blocks[comp])\n",
    "\n",
    "    # --- RECONCILE risk_data_type with actual blocks present ---\n",
    "    # CSV flags may declare components that don't have actual JSON blocks.\n",
    "    # Only list components that are actually present in the record.\n",
    "    actual_types = [c for c in COMPONENT_ORDER if c in ds]\n",
    "    if not actual_types:\n",
    "        return None, 'no_actual_hevl_blocks'\n",
    "\n",
    "    # Re-validate the reconciled combination\n",
    "    is_valid_actual, reason_actual = validate_component_combination(actual_types)\n",
    "    if not is_valid_actual:\n",
    "        return None, f'reconciled_{reason_actual}'\n",
    "\n",
    "    # Update risk_data_type to match reality\n",
    "    ds['risk_data_type'] = actual_types\n",
    "\n",
    "    # Re-derive filename prefix from reconciled types\n",
    "    reconciled_prefix = determine_filename_prefix(actual_types)\n",
    "    ds['id'] = re.sub(r'^rdls_\\w+-hdx_', f'{reconciled_prefix}-hdx_', ds.get('id', ''))\n",
    "\n",
    "    # --- Re-add \"links\" at the very end of the dataset object ---\n",
    "    if links_value is not None:\n",
    "        ds['links'] = links_value\n",
    "\n",
    "    # --- Append HDX provenance note to description ---\n",
    "    hdx_url = f\"https://data.humdata.org/dataset/{hdx_uuid}\"\n",
    "    provenance = (\n",
    "        f\"{HDX_PROVENANCE_NOTE} \"\n",
    "        f\"[Original dataset: {hdx_url}]\"\n",
    "    )\n",
    "    existing_desc = (ds.get('description') or '').rstrip()\n",
    "    if existing_desc:\n",
    "        if existing_desc[-1] not in '.!?:;)\"':  # Add period if no terminal punctuation\n",
    "            existing_desc += '.'\n",
    "        ds['description'] = f\"{existing_desc} {provenance}\"\n",
    "    else:\n",
    "        ds['description'] = provenance\n",
    "\n",
    "    return record, 'ok'\n",
    "\n",
    "print(\"Integration function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing function defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.3 Process All Records\n",
    "\"\"\"\n",
    "\n",
    "def process_integration(\n",
    "    df_hevl: pd.DataFrame,\n",
    "    nb06_records: Dict[str, Dict],\n",
    "    hevl_json_blocks: Dict[str, Dict],\n",
    "    output_dir: Path,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process all datasets with HEVL signals and write integrated records.\n",
    "\n",
    "    All lookups use full 36-char HDX UUIDs as the join key.\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total': len(df_hevl),\n",
    "        'integrated': 0,\n",
    "        'with_json_blocks': 0,\n",
    "        'flags_only': 0,\n",
    "        'no_nb06': 0,\n",
    "        'no_actual_blocks': 0,\n",
    "        'reconciled_vl_invalid': 0,\n",
    "        'standalone_vl_invalid': 0,\n",
    "        'errors': 0,\n",
    "        'by_type': {c: 0 for c in COMPONENT_ORDER},\n",
    "        'by_type_declared': {c: 0 for c in COMPONENT_ORDER},\n",
    "        'by_prefix': {},\n",
    "    }\n",
    "\n",
    "    skipped_records = []\n",
    "\n",
    "    iterator = tqdm(df_hevl.iterrows(), total=len(df_hevl), desc=\"Integrating\") if HAS_TQDM else df_hevl.iterrows()\n",
    "\n",
    "    for idx, row in iterator:\n",
    "        hdx_uuid = row['id']  # Full 36-char UUID from extraction CSV\n",
    "\n",
    "        flags = {\n",
    "            'has_hazard': bool(row.get('has_hazard', False)),\n",
    "            'has_exposure': bool(row.get('has_exposure', False)),\n",
    "            'has_vulnerability': bool(row.get('has_vulnerability', False)),\n",
    "            'has_loss': bool(row.get('has_loss', False)),\n",
    "        }\n",
    "\n",
    "        # Lookup by FULL UUID\n",
    "        nb06_entry = nb06_records.get(hdx_uuid)\n",
    "        blocks = hevl_json_blocks.get(hdx_uuid)\n",
    "\n",
    "        try:\n",
    "            record, status = integrate_record(hdx_uuid, flags, nb06_entry, blocks)\n",
    "\n",
    "            if record is None:\n",
    "                if status == 'no_nb06_record':\n",
    "                    stats['no_nb06'] += 1\n",
    "                elif status == 'no_actual_hevl_blocks':\n",
    "                    stats['no_actual_blocks'] += 1\n",
    "                elif 'reconciled_' in status:\n",
    "                    stats['reconciled_vl_invalid'] += 1\n",
    "                elif 'standalone_vl_invalid' in status:\n",
    "                    stats['standalone_vl_invalid'] += 1\n",
    "                skipped_records.append({'id': hdx_uuid, 'reason': status})\n",
    "                continue\n",
    "\n",
    "            # Determine output filename from the integrated record's RDLS id\n",
    "            rdls_id = record['datasets'][0].get('id', '')\n",
    "            filename = f\"{rdls_id}.json\" if rdls_id else f\"rdls_unk-hdx_{hdx_uuid[:8]}.json\"\n",
    "\n",
    "            # Track prefix distribution\n",
    "            prefix = rdls_id.split('-')[0] if '-' in rdls_id else 'rdls_unk'\n",
    "            stats['by_prefix'][prefix] = stats['by_prefix'].get(prefix, 0) + 1\n",
    "\n",
    "            # Write\n",
    "            with open(output_dir / filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(record, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            stats['integrated'] += 1\n",
    "            if blocks:\n",
    "                stats['with_json_blocks'] += 1\n",
    "            else:\n",
    "                stats['flags_only'] += 1\n",
    "\n",
    "            # Track declared types (from CSV flags) for comparison\n",
    "            for comp in COMPONENT_ORDER:\n",
    "                if flags.get(f'has_{comp}', False):\n",
    "                    stats['by_type_declared'][comp] += 1\n",
    "\n",
    "            # Track actual types (after reconciliation)\n",
    "            risk_types = record['datasets'][0].get('risk_data_type', [])\n",
    "            for rt in risk_types:\n",
    "                stats['by_type'][rt] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            stats['errors'] += 1\n",
    "            skipped_records.append({'id': hdx_uuid, 'reason': f'error: {e}'})\n",
    "\n",
    "    # Save skipped records\n",
    "    if skipped_records:\n",
    "        pd.DataFrame(skipped_records).to_csv(\n",
    "            output_dir / 'integration_skipped.csv', index=False\n",
    "        )\n",
    "\n",
    "    return stats\n",
    "\n",
    "print(\"Processing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 12 Integrated Records]:\n",
      "  rdls_*.json                   : 12,577 files\n",
      "  rdls_index.csv                : 1 files\n",
      "  rdls_index.jsonl              : 1 files\n",
      "  integration_skipped.csv       : 1 files\n",
      "Cleaned 12,580 files. Ready for fresh output.\n",
      "\n",
      "Integrating 22,473 datasets with HEVL signals...\n",
      "  NB 06 base records available: 13,152\n",
      "  HEVL JSON blocks available:   12,594\n",
      "  Standalone V/L guard:         REQUIRE_HE_FOR_VL = True\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816186d4574e447f802f54e4deb8904a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Integrating:   0%|          | 0/22473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INTEGRATION RESULTS\n",
      "============================================================\n",
      "Total candidates:             22,473\n",
      "Successfully integrated:      12,583\n",
      "  With JSON HEVL blocks:      12,583\n",
      "  Flags only (no JSON):            0\n",
      "Skipped (no NB 06):            9,536\n",
      "Skipped (no actual blocks):        5\n",
      "Skipped (reconciled V/L):          0\n",
      "Skipped (standalone V/L):        349\n",
      "Errors:                            0\n",
      "\n",
      "By component type (in integrated records):\n",
      "  hazard              :  2,791\n",
      "  exposure            : 11,522\n",
      "  vulnerability       :  3,431\n",
      "  loss                :    705\n",
      "\n",
      "By filename prefix:\n",
      "  rdls_exp    :  7,563\n",
      "  rdls_vln    :  3,400\n",
      "  rdls_hzd    :    915\n",
      "  rdls_lss    :    705\n",
      "\n",
      "============================================================\n",
      "RECONCILIATION (declared vs actual risk_data_type)\n",
      "============================================================\n",
      "  Component              Declared     Actual    Dropped\n",
      "  hazard                    2,791      2,791          0\n",
      "  exposure                 12,155     11,522        633\n",
      "  vulnerability             3,431      3,431          0\n",
      "  loss                      3,726        705      3,021\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.1 Clean Previous Outputs and Run Integration\n",
    "\n",
    "Removes stale output files before writing fresh records.\n",
    "Controlled by CLEANUP_MODE variable (set in cell 1.2).\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Clean stale output files before writing new ones.\n",
    "\n",
    "    Args:\n",
    "        output_dir: Path to the output directory\n",
    "        patterns: list of glob patterns to match\n",
    "        label: human-readable label for display\n",
    "        mode: \"replace\" | \"prompt\" | \"skip\" | \"abort\"\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: deleted (int), skipped (bool)\n",
    "    \"\"\"\n",
    "    result = {\"deleted\": 0, \"skipped\": False}\n",
    "\n",
    "    # Find all matching files\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    # Nothing to clean\n",
    "    if total == 0:\n",
    "        print(f\"Output cleanup [{label}]: Directory is clean.\")\n",
    "        return result\n",
    "\n",
    "    # Build summary lines\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f\"  {pattern:30s}: {len(files):,} files\")\n",
    "\n",
    "    # --- Mode: skip ---\n",
    "    if mode == \"skip\":\n",
    "        print(f\"Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)\")\n",
    "        result[\"skipped\"] = True\n",
    "        return result\n",
    "\n",
    "    # --- Mode: abort ---\n",
    "    if mode == \"abort\":\n",
    "        print(f\"Output cleanup [{label}]: {total:,} stale files found!\")\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        raise RuntimeError(\n",
    "            f\"Abort: {total:,} stale files in {output_dir.name}/. \"\n",
    "            f\"Clear manually or set CLEANUP_MODE='replace'.\"\n",
    "        )\n",
    "\n",
    "    # --- Mode: prompt ---\n",
    "    if mode == \"prompt\":\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"OUTPUT CLEANUP — {label}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"Found {total:,} existing output files in {output_dir.name}/:\\n\")\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        print()\n",
    "        print(\"Options:\")\n",
    "        print(\"  [R]eplace  - Delete all and write fresh\")\n",
    "        print(\"  [S]kip     - Keep existing, write on top\")\n",
    "        print(\"  [A]bort    - Stop notebook execution\")\n",
    "        print()\n",
    "\n",
    "        choice = input(\"Choose [R/S/A]: \").strip().lower()\n",
    "\n",
    "        if choice in (\"a\", \"abort\"):\n",
    "            raise RuntimeError(\"Cleanup aborted by user.\")\n",
    "        elif choice in (\"s\", \"skip\"):\n",
    "            print(\"Cleanup: SKIPPED by user.\")\n",
    "            result[\"skipped\"] = True\n",
    "            return result\n",
    "        elif choice not in (\"r\", \"replace\", \"\"):\n",
    "            print(f\"Unrecognized '{choice}'. Defaulting to Replace.\")\n",
    "\n",
    "    # --- Mode: replace (default) or user chose Replace ---\n",
    "    print(f\"Output cleanup [{label}]:\")\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result[\"deleted\"] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  WARNING: Could not delete {f.name}: {e}\")\n",
    "\n",
    "    print(f\"Cleaned {result['deleted']:,} files. Ready for fresh output.\\n\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# ── Clean stale output ────────────────────────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    OUTPUT_DIR,\n",
    "    patterns=[\n",
    "        \"rdls_*.json\",\n",
    "        \"rdls_index.csv\",\n",
    "        \"rdls_index.jsonl\",\n",
    "        \"integration_skipped.csv\",\n",
    "    ],\n",
    "    label=\"NB 12 Integrated Records\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n",
    "\n",
    "# ── Run integration ──────────────────────────────────────────────────\n",
    "print(f\"Integrating {len(df_hevl):,} datasets with HEVL signals...\")\n",
    "print(f\"  NB 06 base records available: {len(nb06_records):,}\")\n",
    "print(f\"  HEVL JSON blocks available:   {len(hevl_json_blocks):,}\")\n",
    "print(f\"  Standalone V/L guard:         REQUIRE_HE_FOR_VL = {REQUIRE_HE_FOR_VL}\")\n",
    "print()\n",
    "\n",
    "stats = process_integration(df_hevl, nb06_records, hevl_json_blocks, OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INTEGRATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total candidates:           {stats['total']:>8,}\")\n",
    "print(f\"Successfully integrated:    {stats['integrated']:>8,}\")\n",
    "print(f\"  With JSON HEVL blocks:    {stats['with_json_blocks']:>8,}\")\n",
    "print(f\"  Flags only (no JSON):     {stats['flags_only']:>8,}\")\n",
    "print(f\"Skipped (no NB 06):         {stats['no_nb06']:>8,}\")\n",
    "print(f\"Skipped (no actual blocks): {stats.get('no_actual_blocks', 0):>8,}\")\n",
    "print(f\"Skipped (reconciled V/L):   {stats.get('reconciled_vl_invalid', 0):>8,}\")\n",
    "print(f\"Skipped (standalone V/L):   {stats['standalone_vl_invalid']:>8,}\")\n",
    "print(f\"Errors:                     {stats['errors']:>8,}\")\n",
    "\n",
    "print(f\"\\nBy component type (in integrated records):\")\n",
    "for comp, count in stats['by_type'].items():\n",
    "    print(f\"  {comp:20s}: {count:>6,}\")\n",
    "\n",
    "print(f\"\\nBy filename prefix:\")\n",
    "for prefix in sorted(stats['by_prefix'], key=lambda k: -stats['by_prefix'][k]):\n",
    "    count = stats['by_prefix'][prefix]\n",
    "    print(f\"  {prefix:12s}: {count:>6,}\")\n",
    "\n",
    "# --- Reconciliation summary ---\n",
    "if \"by_type_declared\" in stats:\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RECONCILIATION (declared vs actual risk_data_type)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  {'Component':<20s} {'Declared':>10s} {'Actual':>10s} {'Dropped':>10s}\")\n",
    "    for comp in [\"hazard\", \"exposure\", \"vulnerability\", \"loss\"]:\n",
    "        declared = stats[\"by_type_declared\"].get(comp, 0)\n",
    "        actual = stats[\"by_type\"].get(comp, 0)\n",
    "        dropped = declared - actual\n",
    "        print(f\"  {comp:<20s} {declared:>10,} {actual:>10,} {dropped:>10,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Index and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index generated: 12577 records\n",
      "Saved to: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/integrated/rdls_index.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.1 Generate RDLS Index of Integrated Records\n",
    "\"\"\"\n",
    "\n",
    "def generate_rdls_index(output_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Scan integrated JSON files and build an index DataFrame.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for filepath in sorted(output_dir.glob('rdls_*.json')):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            ds = data.get('datasets', [{}])[0]\n",
    "            \n",
    "            records.append({\n",
    "                'filename': filepath.name,\n",
    "                'id': ds.get('id', ''),\n",
    "                'title': (ds.get('title', '') or '')[:120],\n",
    "                'risk_data_type': '|'.join(ds.get('risk_data_type', [])),\n",
    "                'has_hazard': 'hazard' in ds,\n",
    "                'has_exposure': 'exposure' in ds,\n",
    "                'has_vulnerability': 'vulnerability' in ds,\n",
    "                'has_loss': 'loss' in ds,\n",
    "                'license': ds.get('license', ''),\n",
    "                'resource_count': len(ds.get('resources', [])),\n",
    "                'country_count': len(ds.get('spatial', {}).get('countries', [])),\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(output_dir / 'rdls_index.csv', index=False)\n",
    "    \n",
    "    # JSONL format\n",
    "    with open(output_dir / 'rdls_index.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_index = generate_rdls_index(OUTPUT_DIR)\n",
    "print(f\"Index generated: {len(df_index)} records\")\n",
    "print(f\"Saved to: {OUTPUT_DIR / 'rdls_index.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTEGRATED RDLS RECORDS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total records: 12577\n",
      "\n",
      "By risk_data_type combination:\n",
      "  exposure                                :  6,449\n",
      "  exposure|vulnerability                  :  3,340\n",
      "  hazard|exposure                         :  1,113\n",
      "  hazard                                  :    914\n",
      "  hazard|exposure|loss                    :    527\n",
      "  hazard|loss                             :    145\n",
      "  hazard|exposure|vulnerability           :     57\n",
      "  hazard|exposure|vulnerability|loss      :     31\n",
      "  hazard|vulnerability                    :      1\n",
      "\n",
      "Component coverage:\n",
      "  With hazard block:         2,788\n",
      "  With exposure block:      11,517\n",
      "  With vulnerability block:  3,429\n",
      "  With loss block:             703\n",
      "\n",
      "  Records with HEVL JSON blocks: 12,577\n",
      "  Records flags-only (no block):      0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.2 Summary Report\n",
    "\"\"\"\n",
    "\n",
    "if len(df_index) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INTEGRATED RDLS RECORDS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal records: {len(df_index)}\")\n",
    "    \n",
    "    print(f\"\\nBy risk_data_type combination:\")\n",
    "    for combo, count in df_index['risk_data_type'].value_counts().items():\n",
    "        print(f\"  {combo:40s}: {count:>6,}\")\n",
    "    \n",
    "    print(f\"\\nComponent coverage:\")\n",
    "    print(f\"  With hazard block:        {df_index['has_hazard'].sum():>6,}\")\n",
    "    print(f\"  With exposure block:      {df_index['has_exposure'].sum():>6,}\")\n",
    "    print(f\"  With vulnerability block: {df_index['has_vulnerability'].sum():>6,}\")\n",
    "    print(f\"  With loss block:          {df_index['has_loss'].sum():>6,}\")\n",
    "    \n",
    "    # Records with actual HEVL JSON blocks vs flags-only\n",
    "    has_any_block = df_index['has_hazard'] | df_index['has_exposure'] | df_index['has_vulnerability'] | df_index['has_loss']\n",
    "    print(f\"\\n  Records with HEVL JSON blocks: {has_any_block.sum():>6,}\")\n",
    "    print(f\"  Records flags-only (no block): {(~has_any_block).sum():>6,}\")\n",
    "else:\n",
    "    print(\"No integrated records found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Notebook completed: 2026-02-11T18:25:31.370201\n",
      "Output directory: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/rdls/integrated\n",
      "Records generated: 12577\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Records generated: {len(df_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
