{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 12: RDLS HEVL Integration\n",
    "\n",
    "**Purpose**: Integrate HEVL extractions with existing general metadata to produce complete RDLS v0.3 records.\n",
    "\n",
    "**Process**:\n",
    "1. Load existing RDLS general metadata (from Notebook 06)\n",
    "2. Load HEVL extractions (from Notebooks 09-11)\n",
    "3. Merge extractions into complete RDLS records\n",
    "4. Apply component gating rules (V/L require H or E)\n",
    "5. Generate final RDLS JSON files\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input paths\n",
    "DATASET_METADATA_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'dataset_metadata'\n",
    "SIGNAL_DICT_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'config' / 'signal_dictionary.yaml'\n",
    "RDLS_TEMPLATE_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'template' / 'rdls_template_v03.json'\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "\n",
    "# Extraction results from previous notebooks\n",
    "EXTRACTED_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'extracted'\n",
    "HAZARD_RESULTS = EXTRACTED_DIR / 'hazard_extraction_results.csv'\n",
    "EXPOSURE_RESULTS = EXTRACTED_DIR / 'exposure_extraction_results.csv'\n",
    "VL_RESULTS = EXTRACTED_DIR / 'vuln_loss_extraction_results.csv'\n",
    "\n",
    "# Classification from Notebook 05\n",
    "CLASSIFICATION_FILE = BASE_DIR / 'hdx_dataset_metadata_dump' / 'derived' / 'classification_final.csv'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'integrated'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base: {BASE_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Load Resources\n",
    "\"\"\"\n",
    "\n",
    "# Load signal dictionary\n",
    "with open(SIGNAL_DICT_PATH, 'r', encoding='utf-8') as f:\n",
    "    SIGNAL_DICT = yaml.safe_load(f)\n",
    "\n",
    "# Load RDLS template\n",
    "with open(RDLS_TEMPLATE_PATH, 'r', encoding='utf-8') as f:\n",
    "    RDLS_TEMPLATE = json.load(f)\n",
    "\n",
    "# Load RDLS schema for validation\n",
    "with open(RDLS_SCHEMA_PATH, 'r', encoding='utf-8') as f:\n",
    "    RDLS_SCHEMA = json.load(f)\n",
    "\n",
    "print(\"Resources loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Extraction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Load HEVL Extraction CSVs\n",
    "\"\"\"\n",
    "\n",
    "def load_extraction_csv(path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Load extraction CSV if it exists.\"\"\"\n",
    "    if path.exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded {path.name}: {len(df)} records\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Not found: {path.name}\")\n",
    "        return None\n",
    "\n",
    "df_hazard = load_extraction_csv(HAZARD_RESULTS)\n",
    "df_exposure = load_extraction_csv(EXPOSURE_RESULTS)\n",
    "df_vl = load_extraction_csv(VL_RESULTS)\n",
    "\n",
    "# Also try to load classification if available\n",
    "df_classification = load_extraction_csv(CLASSIFICATION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Merge Extraction Results\n",
    "\"\"\"\n",
    "\n",
    "def merge_extractions(\n",
    "    hazard_df: Optional[pd.DataFrame],\n",
    "    exposure_df: Optional[pd.DataFrame],\n",
    "    vl_df: Optional[pd.DataFrame]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge HEVL extractions into single DataFrame.\n",
    "    \"\"\"\n",
    "    # Start with hazard or exposure (whichever is larger)\n",
    "    dfs_to_merge = []\n",
    "    \n",
    "    if hazard_df is not None:\n",
    "        hdf = hazard_df[['id', 'title', 'organization', 'hazard_types', \n",
    "                        'analysis_type', 'return_periods', 'has_hazard']].copy()\n",
    "        hdf.columns = ['id', 'title', 'organization', 'hazard_types',\n",
    "                      'hazard_analysis_type', 'return_periods', 'has_hazard']\n",
    "        dfs_to_merge.append(hdf)\n",
    "    \n",
    "    if exposure_df is not None:\n",
    "        edf = exposure_df[['id', 'categories', 'taxonomy', 'has_exposure']].copy()\n",
    "        edf.columns = ['id', 'exposure_categories', 'exposure_taxonomy', 'has_exposure']\n",
    "        dfs_to_merge.append(edf)\n",
    "    \n",
    "    if vl_df is not None:\n",
    "        vldf = vl_df[['id', 'has_vulnerability', 'has_socioeconomic', \n",
    "                      'has_loss', 'loss_types', 'loss_hazard']].copy()\n",
    "        dfs_to_merge.append(vldf)\n",
    "    \n",
    "    if not dfs_to_merge:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Merge all on 'id'\n",
    "    merged = dfs_to_merge[0]\n",
    "    for df in dfs_to_merge[1:]:\n",
    "        merged = merged.merge(df, on='id', how='outer')\n",
    "    \n",
    "    # Fill NaN booleans with False\n",
    "    bool_cols = ['has_hazard', 'has_exposure', 'has_vulnerability', \n",
    "                 'has_socioeconomic', 'has_loss']\n",
    "    for col in bool_cols:\n",
    "        if col in merged.columns:\n",
    "            merged[col] = merged[col].fillna(False)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "df_merged = merge_extractions(df_hazard, df_exposure, df_vl)\n",
    "print(f\"\\nMerged DataFrame: {len(df_merged)} records\")\n",
    "print(f\"Columns: {list(df_merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.3 Analyze HEVL Coverage\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HEVL EXTRACTION COVERAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(df_merged) > 0:\n",
    "    total = len(df_merged)\n",
    "    \n",
    "    # Count each component\n",
    "    h_count = df_merged['has_hazard'].sum() if 'has_hazard' in df_merged else 0\n",
    "    e_count = df_merged['has_exposure'].sum() if 'has_exposure' in df_merged else 0\n",
    "    v_count = df_merged['has_vulnerability'].sum() if 'has_vulnerability' in df_merged else 0\n",
    "    l_count = df_merged['has_loss'].sum() if 'has_loss' in df_merged else 0\n",
    "    \n",
    "    print(f\"\\nTotal records: {total:,}\")\n",
    "    print(f\"\\nComponent Coverage:\")\n",
    "    print(f\"  Hazard (H):       {h_count:>6,} ({h_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Exposure (E):     {e_count:>6,} ({e_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Vulnerability (V):{v_count:>6,} ({v_count/total*100:>5.1f}%)\")\n",
    "    print(f\"  Loss (L):         {l_count:>6,} ({l_count/total*100:>5.1f}%)\")\n",
    "    \n",
    "    # HEVL combinations\n",
    "    df_merged['hevl_combo'] = (\n",
    "        df_merged.get('has_hazard', False).apply(lambda x: 'H' if x else '-') +\n",
    "        df_merged.get('has_exposure', False).apply(lambda x: 'E' if x else '-') +\n",
    "        df_merged.get('has_vulnerability', False).apply(lambda x: 'V' if x else '-') +\n",
    "        df_merged.get('has_loss', False).apply(lambda x: 'L' if x else '-')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHEVL Combinations:\")\n",
    "    for combo, count in df_merged['hevl_combo'].value_counts().head(10).items():\n",
    "        print(f\"  {combo}: {count:>6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RDLS Integration Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Component Gating Rules\n",
    "\n",
    "RDLS business rules:\n",
    "- Vulnerability data should be associated with Hazard and/or Exposure\n",
    "- Loss data should be associated with Hazard and/or Exposure\n",
    "\"\"\"\n",
    "\n",
    "def apply_gating_rules(row: pd.Series) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Apply RDLS component gating rules.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        Row from merged DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Gating decision with included components and any blocks\n",
    "    \"\"\"\n",
    "    has_h = row.get('has_hazard', False)\n",
    "    has_e = row.get('has_exposure', False)\n",
    "    has_v = row.get('has_vulnerability', False)\n",
    "    has_l = row.get('has_loss', False)\n",
    "    \n",
    "    result = {\n",
    "        'include_hazard': has_h,\n",
    "        'include_exposure': has_e,\n",
    "        'include_vulnerability': False,\n",
    "        'include_loss': False,\n",
    "        'blocked': False,\n",
    "        'block_reason': None\n",
    "    }\n",
    "    \n",
    "    # Vulnerability gating: needs H or E\n",
    "    if has_v:\n",
    "        if has_h or has_e:\n",
    "            result['include_vulnerability'] = True\n",
    "        else:\n",
    "            result['blocked'] = True\n",
    "            result['block_reason'] = 'vulnerability_without_hazard_or_exposure'\n",
    "    \n",
    "    # Loss gating: needs H or E\n",
    "    if has_l:\n",
    "        if has_h or has_e:\n",
    "            result['include_loss'] = True\n",
    "        else:\n",
    "            if result['blocked']:\n",
    "                result['block_reason'] += '_and_loss_without_hazard_or_exposure'\n",
    "            else:\n",
    "                result['blocked'] = True\n",
    "                result['block_reason'] = 'loss_without_hazard_or_exposure'\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply gating\n",
    "if len(df_merged) > 0:\n",
    "    gating_results = df_merged.apply(apply_gating_rules, axis=1)\n",
    "    df_gating = pd.DataFrame(gating_results.tolist())\n",
    "    df_merged = pd.concat([df_merged, df_gating], axis=1)\n",
    "    \n",
    "    print(\"Gating rules applied.\")\n",
    "    print(f\"Blocked records: {df_merged['blocked'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.2 Determine Primary risk_data_type\n",
    "\"\"\"\n",
    "\n",
    "def determine_risk_data_types(row: pd.Series) -> List[str]:\n",
    "    \"\"\"\n",
    "    Determine risk_data_type array for RDLS record.\n",
    "    \"\"\"\n",
    "    types = []\n",
    "    \n",
    "    if row.get('include_hazard', False):\n",
    "        types.append('hazard')\n",
    "    if row.get('include_exposure', False):\n",
    "        types.append('exposure')\n",
    "    if row.get('include_vulnerability', False):\n",
    "        types.append('vulnerability')\n",
    "    if row.get('include_loss', False):\n",
    "        types.append('loss')\n",
    "    \n",
    "    return types\n",
    "\n",
    "def determine_filename_prefix(risk_types: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Determine RDLS filename prefix based on primary component.\n",
    "    \n",
    "    Priority: hazard > exposure > vulnerability > loss\n",
    "    \"\"\"\n",
    "    if 'hazard' in risk_types:\n",
    "        return 'rdls_hzd'\n",
    "    elif 'exposure' in risk_types:\n",
    "        return 'rdls_exp'\n",
    "    elif 'vulnerability' in risk_types:\n",
    "        return 'rdls_vln'\n",
    "    elif 'loss' in risk_types:\n",
    "        return 'rdls_lss'\n",
    "    else:\n",
    "        return 'rdls_unk'\n",
    "\n",
    "print(\"Risk data type logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RDLS Record Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 HDX to RDLS General Metadata Mapper\n",
    "\"\"\"\n",
    "\n",
    "def map_hdx_to_rdls_general(\n",
    "    hdx_record: Dict[str, Any],\n",
    "    risk_data_types: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Map HDX metadata to RDLS general metadata fields.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hdx_record : Dict[str, Any]\n",
    "        Original HDX metadata\n",
    "    risk_data_types : List[str]\n",
    "        Determined risk_data_type values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        RDLS dataset record (general metadata only)\n",
    "    \"\"\"\n",
    "    dataset_id = hdx_record.get('id', '')\n",
    "    prefix = determine_filename_prefix(risk_data_types)\n",
    "    \n",
    "    rdls = {\n",
    "        'id': f\"{prefix}-hdx_{dataset_id[:8]}\",\n",
    "        'title': hdx_record.get('title', ''),\n",
    "        'description': hdx_record.get('notes', ''),\n",
    "        'risk_data_type': risk_data_types,\n",
    "    }\n",
    "    \n",
    "    # Version (default)\n",
    "    rdls['version'] = '1'\n",
    "    \n",
    "    # Spatial\n",
    "    groups = hdx_record.get('groups', [])\n",
    "    if groups:\n",
    "        # Try to determine scale from groups\n",
    "        if 'World' in groups:\n",
    "            rdls['spatial'] = {'scale': 'global'}\n",
    "        elif len(groups) > 3:\n",
    "            rdls['spatial'] = {'scale': 'regional'}\n",
    "        else:\n",
    "            rdls['spatial'] = {'scale': 'national'}\n",
    "    \n",
    "    # License mapping\n",
    "    license_map = {\n",
    "        'Creative Commons Attribution International': 'CC-BY-4.0',\n",
    "        'Creative Commons Attribution for Intergovernmental Organisations': 'CC-BY-IGO-3.0',\n",
    "        'Public Domain': 'CC0-1.0',\n",
    "        'Other': 'Other',\n",
    "    }\n",
    "    hdx_license = hdx_record.get('license_title', '')\n",
    "    rdls['license'] = license_map.get(hdx_license, hdx_license or 'Other')\n",
    "    \n",
    "    # Attributions\n",
    "    org = hdx_record.get('organization', '')\n",
    "    rdls['attributions'] = [\n",
    "        {\n",
    "            'id': 'attribution_publisher',\n",
    "            'role': 'publisher',\n",
    "            'entity': {'name': org or 'HDX'}\n",
    "        },\n",
    "        {\n",
    "            'id': 'attribution_creator',\n",
    "            'role': 'creator',\n",
    "            'entity': {'name': hdx_record.get('dataset_source', org or 'Unknown')}\n",
    "        },\n",
    "        {\n",
    "            'id': 'attribution_contact',\n",
    "            'role': 'contact_point',\n",
    "            'entity': {'name': org or 'HDX'}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Resources\n",
    "    resources = []\n",
    "    for i, res in enumerate(hdx_record.get('resources', [])):\n",
    "        rdls_res = {\n",
    "            'id': f\"resource_{dataset_id[:8]}_{i+1}\",\n",
    "            'title': res.get('name', ''),\n",
    "            'description': res.get('description', ''),\n",
    "            'data_format': res.get('format', ''),\n",
    "            'access_url': f\"https://data.humdata.org/dataset/{dataset_id}\",\n",
    "            'download_url': res.get('download_url', '')\n",
    "        }\n",
    "        resources.append(rdls_res)\n",
    "    \n",
    "    if resources:\n",
    "        rdls['resources'] = resources\n",
    "    \n",
    "    # Links (schema reference)\n",
    "    rdls['links'] = [{\n",
    "        'href': 'https://docs.riskdatalibrary.org/en/0__3__0/rdls_schema.json',\n",
    "        'rel': 'describedby'\n",
    "    }]\n",
    "    \n",
    "    return rdls\n",
    "\n",
    "print(\"General metadata mapper defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 HEVL Block Builders (Simplified versions)\n",
    "\"\"\"\n",
    "\n",
    "def build_hazard_block_simple(row: pd.Series, dataset_id: str) -> Optional[Dict]:\n",
    "    \"\"\"Build hazard block from extraction row.\"\"\"\n",
    "    hazard_types = row.get('hazard_types', '')\n",
    "    if not hazard_types or pd.isna(hazard_types):\n",
    "        return None\n",
    "    \n",
    "    types = hazard_types.split('|') if isinstance(hazard_types, str) else []\n",
    "    if not types:\n",
    "        return None\n",
    "    \n",
    "    hazards = []\n",
    "    for i, ht in enumerate(types):\n",
    "        hazards.append({\n",
    "            'id': f\"hazard_{dataset_id[:8]}_{i+1}\",\n",
    "            'type': ht\n",
    "        })\n",
    "    \n",
    "    event_set = {\n",
    "        'id': f\"event_set_{dataset_id[:8]}\",\n",
    "        'hazards': hazards\n",
    "    }\n",
    "    \n",
    "    # Add analysis type if available\n",
    "    analysis = row.get('hazard_analysis_type')\n",
    "    if analysis and not pd.isna(analysis):\n",
    "        event_set['analysis_type'] = analysis\n",
    "    \n",
    "    # Add return periods if available\n",
    "    rp_str = row.get('return_periods', '')\n",
    "    if rp_str and not pd.isna(rp_str):\n",
    "        rps = [int(x) for x in str(rp_str).split('|') if x.isdigit()]\n",
    "        if rps:\n",
    "            events = []\n",
    "            for rp in rps:\n",
    "                events.append({\n",
    "                    'id': f\"event_rp{rp}_{dataset_id[:8]}\",\n",
    "                    'calculation_method': 'simulated',\n",
    "                    'hazard': hazards[0],\n",
    "                    'occurrence': {\n",
    "                        'probabilistic': {'return_period': rp}\n",
    "                    }\n",
    "                })\n",
    "            event_set['events'] = events\n",
    "    \n",
    "    return {'event_sets': [event_set]}\n",
    "\n",
    "\n",
    "def build_exposure_block_simple(row: pd.Series, dataset_id: str) -> Optional[List[Dict]]:\n",
    "    \"\"\"Build exposure block from extraction row.\"\"\"\n",
    "    categories = row.get('exposure_categories', '')\n",
    "    if not categories or pd.isna(categories):\n",
    "        return None\n",
    "    \n",
    "    cats = categories.split('|') if isinstance(categories, str) else []\n",
    "    if not cats:\n",
    "        return None\n",
    "    \n",
    "    exposure_items = []\n",
    "    for i, cat in enumerate(cats):\n",
    "        item = {\n",
    "            'id': f\"exposure_{dataset_id[:8]}_{i+1}\",\n",
    "            'category': cat,\n",
    "            'metrics': [{\n",
    "                'id': f\"metric_{dataset_id[:8]}_{i+1}\",\n",
    "                'dimension': 'Structure' if cat == 'buildings' else 'Other',\n",
    "                'quantity_kind': 'count'\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        taxonomy = row.get('exposure_taxonomy')\n",
    "        if taxonomy and not pd.isna(taxonomy):\n",
    "            item['taxonomy'] = taxonomy\n",
    "        \n",
    "        exposure_items.append(item)\n",
    "    \n",
    "    return exposure_items\n",
    "\n",
    "print(\"HEVL block builders defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.3 Complete RDLS Record Builder\n",
    "\"\"\"\n",
    "\n",
    "def build_complete_rdls_record(\n",
    "    hdx_record: Dict[str, Any],\n",
    "    extraction_row: pd.Series\n",
    ") -> Tuple[Optional[Dict], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Build complete RDLS record with HEVL blocks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Optional[Dict], Optional[str]]\n",
    "        (RDLS record, block_reason if blocked)\n",
    "    \"\"\"\n",
    "    # Check if blocked\n",
    "    if extraction_row.get('blocked', False):\n",
    "        return None, extraction_row.get('block_reason', 'unknown')\n",
    "    \n",
    "    # Determine risk data types\n",
    "    risk_types = determine_risk_data_types(extraction_row)\n",
    "    \n",
    "    if not risk_types:\n",
    "        return None, 'no_hevl_signals'\n",
    "    \n",
    "    dataset_id = hdx_record.get('id', '')\n",
    "    \n",
    "    # Build general metadata\n",
    "    rdls_dataset = map_hdx_to_rdls_general(hdx_record, risk_types)\n",
    "    \n",
    "    # Add HEVL blocks\n",
    "    if extraction_row.get('include_hazard', False):\n",
    "        hazard_block = build_hazard_block_simple(extraction_row, dataset_id)\n",
    "        if hazard_block:\n",
    "            rdls_dataset['hazard'] = hazard_block\n",
    "    \n",
    "    if extraction_row.get('include_exposure', False):\n",
    "        exposure_block = build_exposure_block_simple(extraction_row, dataset_id)\n",
    "        if exposure_block:\n",
    "            rdls_dataset['exposure'] = exposure_block\n",
    "    \n",
    "    # Vulnerability and Loss blocks are more complex\n",
    "    # For now, just flag that they're present\n",
    "    if extraction_row.get('include_vulnerability', False):\n",
    "        rdls_dataset['vulnerability'] = {\n",
    "            'functions': {},\n",
    "            '_note': 'Vulnerability data detected - requires manual enrichment'\n",
    "        }\n",
    "    \n",
    "    if extraction_row.get('include_loss', False):\n",
    "        rdls_dataset['loss'] = {\n",
    "            'losses': [],\n",
    "            '_note': 'Loss data detected - requires manual enrichment'\n",
    "        }\n",
    "    \n",
    "    return {'datasets': [rdls_dataset]}, None\n",
    "\n",
    "print(\"Complete record builder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Integrated RDLS Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Process Records\n",
    "\"\"\"\n",
    "\n",
    "def process_integration(\n",
    "    merged_df: pd.DataFrame,\n",
    "    metadata_dir: Path,\n",
    "    output_dir: Path,\n",
    "    limit: Optional[int] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process all records and generate integrated RDLS files.\n",
    "    \"\"\"\n",
    "    # Filter to records with any HEVL signal\n",
    "    hevl_mask = (\n",
    "        merged_df.get('has_hazard', False) |\n",
    "        merged_df.get('has_exposure', False) |\n",
    "        merged_df.get('has_vulnerability', False) |\n",
    "        merged_df.get('has_loss', False)\n",
    "    )\n",
    "    \n",
    "    df_to_process = merged_df[hevl_mask].copy()\n",
    "    \n",
    "    if limit:\n",
    "        df_to_process = df_to_process.head(limit)\n",
    "    \n",
    "    stats = {\n",
    "        'total_candidates': len(df_to_process),\n",
    "        'success': 0,\n",
    "        'blocked': 0,\n",
    "        'errors': 0,\n",
    "        'by_type': {'hazard': 0, 'exposure': 0, 'vulnerability': 0, 'loss': 0}\n",
    "    }\n",
    "    \n",
    "    blocked_records = []\n",
    "    \n",
    "    iterator = tqdm(df_to_process.iterrows(), total=len(df_to_process), desc=\"Integrating\") if HAS_TQDM else df_to_process.iterrows()\n",
    "    \n",
    "    for idx, row in iterator:\n",
    "        dataset_id = row.get('id', '')\n",
    "        if not dataset_id:\n",
    "            continue\n",
    "        \n",
    "        # Find HDX metadata file\n",
    "        hdx_files = list(metadata_dir.glob(f\"{dataset_id}__*.json\"))\n",
    "        if not hdx_files:\n",
    "            stats['errors'] += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(hdx_files[0], 'r', encoding='utf-8') as f:\n",
    "                hdx_record = json.load(f)\n",
    "            \n",
    "            rdls_record, block_reason = build_complete_rdls_record(hdx_record, row)\n",
    "            \n",
    "            if rdls_record:\n",
    "                # Determine filename\n",
    "                risk_types = rdls_record['datasets'][0].get('risk_data_type', [])\n",
    "                prefix = determine_filename_prefix(risk_types)\n",
    "                filename = f\"{prefix}-hdx_{dataset_id[:8]}.json\"\n",
    "                \n",
    "                # Save\n",
    "                output_path = output_dir / filename\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(rdls_record, f, indent=2)\n",
    "                \n",
    "                stats['success'] += 1\n",
    "                for rt in risk_types:\n",
    "                    stats['by_type'][rt] = stats['by_type'].get(rt, 0) + 1\n",
    "            else:\n",
    "                stats['blocked'] += 1\n",
    "                blocked_records.append({\n",
    "                    'id': dataset_id,\n",
    "                    'title': row.get('title', ''),\n",
    "                    'reason': block_reason\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            stats['errors'] += 1\n",
    "    \n",
    "    # Save blocked records report\n",
    "    if blocked_records:\n",
    "        blocked_df = pd.DataFrame(blocked_records)\n",
    "        blocked_df.to_csv(output_dir / 'integration_blocked.csv', index=False)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Process\n",
    "PROCESS_LIMIT = 100  # Set to None for full processing\n",
    "\n",
    "if len(df_merged) > 0:\n",
    "    print(f\"Processing up to {PROCESS_LIMIT or 'all'} records...\")\n",
    "    stats = process_integration(df_merged, DATASET_METADATA_DIR, OUTPUT_DIR, limit=PROCESS_LIMIT)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"INTEGRATION COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total candidates: {stats['total_candidates']}\")\n",
    "    print(f\"Successfully created: {stats['success']}\")\n",
    "    print(f\"Blocked: {stats['blocked']}\")\n",
    "    print(f\"Errors: {stats['errors']}\")\n",
    "    print(f\"\\nBy component type:\")\n",
    "    for t, count in stats['by_type'].items():\n",
    "        print(f\"  {t}: {count}\")\n",
    "else:\n",
    "    print(\"No merged data to process. Run notebooks 09-11 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Index and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Generate RDLS Index\n",
    "\"\"\"\n",
    "\n",
    "def generate_rdls_index(output_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate index of all RDLS records.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for filepath in output_dir.glob('rdls_*.json'):\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            dataset = data.get('datasets', [{}])[0]\n",
    "            \n",
    "            records.append({\n",
    "                'filename': filepath.name,\n",
    "                'id': dataset.get('id', ''),\n",
    "                'title': dataset.get('title', ''),\n",
    "                'risk_data_type': '|'.join(dataset.get('risk_data_type', [])),\n",
    "                'has_hazard': 'hazard' in dataset,\n",
    "                'has_exposure': 'exposure' in dataset,\n",
    "                'has_vulnerability': 'vulnerability' in dataset,\n",
    "                'has_loss': 'loss' in dataset,\n",
    "                'license': dataset.get('license', ''),\n",
    "                'resource_count': len(dataset.get('resources', []))\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Save index\n",
    "    df.to_csv(output_dir / 'rdls_index.csv', index=False)\n",
    "    \n",
    "    # Also save as JSONL\n",
    "    with open(output_dir / 'rdls_index.jsonl', 'w', encoding='utf-8') as f:\n",
    "        for record in records:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_index = generate_rdls_index(OUTPUT_DIR)\n",
    "print(f\"Generated index with {len(df_index)} records.\")\n",
    "print(f\"Saved to: {OUTPUT_DIR / 'rdls_index.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.2 Summary Report\n",
    "\"\"\"\n",
    "\n",
    "if len(df_index) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"INTEGRATED RDLS RECORDS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal records: {len(df_index)}\")\n",
    "    print(f\"\\nBy primary component:\")\n",
    "    print(df_index['risk_data_type'].value_counts())\n",
    "    print(f\"\\nComponent coverage:\")\n",
    "    print(f\"  With hazard block: {df_index['has_hazard'].sum()}\")\n",
    "    print(f\"  With exposure block: {df_index['has_exposure'].sum()}\")\n",
    "    print(f\"  With vulnerability block: {df_index['has_vulnerability'].sum()}\")\n",
    "    print(f\"  With loss block: {df_index['has_loss'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
