{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 05: RDLS Classification Review & Overrides\n",
    "\n",
    "**Purpose**: Provide a human QA loop for the machine classification from Notebook 04.\n",
    "\n",
    "**Process**:\n",
    "1. Build a review pack (CSV) prioritizing low/medium confidence candidates\n",
    "2. Capture human decisions (keep/exclude/adjust) in structured format\n",
    "3. Convert reviewed CSV to `config/overrides.yaml`\n",
    "4. Apply overrides to produce final classification table\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# PyYAML for config files\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Missing dependency: pyyaml. Install with: pip install pyyaml\") from e\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input/Output directories\n",
    "DUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DERIVED_DIR = DUMP_DIR / 'derived'\n",
    "POLICY_DIR = DUMP_DIR / 'policy'\n",
    "CONFIG_DIR = DUMP_DIR / 'config'\n",
    "REVIEW_DIR = DERIVED_DIR / 'review'\n",
    "\n",
    "# Input files from Notebook 04\n",
    "CLASSIFICATION_CSV = DERIVED_DIR / 'classification.csv'\n",
    "OSM_EXCLUDED_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n",
    "\n",
    "# Output files\n",
    "REVIEW_PACK_CSV = REVIEW_DIR / 'review_pack.csv'\n",
    "OVERRIDES_YAML = CONFIG_DIR / 'overrides.yaml'\n",
    "CLASSIFICATION_FINAL_CSV = DERIVED_DIR / 'classification_final.csv'\n",
    "CLASSIFICATION_FINAL_SUMMARY_JSON = DERIVED_DIR / 'classification_final_summary.json'\n",
    "RDLS_INCLUDED_IDS_FINAL_TXT = DERIVED_DIR / 'rdls_included_dataset_ids_final.txt'\n",
    "\n",
    "# Create directories\n",
    "REVIEW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {CLASSIFICATION_CSV}\")\n",
    "print(f\"Output: {REVIEW_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Configuration Parameters\n",
    "\"\"\"\n",
    "\n",
    "# Policy: do NOT allow humans to override OSM exclusion by default\n",
    "ALLOW_OSM_OVERRIDE = False\n",
    "\n",
    "# Review pack size (tune based on review capacity)\n",
    "REVIEW_PACK_SIZE = 1500\n",
    "\n",
    "# Prioritize low/medium confidence for review\n",
    "PRIORITIZE_CONFIDENCE = ('low', 'medium')\n",
    "\n",
    "# Required columns in classification CSV\n",
    "REQUIRED_COLUMNS = [\n",
    "    'dataset_id', 'title', 'organization', 'dataset_source', 'license_title',\n",
    "    'tags', 'groups', 'formats', 'excluded_by_policy', 'rdls_candidate',\n",
    "    'rdls_components', 'confidence', 'score_hazard', 'score_exposure',\n",
    "    'score_vulnerability_proxy', 'score_loss_impact',\n",
    "]\n",
    "\n",
    "print(f\"Review pack size: {REVIEW_PACK_SIZE}\")\n",
    "print(f\"Allow OSM override: {ALLOW_OSM_OVERRIDE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "def _to_bool_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Robust bool coercion for CSV roundtrips.\"\"\"\n",
    "    return (\n",
    "        s.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .map({'true': True, 'false': False, '1': True, '0': False, 'yes': True, 'no': False})\n",
    "        .fillna(False)\n",
    "        .astype(bool)\n",
    "    )\n",
    "\n",
    "\n",
    "def load_classification_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and validate classification CSV.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Path to classification CSV\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Classification dataframe\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing Step 4 output: {path}\")\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "    \n",
    "    # Coerce boolean columns\n",
    "    for col in ['excluded_by_policy', 'rdls_candidate']:\n",
    "        df[col] = _to_bool_series(df[col])\n",
    "    \n",
    "    df['confidence'] = df['confidence'].fillna('unknown').astype(str).str.strip().str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_osm_excluded_ids(path: Path) -> set:\n",
    "    \"\"\"Load OSM exclusion list.\"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: OSM exclusion list not found: {path}\")\n",
    "        return set()\n",
    "    \n",
    "    ids = set()\n",
    "    for line in path.read_text(encoding='utf-8').splitlines():\n",
    "        s = line.strip()\n",
    "        if s:\n",
    "            ids.add(s)\n",
    "    return ids\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Load Classification and OSM Exclusion Data\n",
    "\"\"\"\n",
    "\n",
    "osm_excluded_ids = load_osm_excluded_ids(OSM_EXCLUDED_IDS_TXT)\n",
    "df = load_classification_table(CLASSIFICATION_CSV)\n",
    "\n",
    "# Derive is_osm column if not present\n",
    "if 'is_osm' not in df.columns:\n",
    "    df['is_osm'] = df['dataset_id'].astype(str).isin(osm_excluded_ids)\n",
    "else:\n",
    "    df['is_osm'] = _to_bool_series(df['is_osm'])\n",
    "\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique dataset_id: {df['dataset_id'].nunique():,}\")\n",
    "print(f\"OSM excluded IDs: {len(osm_excluded_ids):,}\")\n",
    "print(f\"OSM flagged in data: {df['is_osm'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Build Review Pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Generate Review Pack CSV\n",
    "\n",
    "Creates a subset for manual review prioritizing low/medium confidence candidates.\n",
    "\"\"\"\n",
    "\n",
    "REVIEW_COLUMNS = [\n",
    "    'dataset_id', 'title', 'organization', 'dataset_source', 'license_title',\n",
    "    'tags', 'groups', 'formats', 'is_osm', 'excluded_by_policy', 'rdls_candidate',\n",
    "    'rdls_components', 'confidence', 'score_hazard', 'score_exposure',\n",
    "    'score_vulnerability_proxy', 'score_loss_impact', 'top_signals',\n",
    "]\n",
    "\n",
    "# Filter: only candidates that are currently included\n",
    "eligible = df[(df['rdls_candidate'] == True) & (df['excluded_by_policy'] == False)].copy()\n",
    "\n",
    "# Calculate total score\n",
    "eligible['total_score'] = (\n",
    "    eligible['score_hazard'] + eligible['score_exposure'] +\n",
    "    eligible['score_vulnerability_proxy'] + eligible['score_loss_impact']\n",
    ")\n",
    "\n",
    "# Prioritize low/medium confidence\n",
    "priority = eligible[eligible['confidence'].isin(PRIORITIZE_CONFIDENCE)].copy()\n",
    "priority = priority.sort_values(['confidence', 'total_score'], ascending=[True, False])\n",
    "review_pack = priority.head(REVIEW_PACK_SIZE)\n",
    "\n",
    "# Top up with high confidence if needed\n",
    "if len(review_pack) < REVIEW_PACK_SIZE:\n",
    "    remaining = eligible[~eligible.index.isin(review_pack.index)].sort_values('total_score', ascending=False)\n",
    "    review_pack = pd.concat([review_pack, remaining.head(REVIEW_PACK_SIZE - len(review_pack))], ignore_index=True)\n",
    "\n",
    "# Keep only available columns\n",
    "available_cols = [c for c in REVIEW_COLUMNS if c in review_pack.columns]\n",
    "review_pack = review_pack[available_cols].copy()\n",
    "\n",
    "# Add empty human-edit fields\n",
    "review_pack['decision'] = ''  # keep | exclude | unsure\n",
    "review_pack['components_override'] = ''  # e.g. hazard,exposure\n",
    "review_pack['notes'] = ''\n",
    "\n",
    "review_pack.to_csv(REVIEW_PACK_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Wrote: {REVIEW_PACK_CSV}\")\n",
    "print(f\"Review pack size: {len(review_pack):,}\")\n",
    "print(f\"\\nConfidence breakdown:\")\n",
    "print(review_pack['confidence'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Convert to Overrides YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Parse Reviewed CSV and Generate overrides.yaml\n",
    "\n",
    "Workflow:\n",
    "1. Open review_pack.csv in Excel/VS Code\n",
    "2. Fill decision (keep/exclude/unsure) and optional components_override\n",
    "3. Re-run this cell to generate overrides.yaml\n",
    "\"\"\"\n",
    "\n",
    "VALID_DECISIONS = {'keep', 'exclude', 'unsure', ''}\n",
    "\n",
    "def parse_components_list(s: str) -> List[str]:\n",
    "    \"\"\"Parse comma-separated components list.\"\"\"\n",
    "    parts = [p.strip().lower() for p in str(s).split(',') if p.strip()]\n",
    "    # Deduplicate preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if p not in seen:\n",
    "            out.append(p)\n",
    "            seen.add(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Load reviewed CSV\n",
    "reviewed = pd.read_csv(REVIEW_PACK_CSV).fillna('')\n",
    "reviewed['decision'] = reviewed['decision'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Validate decisions\n",
    "bad = reviewed[~reviewed['decision'].isin(VALID_DECISIONS)]\n",
    "if len(bad) > 0:\n",
    "    raise ValueError(\n",
    "        f\"Invalid decision values found. Allowed: keep, exclude, unsure, blank.\\n\"\n",
    "        + bad[['dataset_id', 'decision']].head(20).to_string(index=False)\n",
    "    )\n",
    "\n",
    "# Build overrides dict\n",
    "overrides: Dict[str, Any] = {'overrides': {}}\n",
    "\n",
    "for _, r in reviewed.iterrows():\n",
    "    dsid = str(r['dataset_id']).strip()\n",
    "    decision = str(r['decision']).strip().lower()\n",
    "    \n",
    "    if not dsid or not decision or decision == 'unsure':\n",
    "        continue\n",
    "    \n",
    "    entry: Dict[str, Any] = {'decision': decision}\n",
    "    \n",
    "    comps = parse_components_list(r.get('components_override', ''))\n",
    "    if comps:\n",
    "        entry['components'] = comps\n",
    "    \n",
    "    notes = str(r.get('notes', '')).strip()\n",
    "    if notes:\n",
    "        entry['notes'] = notes\n",
    "    \n",
    "    overrides['overrides'][dsid] = entry\n",
    "\n",
    "# Write YAML\n",
    "with OVERRIDES_YAML.open('w', encoding='utf-8') as f:\n",
    "    yaml.safe_dump(overrides, f, sort_keys=True, allow_unicode=True)\n",
    "\n",
    "print(f\"Wrote: {OVERRIDES_YAML}\")\n",
    "print(f\"Override entries: {len(overrides['overrides']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Apply Overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Apply Overrides to Full Classification\n",
    "\"\"\"\n",
    "\n",
    "def load_overrides(path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Load overrides YAML file.\"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: overrides file not found: {path}\")\n",
    "        return {}\n",
    "    data = yaml.safe_load(path.read_text(encoding='utf-8')) or {}\n",
    "    return data.get('overrides', {}) or {}\n",
    "\n",
    "\n",
    "def _parse_components(s: Any) -> set:\n",
    "    \"\"\"Parse components string to set.\"\"\"\n",
    "    parts = [p.strip().lower() for p in str(s).split(',') if p.strip()]\n",
    "    return set(parts)\n",
    "\n",
    "\n",
    "def _join_components(s: set) -> str:\n",
    "    \"\"\"Join components in standard order.\"\"\"\n",
    "    order = ['hazard', 'exposure', 'vulnerability_proxy', 'loss_impact']\n",
    "    return ','.join([c for c in order if c in s])\n",
    "\n",
    "\n",
    "# Load overrides\n",
    "overrides_map = load_overrides(OVERRIDES_YAML)\n",
    "print(f\"Loaded {len(overrides_map):,} overrides\")\n",
    "\n",
    "# Create final dataframe\n",
    "final = df.copy()\n",
    "\n",
    "# Add override tracking columns\n",
    "final['override_decision'] = ''\n",
    "final['override_components'] = ''\n",
    "final['excluded_by_override'] = False\n",
    "\n",
    "# Apply overrides\n",
    "for i, r in final.iterrows():\n",
    "    dsid = r['dataset_id']\n",
    "    ov = overrides_map.get(dsid)\n",
    "    if not ov:\n",
    "        continue\n",
    "    \n",
    "    decision = str(ov.get('decision', '')).strip().lower()\n",
    "    comps = ov.get('components', None)\n",
    "    \n",
    "    if decision in {'exclude', 'keep'}:\n",
    "        final.at[i, 'override_decision'] = decision\n",
    "    \n",
    "    if isinstance(comps, list) and comps:\n",
    "        final.at[i, 'override_components'] = ','.join([str(c).lower() for c in comps])\n",
    "    \n",
    "    if decision == 'exclude':\n",
    "        final.at[i, 'excluded_by_override'] = True\n",
    "    \n",
    "    if decision == 'keep':\n",
    "        final.at[i, 'rdls_candidate'] = True\n",
    "        if isinstance(comps, list) and comps:\n",
    "            final.at[i, 'rdls_components'] = ','.join([str(c).lower() for c in comps])\n",
    "\n",
    "# Final exclusion: policy OR override-exclude\n",
    "final['final_excluded'] = final['excluded_by_policy'] | final['excluded_by_override']\n",
    "\n",
    "print(\"Overrides applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Apply Component Normalization Rules\n",
    "\n",
    "Rules:\n",
    "- vulnerability_proxy requires hazard or exposure\n",
    "- loss_impact requires hazard or exposure\n",
    "If violated, auto-add exposure.\n",
    "\"\"\"\n",
    "\n",
    "final['components_normalized'] = False\n",
    "final['components_normalization_notes'] = ''\n",
    "\n",
    "for i, r in final.iterrows():\n",
    "    if not bool(r.get('rdls_candidate', False)):\n",
    "        continue\n",
    "    if bool(r.get('final_excluded', False)):\n",
    "        continue\n",
    "    \n",
    "    comps = _parse_components(r.get('rdls_components', ''))\n",
    "    if not comps:\n",
    "        continue\n",
    "    \n",
    "    notes = []\n",
    "    \n",
    "    # vulnerability_proxy requires hazard or exposure\n",
    "    if 'vulnerability_proxy' in comps and not ({'hazard', 'exposure'} & comps):\n",
    "        comps.add('exposure')\n",
    "        notes.append('added_exposure_for_vulnerability_proxy')\n",
    "    \n",
    "    # loss_impact requires hazard or exposure\n",
    "    if 'loss_impact' in comps and not ({'hazard', 'exposure'} & comps):\n",
    "        comps.add('exposure')\n",
    "        notes.append('added_exposure_for_loss_impact')\n",
    "    \n",
    "    if notes:\n",
    "        final.at[i, 'rdls_components'] = _join_components(comps)\n",
    "        final.at[i, 'components_normalized'] = True\n",
    "        final.at[i, 'components_normalization_notes'] = ';'.join(notes)\n",
    "\n",
    "# Enforce OSM policy if not allowing overrides\n",
    "if not ALLOW_OSM_OVERRIDE and len(osm_excluded_ids) > 0:\n",
    "    mask_illegal = final['dataset_id'].isin(osm_excluded_ids) & (final['override_decision'] == 'keep')\n",
    "    illegal_count = int(mask_illegal.sum())\n",
    "    if illegal_count > 0:\n",
    "        print(f\"WARNING: {illegal_count} override(s) tried to include OSM-excluded datasets. Reverting.\")\n",
    "        final.loc[mask_illegal, 'final_excluded'] = True\n",
    "\n",
    "# Final included set\n",
    "final['final_included'] = final['rdls_candidate'] & (~final['final_excluded'])\n",
    "\n",
    "print(f\"Component normalization complete.\")\n",
    "print(f\"Normalized: {final['components_normalized'].sum():,} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Save Final Classification and Summary\n",
    "\"\"\"\n",
    "\n",
    "# Write final classification\n",
    "final.to_csv(CLASSIFICATION_FINAL_CSV, index=False, encoding='utf-8')\n",
    "print(f\"Wrote: {CLASSIFICATION_FINAL_CSV}\")\n",
    "\n",
    "# Write included IDs\n",
    "included_ids = final.loc[final['final_included'], 'dataset_id'].astype(str).tolist()\n",
    "RDLS_INCLUDED_IDS_FINAL_TXT.write_text('\\n'.join(included_ids) + '\\n', encoding='utf-8')\n",
    "print(f\"Wrote: {RDLS_INCLUDED_IDS_FINAL_TXT} ({len(included_ids):,} IDs)\")\n",
    "\n",
    "# Generate summary\n",
    "summary = {\n",
    "    'total_datasets': int(len(final)),\n",
    "    'policy': {\n",
    "        'osm_excluded_ids_loaded': int(len(osm_excluded_ids)),\n",
    "        'datasets_excluded_by_policy': int(final['excluded_by_policy'].sum()),\n",
    "    },\n",
    "    'overrides': {\n",
    "        'override_entries_loaded': int(len(overrides_map)),\n",
    "        'datasets_excluded_by_override': int(final['excluded_by_override'].sum()),\n",
    "        'datasets_with_component_override': int((final['override_components'].astype(str) != '').sum()),\n",
    "    },\n",
    "    'rdls': {\n",
    "        'candidates_total': int(final['rdls_candidate'].sum()),\n",
    "        'included_total': int(final['final_included'].sum()),\n",
    "    },\n",
    "    'confidence_counts': final['confidence'].value_counts().to_dict(),\n",
    "    'component_nonzero_counts': {\n",
    "        'hazard': int((final['score_hazard'] > 0).sum()),\n",
    "        'exposure': int((final['score_exposure'] > 0).sum()),\n",
    "        'vulnerability_proxy': int((final['score_vulnerability_proxy'] > 0).sum()),\n",
    "        'loss_impact': int((final['score_loss_impact'] > 0).sum()),\n",
    "    },\n",
    "}\n",
    "\n",
    "CLASSIFICATION_FINAL_SUMMARY_JSON.write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "print(f\"Wrote: {CLASSIFICATION_FINAL_SUMMARY_JSON}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL CLASSIFICATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total datasets: {summary['total_datasets']:,}\")\n",
    "print(f\"RDLS candidates: {summary['rdls']['candidates_total']:,}\")\n",
    "print(f\"Final included: {summary['rdls']['included_total']:,}\")\n",
    "print(f\"Excluded by OSM policy: {summary['policy']['datasets_excluded_by_policy']:,}\")\n",
    "print(f\"Excluded by override: {summary['overrides']['datasets_excluded_by_override']:,}\")\n",
    "\n",
    "print(f\"\\nNext: Run Notebook 06 to translate to RDLS schema.\")\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
