{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2d2575",
   "metadata": {},
   "source": [
    "# Step 5 - RDLS Classification Review & Overrides (HDX → RDLS)\n",
    "\n",
    "**Goal:** add a *defensible* human QA loop to the Step 4 machine classification.\n",
    "\n",
    "This notebook helps you:\n",
    "1) Build a **review pack** (CSV) prioritizing low/medium-confidence RDLS candidates (excluding policy-excluded OSM by default).\n",
    "2) Capture human decisions (keep / exclude / adjust components) in a structured way.\n",
    "3) Convert the reviewed CSV into `config/overrides.yaml`.\n",
    "4) Apply overrides to the full classification to produce a **final** classification table + final included IDs list.\n",
    "\n",
    "## Inputs\n",
    "- `derived/classification.csv` (from Step 4)\n",
    "- `policy/osm_excluded_dataset_ids.txt` (from Step 2) — used indirectly because Step 4 already applied it\n",
    "- Optional existing `config/overrides.yaml` (if you re-run)\n",
    "\n",
    "## Outputs\n",
    "- `derived/review/review_pack.csv` — edit this file (in Excel/VS Code) and fill the decision columns\n",
    "- `config/overrides.yaml` — machine-readable override rules\n",
    "- `derived/classification_final.csv` — Step 4 classification with overrides applied\n",
    "- `derived/classification_final_summary.json`\n",
    "- `derived/rdls_included_dataset_ids_final.txt`\n",
    "\n",
    "## Override policy\n",
    "- **OSM exclusion remains authoritative by default.** If an override tries to include an OSM-excluded dataset, it will be ignored unless you explicitly enable `ALLOW_OSM_OVERRIDE = True` in the config cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c789d46-670d-49e6-ba51-14e1fbb33986",
   "metadata": {},
   "source": [
    "### Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7031a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\n",
      "DUMP_DIR     : C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\n",
      "CLASSIFICATION_CSV exists? True\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration & file paths\n",
    "# =========================\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import yaml  # PyYAML\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"PyYAML is required for this notebook. Install with: pip install pyyaml\"\n",
    "    ) from e\n",
    "\n",
    "\n",
    "# When you run the notebook from ./notebooks/, we want the project root to be the parent.\n",
    "# This makes the notebook robust to being executed from different working directories.\n",
    "def guess_project_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    if cwd.name.lower() in {\"notebook\", \"notebooks\"}:\n",
    "        return cwd.parent\n",
    "    return cwd\n",
    "\n",
    "PROJECT_ROOT = guess_project_root()\n",
    "\n",
    "# Root directory from Step 1 output\n",
    "DUMP_DIR = PROJECT_ROOT / \"hdx_dataset_metadata_dump\"\n",
    "\n",
    "# Step 4 outputs live here\n",
    "DERIVED_DIR = DUMP_DIR / \"derived\"\n",
    "CLASSIFICATION_CSV = DERIVED_DIR / \"classification.csv\"\n",
    "\n",
    "# Step 2 outputs live here\n",
    "POLICY_DIR = DUMP_DIR / \"policy\"\n",
    "OSM_EXCLUDED_IDS_TXT = POLICY_DIR / \"osm_excluded_dataset_ids.txt\"\n",
    "\n",
    "# Step 5 outputs\n",
    "REVIEW_DIR = DERIVED_DIR / \"review\"\n",
    "REVIEW_PACK_CSV = REVIEW_DIR / \"review_pack.csv\"\n",
    "\n",
    "# Overrides config\n",
    "CONFIG_DIR = DUMP_DIR / \"config\"\n",
    "OVERRIDES_YAML = CONFIG_DIR / \"overrides.yaml\"\n",
    "\n",
    "# Final outputs\n",
    "CLASSIFICATION_FINAL_CSV = DERIVED_DIR / \"classification_final.csv\"\n",
    "CLASSIFICATION_FINAL_SUMMARY_JSON = DERIVED_DIR / \"classification_final_summary.json\"\n",
    "RDLS_INCLUDED_IDS_FINAL_TXT = DERIVED_DIR / \"rdls_included_dataset_ids_final.txt\"\n",
    "\n",
    "# Policy switch: by default, do NOT allow humans to override the OSM exclusion decision.\n",
    "# (Enable only for a curated pilot track.)\n",
    "ALLOW_OSM_OVERRIDE = False\n",
    "\n",
    "\n",
    "# How many records to include in the review pack (tune based on your review capacity)\n",
    "REVIEW_PACK_SIZE = 1500\n",
    "\n",
    "# Prioritize RDLS candidates with low/medium confidence and not policy-excluded\n",
    "PRIORITIZE_CONFIDENCE = (\"low\", \"medium\")\n",
    "\n",
    "\n",
    "# Create output folders\n",
    "REVIEW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DUMP_DIR     :\", DUMP_DIR)\n",
    "print(\"CLASSIFICATION_CSV exists?\", CLASSIFICATION_CSV.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b7824-4547-4e2c-8465-4390c0c49c0f",
   "metadata": {},
   "source": [
    "### Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2390f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 26246\n",
      "Unique dataset_id: 26246\n",
      "OSM excluded IDs loaded: 3649\n",
      "Derived is_osm True: 3649\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# Helpers: load & validate\n",
    "# ======================\n",
    "import re\n",
    "\n",
    "REQUIRED_COLUMNS = [\n",
    "    \"dataset_id\",\n",
    "    \"title\",\n",
    "    \"organization\",\n",
    "    \"dataset_source\",\n",
    "    \"license_title\",\n",
    "    \"tags\",\n",
    "    \"groups\",\n",
    "    \"formats\",\n",
    "    \"excluded_by_policy\",\n",
    "    \"rdls_candidate\",\n",
    "    \"rdls_components\",\n",
    "    \"confidence\",\n",
    "    \"score_hazard\",\n",
    "    \"score_exposure\",\n",
    "    \"score_vulnerability_proxy\",\n",
    "    \"score_loss_impact\",\n",
    "]\n",
    "\n",
    "def _to_bool_series(s):\n",
    "    \"\"\"Robust bool coercion for CSV roundtrips (handles True/False/blank).\"\"\"\n",
    "    return (\n",
    "        s.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .map({\"true\": True, \"false\": False, \"1\": True, \"0\": False, \"yes\": True, \"no\": False})\n",
    "        .fillna(False)\n",
    "        .astype(bool)\n",
    "    )\n",
    "\n",
    "def load_classification_table(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing Step 4 output: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            \"classification.csv is missing required columns: \"\n",
    "            + \", \".join(missing)\n",
    "            + \"\\n\"\n",
    "            + \"Did Step 4 run completely with the expected version?\"\n",
    "        )\n",
    "\n",
    "    # Coerce only the columns we *know* exist here.\n",
    "    for col in [\"excluded_by_policy\", \"rdls_candidate\"]:\n",
    "        df[col] = _to_bool_series(df[col])\n",
    "\n",
    "    df[\"confidence\"] = df[\"confidence\"].fillna(\"unknown\").astype(str).str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_osm_excluded_ids(path: Path) -> set[str]:\n",
    "    # Note: Step 4 already applied this policy, but we load it to enforce \"no override include\" by default.\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: OSM exclusion list not found: {path}. Proceeding with empty exclusion set.\")\n",
    "        return set()\n",
    "\n",
    "    ids = set()\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        s = line.strip()\n",
    "        if s:\n",
    "            ids.add(s)\n",
    "    return ids\n",
    "\n",
    "\n",
    "# --- Load inputs ---\n",
    "osm_excluded_ids = load_osm_excluded_ids(OSM_EXCLUDED_IDS_TXT)\n",
    "df = load_classification_table(CLASSIFICATION_CSV)\n",
    "\n",
    "# --- Derive is_osm (optional column) ---\n",
    "# If Step 4 didn't export `is_osm`, compute it from Step 2 exclusion list.\n",
    "if \"is_osm\" not in df.columns:\n",
    "    df[\"is_osm\"] = df[\"dataset_id\"].astype(str).isin(osm_excluded_ids)\n",
    "else:\n",
    "    df[\"is_osm\"] = _to_bool_series(df[\"is_osm\"])\n",
    "\n",
    "# Defensive: ensure booleans are correct (again, but now `is_osm` exists)\n",
    "df[\"excluded_by_policy\"] = _to_bool_series(df[\"excluded_by_policy\"])\n",
    "df[\"rdls_candidate\"] = _to_bool_series(df[\"rdls_candidate\"])\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Unique dataset_id:\", df[\"dataset_id\"].nunique())\n",
    "print(\"OSM excluded IDs loaded:\", len(osm_excluded_ids))\n",
    "print(\"Derived is_osm True:\", int(df[\"is_osm\"].sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa913d-14f7-40b6-afe2-49c9bfa4c02b",
   "metadata": {},
   "source": [
    "### Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4442d45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote review pack: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\derived\\review\\review_pack.csv (rows=1500)\n",
      "\n",
      "Review pack breakdown:\n",
      "confidence\n",
      "medium    1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Build a review pack (CSV) for human labeling\n",
    "# ==========================================\n",
    "#\n",
    "# The review pack is a subset of datasets you will manually check.\n",
    "# You will edit the output CSV and fill these columns:\n",
    "#\n",
    "# - decision: keep | exclude | unsure\n",
    "# - components_override: comma-separated list, e.g. \"hazard,exposure\"\n",
    "# - notes: free text, why you changed it\n",
    "#\n",
    "# The notebook will then convert that CSV into config/overrides.yaml.\n",
    "\n",
    "REVIEW_COLUMNS = [\n",
    "    \"dataset_id\",\n",
    "    \"title\",\n",
    "    \"organization\",\n",
    "    \"dataset_source\",\n",
    "    \"license_title\",\n",
    "    \"tags\",\n",
    "    \"groups\",\n",
    "    \"formats\",\n",
    "    \"url\",\n",
    "    \"is_osm\",\n",
    "    \"excluded_by_policy\",\n",
    "    \"rdls_candidate\",\n",
    "    \"rdls_components\",\n",
    "    \"confidence\",\n",
    "    \"score_hazard\",\n",
    "    \"score_exposure\",\n",
    "    \"score_vulnerability_proxy\",\n",
    "    \"score_loss_impact\",\n",
    "    \"top_signals\",\n",
    "]\n",
    "\n",
    "# Filter: only candidates that are currently included\n",
    "eligible = df[(df[\"rdls_candidate\"] == True) & (df[\"excluded_by_policy\"] == False)].copy()\n",
    "\n",
    "# Prioritize by confidence, then by total score descending\n",
    "eligible[\"total_score\"] = (\n",
    "    eligible[\"score_hazard\"]\n",
    "    + eligible[\"score_exposure\"]\n",
    "    + eligible[\"score_vulnerability_proxy\"]\n",
    "    + eligible[\"score_loss_impact\"]\n",
    ")\n",
    "\n",
    "# Priority subset (low/medium confidence)\n",
    "priority = eligible[eligible[\"confidence\"].isin(PRIORITIZE_CONFIDENCE)].copy()\n",
    "\n",
    "# If priority subset is smaller than requested, top up with \"high\" confidence to keep review_pack_size stable.\n",
    "priority = priority.sort_values([\"confidence\", \"total_score\"], ascending=[True, False])\n",
    "review_pack = priority.head(REVIEW_PACK_SIZE)\n",
    "\n",
    "if len(review_pack) < REVIEW_PACK_SIZE:\n",
    "    remaining = eligible[~eligible.index.isin(review_pack.index)].sort_values(\n",
    "        [\"total_score\"], ascending=[False]\n",
    "    )\n",
    "    review_pack = pd.concat([review_pack, remaining.head(REVIEW_PACK_SIZE - len(review_pack))], ignore_index=True)\n",
    "\n",
    "# Keep only columns we want in the pack (if present)\n",
    "available_cols = [c for c in REVIEW_COLUMNS if c in review_pack.columns]\n",
    "review_pack = review_pack[available_cols].copy()\n",
    "\n",
    "# Add empty human-edit fields\n",
    "review_pack[\"decision\"] = \"\"  # keep | exclude | unsure\n",
    "review_pack[\"components_override\"] = \"\"  # e.g. hazard,exposure\n",
    "review_pack[\"notes\"] = \"\"\n",
    "\n",
    "review_pack.to_csv(REVIEW_PACK_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Wrote review pack: {REVIEW_PACK_CSV} (rows={len(review_pack)})\")\n",
    "\n",
    "# Quick QA summary for reviewers\n",
    "print(\"\\nReview pack breakdown:\")\n",
    "print(review_pack[\"confidence\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e078041-7790-428a-acf2-c17653790ffe",
   "metadata": {},
   "source": [
    "### Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eea1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote overrides: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\config\\overrides.yaml (entries=0)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Convert reviewed CSV -> overrides.yaml (after you edit the CSV)\n",
    "# ==========================================================\n",
    "#\n",
    "# Workflow:\n",
    "# 1) Open derived/review/review_pack.csv\n",
    "# 2) Fill decision (keep/exclude/unsure) and optional components_override + notes\n",
    "# 3) Re-run this cell to generate config/overrides.yaml\n",
    "#\n",
    "# Notes:\n",
    "# - components_override is optional. If omitted and decision=keep, the Step 4 components remain.\n",
    "# - If decision=exclude, the dataset will be excluded even if Step 4 included it.\n",
    "# - If decision=keep and Step 4 excluded by policy (OSM), it will only be included when ALLOW_OSM_OVERRIDE=True.\n",
    "\n",
    "VALID_DECISIONS = {\"keep\", \"exclude\", \"unsure\", \"\"}\n",
    "\n",
    "def parse_components_list(s: str) -> List[str]:\n",
    "    parts = [p.strip().lower() for p in str(s).split(\",\") if p.strip()]\n",
    "    # de-duplicate while preserving order\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if p not in out:\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "reviewed = pd.read_csv(REVIEW_PACK_CSV).fillna(\"\")\n",
    "reviewed[\"decision\"] = reviewed[\"decision\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "bad = reviewed[~reviewed[\"decision\"].isin(VALID_DECISIONS)]\n",
    "if len(bad) > 0:\n",
    "    raise ValueError(\n",
    "        \"Invalid decision value(s) found. Allowed: keep, exclude, unsure, blank.\\n\"\n",
    "        + bad[[\"dataset_id\", \"decision\"]].head(20).to_string(index=False)\n",
    "    )\n",
    "\n",
    "overrides: Dict[str, Any] = {\"overrides\": {}}\n",
    "\n",
    "for _, r in reviewed.iterrows():\n",
    "    dsid = str(r[\"dataset_id\"]).strip()\n",
    "    decision = str(r[\"decision\"]).strip().lower()\n",
    "    if not dsid or not decision or decision == \"unsure\":\n",
    "        continue\n",
    "\n",
    "    entry: Dict[str, Any] = {\"decision\": decision}\n",
    "\n",
    "    comps = parse_components_list(r.get(\"components_override\", \"\"))\n",
    "    if comps:\n",
    "        entry[\"components\"] = comps\n",
    "\n",
    "    notes = str(r.get(\"notes\", \"\")).strip()\n",
    "    if notes:\n",
    "        entry[\"notes\"] = notes\n",
    "\n",
    "    overrides[\"overrides\"][dsid] = entry\n",
    "\n",
    "# Write overrides.yaml deterministically\n",
    "with OVERRIDES_YAML.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(overrides, f, sort_keys=True, allow_unicode=True)\n",
    "\n",
    "print(f\"Wrote overrides: {OVERRIDES_YAML} (entries={len(overrides['overrides'])})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54861cb5-2b94-4e8a-9b42-6b918f95228f",
   "metadata": {},
   "source": [
    "### Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0534f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\derived\\classification_final.csv\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\derived\\rdls_included_dataset_ids_final.txt ( 10759 ids )\n",
      "Wrote: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\derived\\classification_final_summary.json\n",
      "{'total_datasets': 26246, 'policy': {'osm_excluded_ids_loaded': 3649, 'datasets_excluded_by_policy': 3649}, 'overrides': {'override_entries_loaded': 0, 'datasets_excluded_by_override': 0, 'datasets_with_component_override': 0}, 'rdls': {'candidates_total': 13668, 'included_total': 10759}, 'confidence_counts': {'low': 12578, 'high': 7216, 'medium': 6452}, 'component_nonzero_counts': {'hazard': 4056, 'exposure': 13671, 'vulnerability_proxy': 12952, 'loss_impact': 2745}}\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Apply overrides to the full classification\n",
    "# ==========================================\n",
    "#\n",
    "# Produces:\n",
    "# - derived/classification_final.csv\n",
    "# - derived/classification_final_summary.json\n",
    "# - derived/rdls_included_dataset_ids_final.txt\n",
    "\n",
    "def load_overrides(path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: overrides file not found: {path}. Proceeding with no overrides.\")\n",
    "        return {}\n",
    "    data = yaml.safe_load(path.read_text(encoding=\"utf-8\")) or {}\n",
    "    return data.get(\"overrides\", {}) or {}\n",
    "\n",
    "overrides_map = load_overrides(OVERRIDES_YAML)\n",
    "\n",
    "final = df.copy()\n",
    "\n",
    "# Add columns to track overrides\n",
    "final[\"override_decision\"] = \"\"\n",
    "final[\"override_components\"] = \"\"\n",
    "final[\"excluded_by_override\"] = False\n",
    "\n",
    "# Apply overrides row-by-row by dataset_id (efficient enough for 26k rows)\n",
    "for i, r in final.iterrows():\n",
    "    dsid = r[\"dataset_id\"]\n",
    "    ov = overrides_map.get(dsid)\n",
    "    if not ov:\n",
    "        continue\n",
    "\n",
    "    decision = str(ov.get(\"decision\", \"\")).strip().lower()\n",
    "    comps = ov.get(\"components\", None)\n",
    "\n",
    "    if decision in {\"exclude\", \"keep\"}:\n",
    "        final.at[i, \"override_decision\"] = decision\n",
    "\n",
    "    if isinstance(comps, list) and comps:\n",
    "        final.at[i, \"override_components\"] = \",\".join([str(c).lower() for c in comps])\n",
    "\n",
    "    if decision == \"exclude\":\n",
    "        final.at[i, \"excluded_by_override\"] = True\n",
    "\n",
    "    if decision == \"keep\":\n",
    "        # 'keep' means \"treat as RDLS candidate\" with optional component override\n",
    "        final.at[i, \"rdls_candidate\"] = True\n",
    "        if isinstance(comps, list) and comps:\n",
    "            final.at[i, \"rdls_components\"] = \",\".join([str(c).lower() for c in comps])\n",
    "\n",
    "# Final exclusion rule: policy OR override-exclude\n",
    "final[\"final_excluded\"] = final[\"excluded_by_policy\"] | final[\"excluded_by_override\"]\n",
    "\n",
    "# =======================================\n",
    "# Enforce RDLS component combination rules\n",
    "# =======================================\n",
    "# Policy:\n",
    "# - vulnerability must co-occur with hazard or exposure\n",
    "# - loss must co-occur with hazard or exposure\n",
    "# Inclusive normalization approach:\n",
    "# - if vulnerability-only -> add exposure\n",
    "# - if loss-only -> add exposure\n",
    "#\n",
    "# We keep a trace column so you can audit what changed.\n",
    "\n",
    "final[\"components_normalized\"] = False\n",
    "final[\"components_normalization_notes\"] = \"\"\n",
    "\n",
    "def _parse_components(s: Any) -> set[str]:\n",
    "    parts = [p.strip().lower() for p in str(s).split(\",\") if p.strip()]\n",
    "    return set(parts)\n",
    "\n",
    "def _join_components(s: set[str]) -> str:\n",
    "    # stable order\n",
    "    # order = [\"hazard\", \"exposure\", \"vulnerability\", \"loss\"]\n",
    "    order = [\"hazard\", \"exposure\", \"vulnerability_proxy\", \"loss_impact\"]\n",
    "    return \",\".join([c for c in order if c in s])\n",
    "\n",
    "for i, r in final.iterrows():\n",
    "    if not bool(r.get(\"rdls_candidate\", False)):\n",
    "        continue\n",
    "    if bool(r.get(\"final_excluded\", False)):\n",
    "        continue\n",
    "\n",
    "    comps = _parse_components(r.get(\"rdls_components\", \"\"))\n",
    "\n",
    "    # skip empty (should not happen for candidates, but keep safe)\n",
    "    if not comps:\n",
    "        continue\n",
    "\n",
    "    notes = []\n",
    "\n",
    "    # vulnerability requires hazard or exposure\n",
    "    # if \"vulnerability\" in comps and not ((\"hazard\" in comps) or (\"exposure\" in comps)):\n",
    "    #     comps.add(\"exposure\")\n",
    "    #     notes.append(\"added_exposure_for_vulnerability\")\n",
    "    if \"vulnerability_proxy\" in comps and not ((\"hazard\" in comps) or (\"exposure\" in comps)):\n",
    "        comps.add(\"exposure\")\n",
    "        notes.append(\"added_exposure_for_vulnerability_proxy\")\n",
    "\n",
    "    # loss requires hazard or exposure\n",
    "    # if \"loss\" in comps and not ((\"hazard\" in comps) or (\"exposure\" in comps)):\n",
    "    #     comps.add(\"exposure\")\n",
    "    #     notes.append(\"added_exposure_for_loss\")\n",
    "    if \"loss_impact\" in comps and not ((\"hazard\" in comps) or (\"exposure\" in comps)):\n",
    "        comps.add(\"exposure\")\n",
    "        notes.append(\"added_exposure_for_loss_impact\")\n",
    "\n",
    "    # if any changes happened, write back\n",
    "    if notes:\n",
    "        final.at[i, \"rdls_components\"] = _join_components(comps)\n",
    "        final.at[i, \"components_normalized\"] = True\n",
    "        final.at[i, \"components_normalization_notes\"] = \";\".join(notes)\n",
    "\n",
    "# Enforce OSM policy unless explicitly allowed\n",
    "if not ALLOW_OSM_OVERRIDE and len(osm_excluded_ids) > 0:\n",
    "    # If someone tried to keep an OSM-excluded dataset, revert inclusion.\n",
    "    mask_illegal_keep = final[\"dataset_id\"].isin(osm_excluded_ids) & (final[\"override_decision\"] == \"keep\")\n",
    "    illegal = int(mask_illegal_keep.sum())\n",
    "    if illegal > 0:\n",
    "        print(\n",
    "            f\"WARNING: {illegal} override(s) attempted to include OSM-excluded datasets. \"\n",
    "            \"Reverting to excluded (ALLOW_OSM_OVERRIDE=False).\"\n",
    "        )\n",
    "        final.loc[mask_illegal_keep, \"final_excluded\"] = True\n",
    "\n",
    "# Final included set: must be rdls_candidate AND not finally excluded\n",
    "final[\"final_included\"] = final[\"rdls_candidate\"] & (~final[\"final_excluded\"])\n",
    "\n",
    "# Write final artifacts\n",
    "final.to_csv(CLASSIFICATION_FINAL_CSV, index=False, encoding=\"utf-8\")\n",
    "included_ids = final.loc[final[\"final_included\"], \"dataset_id\"].astype(str).tolist()\n",
    "RDLS_INCLUDED_IDS_FINAL_TXT.write_text(\"\\n\".join(included_ids) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "summary = {\n",
    "    \"total_datasets\": int(len(final)),\n",
    "    \"policy\": {\n",
    "        \"osm_excluded_ids_loaded\": int(len(osm_excluded_ids)),\n",
    "        \"datasets_excluded_by_policy\": int(final[\"excluded_by_policy\"].sum()),\n",
    "    },\n",
    "    \"overrides\": {\n",
    "        \"override_entries_loaded\": int(len(overrides_map)),\n",
    "        \"datasets_excluded_by_override\": int(final[\"excluded_by_override\"].sum()),\n",
    "        \"datasets_with_component_override\": int((final[\"override_components\"].astype(str) != \"\").sum()),\n",
    "    },\n",
    "    \"rdls\": {\n",
    "        \"candidates_total\": int(final[\"rdls_candidate\"].sum()),\n",
    "        \"included_total\": int(final[\"final_included\"].sum()),\n",
    "    },\n",
    "    \"confidence_counts\": final[\"confidence\"].value_counts().to_dict(),\n",
    "    \"component_nonzero_counts\": {\n",
    "        \"hazard\": int((final[\"score_hazard\"] > 0).sum()),\n",
    "        \"exposure\": int((final[\"score_exposure\"] > 0).sum()),\n",
    "        \"vulnerability_proxy\": int((final[\"score_vulnerability_proxy\"] > 0).sum()),\n",
    "        \"loss_impact\": int((final[\"score_loss_impact\"] > 0).sum()),\n",
    "    },\n",
    "}\n",
    "\n",
    "CLASSIFICATION_FINAL_SUMMARY_JSON.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote: {CLASSIFICATION_FINAL_CSV}\")\n",
    "print(f\"Wrote: {RDLS_INCLUDED_IDS_FINAL_TXT} ( {len(included_ids)} ids )\")\n",
    "print(f\"Wrote: {CLASSIFICATION_FINAL_SUMMARY_JSON}\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe02ef-b883-4a3a-aa3e-94a08b58db60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
