{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 05: RDLS Classification Review & Overrides\n",
    "\n",
    "**Purpose**: Provide a human QA loop for the machine classification from Notebook 04.\n",
    "\n",
    "**Process**:\n",
    "1. Build a review pack (CSV) prioritizing low/medium confidence candidates\n",
    "2. Capture human decisions (keep/exclude/adjust) in structured format\n",
    "3. Convert reviewed CSV to `config/overrides.yaml`\n",
    "4. Apply overrides to produce final classification table\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started: 2026-02-10T21:42:18.365470\n",
      "Progress bars: Available\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# PyYAML for config files\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Missing dependency: pyyaml. Install with: pip install pyyaml\") from e\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/classification.csv\n",
      "Output: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/review\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input/Output directories\n",
    "DUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DERIVED_DIR = DUMP_DIR / 'derived'\n",
    "POLICY_DIR = DUMP_DIR / 'policy'\n",
    "CONFIG_DIR = DUMP_DIR / 'config'\n",
    "REVIEW_DIR = DERIVED_DIR / 'review'\n",
    "\n",
    "# Input files from Notebook 04\n",
    "CLASSIFICATION_CSV = DERIVED_DIR / 'classification.csv'\n",
    "OSM_EXCLUDED_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n",
    "\n",
    "# Output files\n",
    "REVIEW_PACK_CSV = REVIEW_DIR / 'review_pack.csv'\n",
    "OVERRIDES_YAML = CONFIG_DIR / 'overrides.yaml'\n",
    "CLASSIFICATION_FINAL_CSV = DERIVED_DIR / 'classification_final.csv'\n",
    "CLASSIFICATION_FINAL_SUMMARY_JSON = DERIVED_DIR / 'classification_final_summary.json'\n",
    "RDLS_INCLUDED_IDS_FINAL_TXT = DERIVED_DIR / 'rdls_included_dataset_ids_final.txt'\n",
    "\n",
    "# Create directories\n",
    "REVIEW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {CLASSIFICATION_CSV}\")\n",
    "print(f\"Output: {REVIEW_DIR}\")\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "# Default \"skip\" because review/override files contain human decisions.\n",
    "CLEANUP_MODE = \"replace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-1-3-clean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 05 Review Pack]:\n",
      "  review_pack.csv                         : 1 files\n",
      "  Cleaned 1 files. Ready for fresh output.\n",
      "\n",
      "Output cleanup [NB 05 Overrides Config]:\n",
      "  overrides.yaml                          : 1 files\n",
      "  Cleaned 1 files. Ready for fresh output.\n",
      "\n",
      "Output cleanup [NB 05 Final Classification]:\n",
      "  classification_final.csv                : 1 files\n",
      "  classification_final_summary.json       : 1 files\n",
      "  rdls_included_dataset_ids_final.txt     : 1 files\n",
      "  Cleaned 3 files. Ready for fresh output.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deleted': 3, 'skipped': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Clean Previous Outputs\n",
    "\n",
    "Remove stale output files from previous runs (controlled by CLEANUP_MODE).\n",
    "Default is \"skip\" because review/override files contain human decisions.\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice, defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "# ── Run cleanup ────────────────────────────────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    REVIEW_DIR,\n",
    "    patterns=[\"review_pack.csv\"],\n",
    "    label=\"NB 05 Review Pack\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n",
    "\n",
    "clean_previous_outputs(\n",
    "    CONFIG_DIR,\n",
    "    patterns=[\"overrides.yaml\"],\n",
    "    label=\"NB 05 Overrides Config\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n",
    "\n",
    "clean_previous_outputs(\n",
    "    DERIVED_DIR,\n",
    "    patterns=[\n",
    "        \"classification_final.csv\",\n",
    "        \"classification_final_summary.json\",\n",
    "        \"rdls_included_dataset_ids_final.txt\",\n",
    "    ],\n",
    "    label=\"NB 05 Final Classification\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-1-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review pack size: 1500\n",
      "Allow OSM override: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Configuration Parameters\n",
    "\"\"\"\n",
    "\n",
    "# Policy: do NOT allow humans to override OSM exclusion by default\n",
    "ALLOW_OSM_OVERRIDE = False\n",
    "\n",
    "# Review pack size (tune based on review capacity)\n",
    "REVIEW_PACK_SIZE = 1500\n",
    "\n",
    "# Prioritize low/medium confidence for review\n",
    "PRIORITIZE_CONFIDENCE = ('low', 'medium')\n",
    "\n",
    "# Required columns in classification CSV\n",
    "REQUIRED_COLUMNS = [\n",
    "    'dataset_id', 'title', 'organization', 'dataset_source', 'license_title',\n",
    "    'tags', 'groups', 'formats', 'excluded_by_policy', 'rdls_candidate',\n",
    "    'rdls_components', 'confidence', 'score_hazard', 'score_exposure',\n",
    "    'score_vulnerability_proxy', 'score_loss_impact',\n",
    "]\n",
    "\n",
    "print(f\"Review pack size: {REVIEW_PACK_SIZE}\")\n",
    "print(f\"Allow OSM override: {ALLOW_OSM_OVERRIDE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "def _to_bool_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Robust bool coercion for CSV roundtrips.\"\"\"\n",
    "    return (\n",
    "        s.astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .map({'true': True, 'false': False, '1': True, '0': False, 'yes': True, 'no': False})\n",
    "        .fillna(False)\n",
    "        .astype(bool)\n",
    "    )\n",
    "\n",
    "\n",
    "def load_classification_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and validate classification CSV.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Path to classification CSV\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Classification dataframe\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing Step 4 output: {path}\")\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "    \n",
    "    # Coerce boolean columns\n",
    "    for col in ['excluded_by_policy', 'rdls_candidate']:\n",
    "        df[col] = _to_bool_series(df[col])\n",
    "    \n",
    "    df['confidence'] = df['confidence'].fillna('unknown').astype(str).str.strip().str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_osm_excluded_ids(path: Path) -> set:\n",
    "    \"\"\"Load OSM exclusion list.\"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: OSM exclusion list not found: {path}\")\n",
    "        return set()\n",
    "    \n",
    "    ids = set()\n",
    "    for line in path.read_text(encoding='utf-8').splitlines():\n",
    "        s = line.strip()\n",
    "        if s:\n",
    "            ids.add(s)\n",
    "    return ids\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-2-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 26,246\n",
      "Unique dataset_id: 26,246\n",
      "OSM excluded IDs: 3,649\n",
      "OSM flagged in data: 3,649\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Load Classification and OSM Exclusion Data\n",
    "\"\"\"\n",
    "\n",
    "osm_excluded_ids = load_osm_excluded_ids(OSM_EXCLUDED_IDS_TXT)\n",
    "df = load_classification_table(CLASSIFICATION_CSV)\n",
    "\n",
    "# Derive is_osm column if not present\n",
    "if 'is_osm' not in df.columns:\n",
    "    df['is_osm'] = df['dataset_id'].astype(str).isin(osm_excluded_ids)\n",
    "else:\n",
    "    df['is_osm'] = _to_bool_series(df['is_osm'])\n",
    "\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique dataset_id: {df['dataset_id'].nunique():,}\")\n",
    "print(f\"OSM excluded IDs: {len(osm_excluded_ids):,}\")\n",
    "print(f\"OSM flagged in data: {df['is_osm'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Build Review Pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/review/review_pack.csv\n",
      "Review pack size: 1,500\n",
      "\n",
      "Confidence breakdown:\n",
      "confidence\n",
      "medium    1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.1 Generate Review Pack CSV\n",
    "\n",
    "Creates a subset for manual review prioritizing low/medium confidence candidates.\n",
    "\"\"\"\n",
    "\n",
    "REVIEW_COLUMNS = [\n",
    "    'dataset_id', 'title', 'organization', 'dataset_source', 'license_title',\n",
    "    'tags', 'groups', 'formats', 'is_osm', 'excluded_by_policy', 'rdls_candidate',\n",
    "    'rdls_components', 'confidence', 'score_hazard', 'score_exposure',\n",
    "    'score_vulnerability_proxy', 'score_loss_impact', 'top_signals',\n",
    "]\n",
    "\n",
    "# Filter: only candidates that are currently included\n",
    "eligible = df[(df['rdls_candidate'] == True) & (df['excluded_by_policy'] == False)].copy()\n",
    "\n",
    "# Calculate total score\n",
    "eligible['total_score'] = (\n",
    "    eligible['score_hazard'] + eligible['score_exposure'] +\n",
    "    eligible['score_vulnerability_proxy'] + eligible['score_loss_impact']\n",
    ")\n",
    "\n",
    "# Prioritize low/medium confidence\n",
    "priority = eligible[eligible['confidence'].isin(PRIORITIZE_CONFIDENCE)].copy()\n",
    "priority = priority.sort_values(['confidence', 'total_score'], ascending=[True, False])\n",
    "review_pack = priority.head(REVIEW_PACK_SIZE)\n",
    "\n",
    "# Top up with high confidence if needed\n",
    "if len(review_pack) < REVIEW_PACK_SIZE:\n",
    "    remaining = eligible[~eligible.index.isin(review_pack.index)].sort_values('total_score', ascending=False)\n",
    "    review_pack = pd.concat([review_pack, remaining.head(REVIEW_PACK_SIZE - len(review_pack))], ignore_index=True)\n",
    "\n",
    "# Keep only available columns\n",
    "available_cols = [c for c in REVIEW_COLUMNS if c in review_pack.columns]\n",
    "review_pack = review_pack[available_cols].copy()\n",
    "\n",
    "# Add empty human-edit fields\n",
    "review_pack['decision'] = ''  # keep | exclude | unsure\n",
    "review_pack['components_override'] = ''  # e.g. hazard,exposure\n",
    "review_pack['notes'] = ''\n",
    "\n",
    "review_pack.to_csv(REVIEW_PACK_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Wrote: {REVIEW_PACK_CSV}\")\n",
    "print(f\"Review pack size: {len(review_pack):,}\")\n",
    "print(f\"\\nConfidence breakdown:\")\n",
    "print(review_pack['confidence'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Convert to Overrides YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config/overrides.yaml\n",
      "Override entries: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.1 Parse Reviewed CSV and Generate overrides.yaml\n",
    "\n",
    "Workflow:\n",
    "1. Open review_pack.csv in Excel/VS Code\n",
    "2. Fill decision (keep/exclude/unsure) and optional components_override\n",
    "3. Re-run this cell to generate overrides.yaml\n",
    "\"\"\"\n",
    "\n",
    "VALID_DECISIONS = {'keep', 'exclude', 'unsure', ''}\n",
    "\n",
    "def parse_components_list(s: str) -> List[str]:\n",
    "    \"\"\"Parse comma-separated components list.\"\"\"\n",
    "    parts = [p.strip().lower() for p in str(s).split(',') if p.strip()]\n",
    "    # Deduplicate preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for p in parts:\n",
    "        if p not in seen:\n",
    "            out.append(p)\n",
    "            seen.add(p)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Load reviewed CSV\n",
    "reviewed = pd.read_csv(REVIEW_PACK_CSV).fillna('')\n",
    "reviewed['decision'] = reviewed['decision'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Validate decisions\n",
    "bad = reviewed[~reviewed['decision'].isin(VALID_DECISIONS)]\n",
    "if len(bad) > 0:\n",
    "    raise ValueError(\n",
    "        f\"Invalid decision values found. Allowed: keep, exclude, unsure, blank.\\n\"\n",
    "        + bad[['dataset_id', 'decision']].head(20).to_string(index=False)\n",
    "    )\n",
    "\n",
    "# Build overrides dict\n",
    "overrides: Dict[str, Any] = {'overrides': {}}\n",
    "\n",
    "for _, r in reviewed.iterrows():\n",
    "    dsid = str(r['dataset_id']).strip()\n",
    "    decision = str(r['decision']).strip().lower()\n",
    "    \n",
    "    if not dsid or not decision or decision == 'unsure':\n",
    "        continue\n",
    "    \n",
    "    entry: Dict[str, Any] = {'decision': decision}\n",
    "    \n",
    "    comps = parse_components_list(r.get('components_override', ''))\n",
    "    if comps:\n",
    "        entry['components'] = comps\n",
    "    \n",
    "    notes = str(r.get('notes', '')).strip()\n",
    "    if notes:\n",
    "        entry['notes'] = notes\n",
    "    \n",
    "    overrides['overrides'][dsid] = entry\n",
    "\n",
    "# Write YAML\n",
    "with OVERRIDES_YAML.open('w', encoding='utf-8') as f:\n",
    "    yaml.safe_dump(overrides, f, sort_keys=True, allow_unicode=True)\n",
    "\n",
    "print(f\"Wrote: {OVERRIDES_YAML}\")\n",
    "print(f\"Override entries: {len(overrides['overrides']):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Apply Overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 overrides\n",
      "Overrides applied.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.1 Apply Overrides to Full Classification\n",
    "\"\"\"\n",
    "\n",
    "def load_overrides(path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Load overrides YAML file.\"\"\"\n",
    "    if not path.exists():\n",
    "        print(f\"WARNING: overrides file not found: {path}\")\n",
    "        return {}\n",
    "    data = yaml.safe_load(path.read_text(encoding='utf-8')) or {}\n",
    "    return data.get('overrides', {}) or {}\n",
    "\n",
    "\n",
    "def _parse_components(s: Any) -> set:\n",
    "    \"\"\"Parse components string to set.\"\"\"\n",
    "    parts = [p.strip().lower() for p in str(s).split(',') if p.strip()]\n",
    "    return set(parts)\n",
    "\n",
    "\n",
    "def _join_components(s: set) -> str:\n",
    "    \"\"\"Join components in standard order.\"\"\"\n",
    "    order = ['hazard', 'exposure', 'vulnerability_proxy', 'loss_impact']\n",
    "    return ','.join([c for c in order if c in s])\n",
    "\n",
    "\n",
    "# Load overrides\n",
    "overrides_map = load_overrides(OVERRIDES_YAML)\n",
    "print(f\"Loaded {len(overrides_map):,} overrides\")\n",
    "\n",
    "# Create final dataframe\n",
    "final = df.copy()\n",
    "\n",
    "# Add override tracking columns\n",
    "final['override_decision'] = ''\n",
    "final['override_components'] = ''\n",
    "final['excluded_by_override'] = False\n",
    "\n",
    "# Apply overrides\n",
    "for i, r in final.iterrows():\n",
    "    dsid = r['dataset_id']\n",
    "    ov = overrides_map.get(dsid)\n",
    "    if not ov:\n",
    "        continue\n",
    "    \n",
    "    decision = str(ov.get('decision', '')).strip().lower()\n",
    "    comps = ov.get('components', None)\n",
    "    \n",
    "    if decision in {'exclude', 'keep'}:\n",
    "        final.at[i, 'override_decision'] = decision\n",
    "    \n",
    "    if isinstance(comps, list) and comps:\n",
    "        final.at[i, 'override_components'] = ','.join([str(c).lower() for c in comps])\n",
    "    \n",
    "    if decision == 'exclude':\n",
    "        final.at[i, 'excluded_by_override'] = True\n",
    "    \n",
    "    if decision == 'keep':\n",
    "        final.at[i, 'rdls_candidate'] = True\n",
    "        if isinstance(comps, list) and comps:\n",
    "            final.at[i, 'rdls_components'] = ','.join([str(c).lower() for c in comps])\n",
    "\n",
    "# Final exclusion: policy OR override-exclude\n",
    "final['final_excluded'] = final['excluded_by_policy'] | final['excluded_by_override']\n",
    "\n",
    "print(\"Overrides applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-5-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component normalization complete.\n",
      "Component normalization: 2837 datasets had components added (2481 for vulnerability_proxy, 356 for loss_impact)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.2 Apply Component Normalization Rules\n",
    "\n",
    "Rules:\n",
    "- vulnerability_proxy requires hazard or exposure\n",
    "- loss_impact requires hazard or exposure\n",
    "If violated, auto-add exposure.\n",
    "\"\"\"\n",
    "\n",
    "# M5: Component dependency enforcement -- V/L candidates must co-occur with H or E.\n",
    "# If vulnerability_proxy or loss_impact appears alone, exposure is auto-added.\n",
    "final['components_normalized'] = False\n",
    "final['components_normalization_notes'] = ''\n",
    "\n",
    "_norm_vuln_count = 0\n",
    "_norm_loss_count = 0\n",
    "\n",
    "for i, r in final.iterrows():\n",
    "    if not bool(r.get('rdls_candidate', False)):\n",
    "        continue\n",
    "    if bool(r.get('final_excluded', False)):\n",
    "        continue\n",
    "    \n",
    "    comps = _parse_components(r.get('rdls_components', ''))\n",
    "    if not comps:\n",
    "        continue\n",
    "    \n",
    "    notes = []\n",
    "    \n",
    "    # vulnerability_proxy requires hazard or exposure\n",
    "    if 'vulnerability_proxy' in comps and not ({'hazard', 'exposure'} & comps):\n",
    "        comps.add('exposure')\n",
    "        notes.append('added_exposure_for_vulnerability_proxy')\n",
    "        _norm_vuln_count += 1\n",
    "    \n",
    "    # loss_impact requires hazard or exposure\n",
    "    if 'loss_impact' in comps and not ({'hazard', 'exposure'} & comps):\n",
    "        comps.add('exposure')\n",
    "        notes.append('added_exposure_for_loss_impact')\n",
    "        _norm_loss_count += 1\n",
    "    \n",
    "    if notes:\n",
    "        final.at[i, 'rdls_components'] = _join_components(comps)\n",
    "        final.at[i, 'components_normalized'] = True\n",
    "        final.at[i, 'components_normalization_notes'] = ';'.join(notes)\n",
    "\n",
    "# Enforce OSM policy if not allowing overrides\n",
    "if not ALLOW_OSM_OVERRIDE and len(osm_excluded_ids) > 0:\n",
    "    mask_illegal = final['dataset_id'].isin(osm_excluded_ids) & (final['override_decision'] == 'keep')\n",
    "    illegal_count = int(mask_illegal.sum())\n",
    "    if illegal_count > 0:\n",
    "        print(f\"WARNING: {illegal_count} override(s) tried to include OSM-excluded datasets. Reverting.\")\n",
    "        final.loc[mask_illegal, 'final_excluded'] = True\n",
    "\n",
    "# Final included set\n",
    "final['final_included'] = final['rdls_candidate'] & (~final['final_excluded'])\n",
    "\n",
    "print(f\"Component normalization complete.\")\n",
    "_total_norm = int(final['components_normalized'].sum())\n",
    "print(f\"Component normalization: {_total_norm} datasets had components added \"\n",
    "      f\"({_norm_vuln_count} for vulnerability_proxy, {_norm_loss_count} for loss_impact)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-6-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/classification_final.csv\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/rdls_included_dataset_ids_final.txt (13,152 IDs)\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/classification_final_summary.json\n",
      "\n",
      "============================================================\n",
      "FINAL CLASSIFICATION SUMMARY\n",
      "============================================================\n",
      "Total datasets: 26,246\n",
      "RDLS candidates: 16,224\n",
      "Final included: 13,152\n",
      "Excluded by OSM policy: 3,649\n",
      "Excluded by override: 0\n",
      "\n",
      "Next: Run Notebook 06 to translate to RDLS schema.\n",
      "\n",
      "Notebook completed: 2026-02-10T21:42:47.361452\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.1 Save Final Classification and Summary\n",
    "\"\"\"\n",
    "\n",
    "# Write final classification\n",
    "final.to_csv(CLASSIFICATION_FINAL_CSV, index=False, encoding='utf-8')\n",
    "print(f\"Wrote: {CLASSIFICATION_FINAL_CSV}\")\n",
    "\n",
    "# Write included IDs\n",
    "included_ids = final.loc[final['final_included'], 'dataset_id'].astype(str).tolist()\n",
    "RDLS_INCLUDED_IDS_FINAL_TXT.write_text('\\n'.join(included_ids) + '\\n', encoding='utf-8')\n",
    "print(f\"Wrote: {RDLS_INCLUDED_IDS_FINAL_TXT} ({len(included_ids):,} IDs)\")\n",
    "\n",
    "# Generate summary\n",
    "summary = {\n",
    "    'total_datasets': int(len(final)),\n",
    "    'policy': {\n",
    "        'osm_excluded_ids_loaded': int(len(osm_excluded_ids)),\n",
    "        'datasets_excluded_by_policy': int(final['excluded_by_policy'].sum()),\n",
    "    },\n",
    "    'overrides': {\n",
    "        'override_entries_loaded': int(len(overrides_map)),\n",
    "        'datasets_excluded_by_override': int(final['excluded_by_override'].sum()),\n",
    "        'datasets_with_component_override': int((final['override_components'].astype(str) != '').sum()),\n",
    "    },\n",
    "    'rdls': {\n",
    "        'candidates_total': int(final['rdls_candidate'].sum()),\n",
    "        'included_total': int(final['final_included'].sum()),\n",
    "    },\n",
    "    'confidence_counts': final['confidence'].value_counts().to_dict(),\n",
    "    'component_nonzero_counts': {\n",
    "        'hazard': int((final['score_hazard'] > 0).sum()),\n",
    "        'exposure': int((final['score_exposure'] > 0).sum()),\n",
    "        'vulnerability_proxy': int((final['score_vulnerability_proxy'] > 0).sum()),\n",
    "        'loss_impact': int((final['score_loss_impact'] > 0).sum()),\n",
    "    },\n",
    "}\n",
    "\n",
    "CLASSIFICATION_FINAL_SUMMARY_JSON.write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "print(f\"Wrote: {CLASSIFICATION_FINAL_SUMMARY_JSON}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL CLASSIFICATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total datasets: {summary['total_datasets']:,}\")\n",
    "print(f\"RDLS candidates: {summary['rdls']['candidates_total']:,}\")\n",
    "print(f\"Final included: {summary['rdls']['included_total']:,}\")\n",
    "print(f\"Excluded by OSM policy: {summary['policy']['datasets_excluded_by_policy']:,}\")\n",
    "print(f\"Excluded by override: {summary['overrides']['datasets_excluded_by_override']:,}\")\n",
    "\n",
    "print(f\"\\nNext: Run Notebook 06 to translate to RDLS schema.\")\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85d11a-4074-4625-97ee-ce66ce6215da",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
