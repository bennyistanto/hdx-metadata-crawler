{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 04: RDLS Candidate Classification\n",
    "\n",
    "**Purpose**: Classify HDX datasets into RDLS components using weighted scoring.\n",
    "\n",
    "**Process**:\n",
    "1. Load mapping configs from Notebook 03\n",
    "2. Score each dataset using tags, keywords, and org hints\n",
    "3. Assign RDLS components (hazard, exposure, vulnerability_proxy, loss_impact)\n",
    "4. Apply OSM exclusion policy from Notebook 02\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# PyYAML for config files\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Missing dependency: pyyaml. Install with: pip install pyyaml\") from e\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input directories\n",
    "DUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DATASET_DIR = DUMP_DIR / 'dataset_metadata'\n",
    "POLICY_DIR = DUMP_DIR / 'policy'\n",
    "CONFIG_DIR = DUMP_DIR / 'config'\n",
    "\n",
    "# Config files from Notebook 03\n",
    "OSM_EXCLUDED_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n",
    "TAG_MAP_YAML = CONFIG_DIR / 'tag_to_rdls_component.yaml'\n",
    "KEYWORD_MAP_YAML = CONFIG_DIR / 'keyword_to_rdls_component.yaml'\n",
    "ORG_HINTS_YAML = CONFIG_DIR / 'org_hints.yaml'\n",
    "\n",
    "# Output directories\n",
    "DERIVED_DIR = DUMP_DIR / 'derived'\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output files\n",
    "OUT_CLASSIFICATION_CSV = DERIVED_DIR / 'classification.csv'\n",
    "OUT_SUMMARY_JSON = DERIVED_DIR / 'classification_summary.json'\n",
    "OUT_INCLUDED_IDS_TXT = DERIVED_DIR / 'rdls_included_dataset_ids.txt'\n",
    "OUT_ERRORS_JSONL = DERIVED_DIR / 'errors_classification.jsonl'\n",
    "\n",
    "print(f\"Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"Config dir: {CONFIG_DIR}\")\n",
    "print(f\"Output dir: {DERIVED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Classification Configuration\n",
    "\"\"\"\n",
    "\n",
    "# RDLS component categories\n",
    "RDLS_COMPONENTS = ('hazard', 'exposure', 'vulnerability_proxy', 'loss_impact')\n",
    "\n",
    "# Scoring thresholds\n",
    "KEYWORD_HIT_WEIGHT = 1    # Weight per keyword pattern match\n",
    "CANDIDATE_MIN_SCORE = 4   # Minimum score to be RDLS candidate\n",
    "CONF_HIGH = 7             # Score >= this = high confidence\n",
    "CONF_MED = 4              # Score >= this = medium confidence\n",
    "\n",
    "print(f\"RDLS Components: {RDLS_COMPONENTS}\")\n",
    "print(f\"Candidate threshold: {CANDIDATE_MIN_SCORE}\")\n",
    "print(f\"High confidence threshold: {CONF_HIGH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Load Mapping Configs from Notebook 03\n",
    "\"\"\"\n",
    "\n",
    "def load_yaml(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load YAML file safely.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing config file: {path}\")\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f) or {}\n",
    "\n",
    "\n",
    "# Load mapping configs\n",
    "tag_weights: Dict[str, Dict[str, int]] = load_yaml(TAG_MAP_YAML)\n",
    "keyword_patterns: Dict[str, List[str]] = load_yaml(KEYWORD_MAP_YAML)\n",
    "org_hints: Dict[str, Dict[str, int]] = load_yaml(ORG_HINTS_YAML)\n",
    "\n",
    "# Compile regex patterns for keywords\n",
    "compiled_keywords: Dict[str, List[re.Pattern]] = {}\n",
    "for comp in RDLS_COMPONENTS:\n",
    "    pats = keyword_patterns.get(comp, []) or []\n",
    "    compiled_keywords[comp] = [re.compile(p, flags=re.IGNORECASE) for p in pats]\n",
    "\n",
    "print(f\"Loaded tag maps: {list(tag_weights.keys())}\")\n",
    "print(f\"Loaded keyword maps: {list(keyword_patterns.keys())}\")\n",
    "print(f\"Loaded org hints: {len(org_hints)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Load OSM Exclusion List from Notebook 02\n",
    "\"\"\"\n",
    "\n",
    "osm_excluded: set = set()\n",
    "\n",
    "if OSM_EXCLUDED_IDS_TXT.exists():\n",
    "    with OSM_EXCLUDED_IDS_TXT.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            v = line.strip()\n",
    "            if v:\n",
    "                osm_excluded.add(v)\n",
    "    print(f\"Loaded {len(osm_excluded):,} OSM excluded IDs\")\n",
    "else:\n",
    "    print(f\"WARNING: OSM exclusion list not found: {OSM_EXCLUDED_IDS_TXT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Classification Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in folder, sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob('*.json'))\n",
    "\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file safely.\"\"\"\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def as_list(x: Any) -> List[Any]:\n",
    "    \"\"\"Convert value to list.\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    \"\"\"Normalize text value.\"\"\"\n",
    "    if not s:\n",
    "        return ''\n",
    "    return str(s).strip()\n",
    "\n",
    "\n",
    "def extract_formats(resources: Any) -> List[str]:\n",
    "    \"\"\"Extract unique formats from resources.\"\"\"\n",
    "    fmts = []\n",
    "    for r in as_list(resources):\n",
    "        fmt = normalize_text(r.get('format') if isinstance(r, dict) else '')\n",
    "        if fmt:\n",
    "            fmts.append(fmt.upper())\n",
    "    # Deduplicate preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for f in fmts:\n",
    "        if f not in seen:\n",
    "            out.append(f)\n",
    "            seen.add(f)\n",
    "    return out\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.2 Classification Dataclass and Function\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Classification:\n",
    "    \"\"\"Classification result for a dataset.\"\"\"\n",
    "    scores: Dict[str, int]\n",
    "    components: List[str]\n",
    "    rdls_candidate: bool\n",
    "    confidence: str\n",
    "    top_signals: List[str]\n",
    "\n",
    "\n",
    "def classify_dataset(meta: Dict[str, Any]) -> Classification:\n",
    "    \"\"\"\n",
    "    Classify a dataset into RDLS components.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    meta : Dict[str, Any]\n",
    "        Dataset metadata\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Classification\n",
    "        Classification result with scores and components\n",
    "    \"\"\"\n",
    "    # Extract text fields\n",
    "    title = normalize_text(meta.get('title'))\n",
    "    notes = normalize_text(meta.get('notes'))\n",
    "    org = normalize_text(meta.get('organization'))\n",
    "    text = f\"{title}\\n{notes}\".strip()\n",
    "    \n",
    "    # Extract and normalize tags\n",
    "    tags = [normalize_text(t) for t in as_list(meta.get('tags'))]\n",
    "    tags_lower = [t.lower() for t in tags if t]\n",
    "    \n",
    "    # Initialize scores\n",
    "    scores = {c: 0 for c in RDLS_COMPONENTS}\n",
    "    signals: List[Tuple[int, str]] = []\n",
    "    \n",
    "    # 1) Tag weights\n",
    "    for comp, weights in tag_weights.items():\n",
    "        if comp not in RDLS_COMPONENTS:\n",
    "            continue\n",
    "        for t in tags_lower:\n",
    "            w = weights.get(t)\n",
    "            if w:\n",
    "                scores[comp] += int(w)\n",
    "                signals.append((abs(int(w)), f\"tag:{t}(+{w})→{comp}\"))\n",
    "    \n",
    "    # 2) Keyword pattern matches\n",
    "    if text:\n",
    "        for comp in RDLS_COMPONENTS:\n",
    "            hits = 0\n",
    "            for pat in compiled_keywords.get(comp, []):\n",
    "                if pat.search(text):\n",
    "                    hits += 1\n",
    "                    signals.append((KEYWORD_HIT_WEIGHT, f\"kw:{pat.pattern}(+{KEYWORD_HIT_WEIGHT})→{comp}\"))\n",
    "            if hits:\n",
    "                scores[comp] += hits * KEYWORD_HIT_WEIGHT\n",
    "    \n",
    "    # 3) Organization hints\n",
    "    org_norm = org.lower()\n",
    "    for hint, comp_weights in org_hints.items():\n",
    "        if not hint:\n",
    "            continue\n",
    "        if hint.lower() in org_norm:\n",
    "            for comp, w in comp_weights.items():\n",
    "                if comp in RDLS_COMPONENTS and w:\n",
    "                    scores[comp] += int(w)\n",
    "                    signals.append((abs(int(w)), f\"org:{hint}(+{w})→{comp}\"))\n",
    "    \n",
    "    # Determine components\n",
    "    max_score = max(scores.values()) if scores else 0\n",
    "    components = [c for c, s in scores.items() if s >= CANDIDATE_MIN_SCORE]\n",
    "    \n",
    "    # Fallback: include best component if any signal exists\n",
    "    if not components and max_score > 0:\n",
    "        best = [c for c, s in scores.items() if s == max_score]\n",
    "        components = best[:1]\n",
    "    \n",
    "    rdls_candidate = max_score >= CANDIDATE_MIN_SCORE\n",
    "    \n",
    "    # Determine confidence\n",
    "    if max_score >= CONF_HIGH:\n",
    "        confidence = 'high'\n",
    "    elif max_score >= CONF_MED:\n",
    "        confidence = 'medium'\n",
    "    else:\n",
    "        confidence = 'low'\n",
    "    \n",
    "    # Top signals for debugging\n",
    "    signals_sorted = [s for _, s in sorted(signals, key=lambda x: x[0], reverse=True)]\n",
    "    top_signals = signals_sorted[:8]\n",
    "    \n",
    "    return Classification(\n",
    "        scores=scores,\n",
    "        components=components,\n",
    "        rdls_candidate=rdls_candidate,\n",
    "        confidence=confidence,\n",
    "        top_signals=top_signals\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Classification function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Process All Datasets\n",
    "\"\"\"\n",
    "\n",
    "files = list(iter_json_files(DATASET_DIR))\n",
    "total = len(files)\n",
    "\n",
    "print(f\"Processing {total:,} dataset files...\")\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "errors = 0\n",
    "\n",
    "# Reset errors log\n",
    "if OUT_ERRORS_JSONL.exists():\n",
    "    OUT_ERRORS_JSONL.unlink()\n",
    "\n",
    "# Create iterator with progress bar\n",
    "iterator = tqdm(files, desc=\"Classifying datasets\") if HAS_TQDM else files\n",
    "\n",
    "for i, fp in enumerate(iterator, start=1):\n",
    "    try:\n",
    "        meta = safe_load_json(fp)\n",
    "        \n",
    "        dataset_id = normalize_text(meta.get('id'))\n",
    "        cls = classify_dataset(meta)\n",
    "        \n",
    "        excluded_by_policy = dataset_id in osm_excluded\n",
    "        resources = as_list(meta.get('resources'))\n",
    "        formats = extract_formats(resources)\n",
    "        \n",
    "        row = {\n",
    "            'dataset_id': dataset_id,\n",
    "            'name': normalize_text(meta.get('name')),\n",
    "            'title': normalize_text(meta.get('title')),\n",
    "            'organization': normalize_text(meta.get('organization')),\n",
    "            'dataset_source': normalize_text(meta.get('dataset_source')),\n",
    "            'license_title': normalize_text(meta.get('license_title')),\n",
    "            'dataset_date': normalize_text(meta.get('dataset_date')),\n",
    "            'last_modified': normalize_text(meta.get('last_modified')),\n",
    "            'data_update_frequency': normalize_text(meta.get('data_update_frequency')),\n",
    "            'groups': ';'.join([normalize_text(g) for g in as_list(meta.get('groups')) if normalize_text(g)]),\n",
    "            'tags': ';'.join([t for t in as_list(meta.get('tags')) if normalize_text(t)]),\n",
    "            'resource_count': len(resources),\n",
    "            'formats': ';'.join(formats),\n",
    "            **{f'score_{k}': int(v) for k, v in cls.scores.items()},\n",
    "            'rdls_components': ';'.join(cls.components),\n",
    "            'rdls_candidate': bool(cls.rdls_candidate),\n",
    "            'confidence': cls.confidence,\n",
    "            'excluded_by_policy': bool(excluded_by_policy),\n",
    "            'exclusion_reason': 'osm_policy' if excluded_by_policy else '',\n",
    "            'top_signals': ' | '.join(cls.top_signals),\n",
    "            'source_file': fp.name,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        with OUT_ERRORS_JSONL.open('a', encoding='utf-8') as ef:\n",
    "            ef.write(json.dumps({'file': fp.name, 'error': str(e)}, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    # Progress for non-tqdm\n",
    "    if not HAS_TQDM and i % 5000 == 0:\n",
    "        print(f\"  Processed {i:,}/{total:,}\")\n",
    "\n",
    "print(f\"\\nTotal rows: {len(rows):,}\")\n",
    "print(f\"Errors: {errors:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Save Classification Results\n",
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Stable column order\n",
    "base_cols = [\n",
    "    'dataset_id', 'name', 'title', 'organization', 'dataset_source', 'license_title',\n",
    "    'dataset_date', 'last_modified', 'data_update_frequency', 'groups', 'tags',\n",
    "    'resource_count', 'formats',\n",
    "    'score_hazard', 'score_exposure', 'score_vulnerability_proxy', 'score_loss_impact',\n",
    "    'rdls_components', 'rdls_candidate', 'confidence',\n",
    "    'excluded_by_policy', 'exclusion_reason',\n",
    "    'top_signals', 'source_file'\n",
    "]\n",
    "\n",
    "for c in base_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = ''\n",
    "\n",
    "df = df[base_cols]\n",
    "df.to_csv(OUT_CLASSIFICATION_CSV, index=False, encoding='utf-8')\n",
    "print(f\"Wrote: {OUT_CLASSIFICATION_CSV}\")\n",
    "\n",
    "# Included IDs = RDLS candidate AND not excluded\n",
    "included_ids = df.loc[\n",
    "    (df['rdls_candidate'] == True) & (df['excluded_by_policy'] == False), \n",
    "    'dataset_id'\n",
    "].dropna().tolist()\n",
    "\n",
    "OUT_INCLUDED_IDS_TXT.write_text(\n",
    "    '\\n'.join(included_ids) + ('\\n' if included_ids else ''), \n",
    "    encoding='utf-8'\n",
    ")\n",
    "print(f\"Wrote: {OUT_INCLUDED_IDS_TXT} ({len(included_ids):,} IDs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Generate Summary Statistics\n",
    "\"\"\"\n",
    "\n",
    "summary = {\n",
    "    'total_datasets': int(len(df)),\n",
    "    'errors': int(errors),\n",
    "    'policy': {\n",
    "        'osm_excluded_ids_loaded': int(len(osm_excluded)),\n",
    "        'datasets_excluded_by_policy': int(df['excluded_by_policy'].sum()),\n",
    "    },\n",
    "    'rdls': {\n",
    "        'candidates_total': int(df['rdls_candidate'].sum()),\n",
    "        'included_total': int(((df['rdls_candidate'] == True) & (df['excluded_by_policy'] == False)).sum()),\n",
    "    },\n",
    "    'confidence_counts': df['confidence'].value_counts(dropna=False).to_dict(),\n",
    "    'component_nonzero_counts': {\n",
    "        comp: int((df[f'score_{comp}'] > 0).sum()) for comp in RDLS_COMPONENTS\n",
    "    },\n",
    "}\n",
    "\n",
    "OUT_SUMMARY_JSON.write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f\"Wrote: {OUT_SUMMARY_JSON}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLASSIFICATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total datasets: {summary['total_datasets']:,}\")\n",
    "print(f\"RDLS candidates: {summary['rdls']['candidates_total']:,}\")\n",
    "print(f\"Included (after policy): {summary['rdls']['included_total']:,}\")\n",
    "print(f\"Excluded by OSM policy: {summary['policy']['datasets_excluded_by_policy']:,}\")\n",
    "print(f\"\\nConfidence distribution:\")\n",
    "for conf, count in summary['confidence_counts'].items():\n",
    "    print(f\"  {conf}: {count:,}\")\n",
    "print(f\"\\nComponent coverage (non-zero scores):\")\n",
    "for comp, count in summary['component_nonzero_counts'].items():\n",
    "    print(f\"  {comp}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Display Top Candidates\n",
    "\"\"\"\n",
    "\n",
    "# Add max score column\n",
    "df['score_max'] = df[[f'score_{c}' for c in RDLS_COMPONENTS]].max(axis=1)\n",
    "\n",
    "# Show top candidates\n",
    "print(\"\\nTop 15 RDLS Candidates (by score):\")\n",
    "top_candidates = df.loc[\n",
    "    df['excluded_by_policy'] == False\n",
    "].sort_values('score_max', ascending=False).head(15)\n",
    "\n",
    "display_cols = ['dataset_id', 'title', 'rdls_components', 'score_max', 'confidence']\n",
    "print(top_candidates[display_cols].to_string(index=False))\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - {OUT_CLASSIFICATION_CSV}\")\n",
    "print(f\"  - {OUT_SUMMARY_JSON}\")\n",
    "print(f\"  - {OUT_INCLUDED_IDS_TXT}\")\n",
    "\n",
    "print(f\"\\nNext: Run Notebook 05 to apply manual overrides.\")\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
