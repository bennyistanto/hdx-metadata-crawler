{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 04: RDLS Candidate Classification\n",
    "\n",
    "**Purpose**: Classify HDX datasets into RDLS components using weighted scoring.\n",
    "\n",
    "**Process**:\n",
    "1. Load mapping configs from Notebook 03\n",
    "2. Score each dataset using tags, keywords, and org hints\n",
    "3. Assign RDLS components (hazard, exposure, vulnerability_proxy, loss_impact)\n",
    "4. Apply OSM exclusion policy from Notebook 02\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started: 2026-02-10T21:33:23.100253\n",
      "Progress bars: Available\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# PyYAML for config files\n",
    "try:\n",
    "    import yaml\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Missing dependency: pyyaml. Install with: pip install pyyaml\") from e\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dir: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/dataset_metadata\n",
      "Config dir: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/config\n",
      "Output dir: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input directories\n",
    "DUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DATASET_DIR = DUMP_DIR / 'dataset_metadata'\n",
    "POLICY_DIR = DUMP_DIR / 'policy'\n",
    "CONFIG_DIR = DUMP_DIR / 'config'\n",
    "\n",
    "# Config files from Notebook 03\n",
    "OSM_EXCLUDED_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n",
    "TAG_MAP_YAML = CONFIG_DIR / 'tag_to_rdls_component.yaml'\n",
    "KEYWORD_MAP_YAML = CONFIG_DIR / 'keyword_to_rdls_component.yaml'\n",
    "ORG_HINTS_YAML = CONFIG_DIR / 'org_hints.yaml'\n",
    "\n",
    "# Output directories\n",
    "DERIVED_DIR = DUMP_DIR / 'derived'\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output files\n",
    "OUT_CLASSIFICATION_CSV = DERIVED_DIR / 'classification.csv'\n",
    "OUT_SUMMARY_JSON = DERIVED_DIR / 'classification_summary.json'\n",
    "OUT_INCLUDED_IDS_TXT = DERIVED_DIR / 'rdls_included_dataset_ids.txt'\n",
    "OUT_ERRORS_JSONL = DERIVED_DIR / 'errors_classification.jsonl'\n",
    "\n",
    "print(f\"Dataset dir: {DATASET_DIR}\")\n",
    "print(f\"Config dir: {CONFIG_DIR}\")\n",
    "print(f\"Output dir: {DERIVED_DIR}\")\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "CLEANUP_MODE = \"replace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-1-3-clean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 04 Classification]:\n",
      "  classification.csv                      : 1 files\n",
      "  classification_summary.json             : 1 files\n",
      "  rdls_included_dataset_ids.txt           : 1 files\n",
      "  Cleaned 3 files. Ready for fresh output.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deleted': 3, 'skipped': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Clean Previous Outputs\n",
    "\n",
    "Remove stale output files from previous runs (controlled by CLEANUP_MODE).\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice, defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "# ── Run cleanup ────────────────────────────────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    DERIVED_DIR,\n",
    "    patterns=[\n",
    "        \"classification.csv\",\n",
    "        \"classification_summary.json\",\n",
    "        \"rdls_included_dataset_ids.txt\",\n",
    "        \"errors_classification.jsonl\",\n",
    "    ],\n",
    "    label=\"NB 04 Classification\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-1-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDLS Components: ('hazard', 'exposure', 'vulnerability_proxy', 'loss_impact')\n",
      "Candidate threshold: 5\n",
      "High confidence threshold: 7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Classification Configuration\n",
    "\"\"\"\n",
    "\n",
    "# RDLS component categories\n",
    "RDLS_COMPONENTS = ('hazard', 'exposure', 'vulnerability_proxy', 'loss_impact')\n",
    "\n",
    "# Scoring thresholds\n",
    "KEYWORD_HIT_WEIGHT = 2    # Weight per keyword pattern match (balanced vs tag weights of 2-5)\n",
    "CANDIDATE_MIN_SCORE = 5   # Minimum score to be RDLS candidate (raised from 4 to reduce weak-signal noise)\n",
    "CONF_HIGH = 7             # Score >= this = high confidence\n",
    "CONF_MED = 4              # Score >= this = medium confidence\n",
    "\n",
    "print(f\"RDLS Components: {RDLS_COMPONENTS}\")\n",
    "print(f\"Candidate threshold: {CANDIDATE_MIN_SCORE}\")\n",
    "print(f\"High confidence threshold: {CONF_HIGH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tag maps: ['exposure', 'hazard', 'loss_impact', 'vulnerability_proxy']\n",
      "Loaded keyword maps: ['exposure', 'hazard', 'loss_impact', 'vulnerability_proxy']\n",
      "Loaded org hints: 4\n",
      "Signal dictionary loaded: 88 additional patterns merged into compiled_keywords\n",
      "Exclusion patterns loaded: 7 patterns across 2 components\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 Load Mapping Configs from Notebook 03\n",
    "\"\"\"\n",
    "\n",
    "def load_yaml(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load YAML file safely.\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing config file: {path}\")\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f) or {}\n",
    "\n",
    "\n",
    "# Load mapping configs\n",
    "tag_weights: Dict[str, Dict[str, int]] = load_yaml(TAG_MAP_YAML)\n",
    "keyword_patterns: Dict[str, List[str]] = load_yaml(KEYWORD_MAP_YAML)\n",
    "org_hints: Dict[str, Dict[str, int]] = load_yaml(ORG_HINTS_YAML)\n",
    "\n",
    "# Compile regex patterns for keywords\n",
    "compiled_keywords: Dict[str, List[re.Pattern]] = {}\n",
    "for comp in RDLS_COMPONENTS:\n",
    "    pats = keyword_patterns.get(comp, []) or []\n",
    "    compiled_keywords[comp] = [re.compile(p, flags=re.IGNORECASE) for p in pats]\n",
    "\n",
    "print(f\"Loaded tag maps: {list(tag_weights.keys())}\")\n",
    "print(f\"Loaded keyword maps: {list(keyword_patterns.keys())}\")\n",
    "print(f\"Loaded org hints: {len(org_hints)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# H3: Load signal dictionary and merge additional patterns into compiled_keywords\n",
    "# H5: Load exclusion patterns for false-positive reduction\n",
    "# ---------------------------------------------------------------------------\n",
    "SIGNAL_DICT_YAML = CONFIG_DIR / 'signal_dictionary.yaml'\n",
    "compiled_exclusion_patterns: Dict[str, List[re.Pattern]] = {}\n",
    "\n",
    "if SIGNAL_DICT_YAML.exists():\n",
    "    signal_dict = load_yaml(SIGNAL_DICT_YAML)\n",
    "    _added = 0\n",
    "\n",
    "    # --- H3: Merge hazard_type patterns into compiled_keywords['hazard'] ---\n",
    "    existing_hazard_pats = {p.pattern for p in compiled_keywords.get('hazard', [])}\n",
    "    for _htype, info in (signal_dict.get('hazard_type') or {}).items():\n",
    "        for pat_str in (info.get('patterns') or []):\n",
    "            if pat_str not in existing_hazard_pats:\n",
    "                compiled_keywords.setdefault('hazard', []).append(\n",
    "                    re.compile(pat_str, flags=re.IGNORECASE)\n",
    "                )\n",
    "                existing_hazard_pats.add(pat_str)\n",
    "                _added += 1\n",
    "\n",
    "    # --- H3: Merge exposure_category patterns into compiled_keywords['exposure'] ---\n",
    "    existing_exp_pats = {p.pattern for p in compiled_keywords.get('exposure', [])}\n",
    "    for _ecat, info in (signal_dict.get('exposure_category') or {}).items():\n",
    "        for pat_str in (info.get('patterns') or []):\n",
    "            if pat_str not in existing_exp_pats:\n",
    "                compiled_keywords.setdefault('exposure', []).append(\n",
    "                    re.compile(pat_str, flags=re.IGNORECASE)\n",
    "                )\n",
    "                existing_exp_pats.add(pat_str)\n",
    "                _added += 1\n",
    "\n",
    "    print(f\"Signal dictionary loaded: {_added} additional patterns merged into compiled_keywords\")\n",
    "\n",
    "    # --- H5: Load exclusion patterns for false-positive reduction ---\n",
    "    _excl_loaded = 0\n",
    "    for group_name, patterns in (signal_dict.get('exclusion_patterns') or {}).items():\n",
    "        if not isinstance(patterns, list):\n",
    "            continue\n",
    "        # Map exclusion group to RDLS component\n",
    "        if 'flood' in group_name:\n",
    "            comp_key = 'hazard'\n",
    "        elif 'population' in group_name:\n",
    "            comp_key = 'exposure'\n",
    "        elif 'infrastructure' in group_name:\n",
    "            comp_key = 'exposure'\n",
    "        else:\n",
    "            comp_key = 'hazard'  # default\n",
    "        compiled_exclusion_patterns.setdefault(comp_key, [])\n",
    "        for pat_str in patterns:\n",
    "            compiled_exclusion_patterns[comp_key].append(\n",
    "                re.compile(pat_str, flags=re.IGNORECASE)\n",
    "            )\n",
    "            _excl_loaded += 1\n",
    "    print(f\"Exclusion patterns loaded: {_excl_loaded} patterns across {len(compiled_exclusion_patterns)} components\")\n",
    "else:\n",
    "    print(f\"Signal dictionary not found at {SIGNAL_DICT_YAML} - skipping enrichment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-2-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,649 OSM excluded IDs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Load OSM Exclusion List from Notebook 02\n",
    "\"\"\"\n",
    "\n",
    "osm_excluded: set = set()\n",
    "\n",
    "if OSM_EXCLUDED_IDS_TXT.exists():\n",
    "    with OSM_EXCLUDED_IDS_TXT.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            v = line.strip()\n",
    "            if v:\n",
    "                osm_excluded.add(v)\n",
    "    print(f\"Loaded {len(osm_excluded):,} OSM excluded IDs\")\n",
    "else:\n",
    "    print(f\"WARNING: OSM exclusion list not found: {OSM_EXCLUDED_IDS_TXT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Classification Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.1 Helper Functions\n",
    "\"\"\"\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in folder, sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Dataset folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob('*.json'))\n",
    "\n",
    "\n",
    "def safe_load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file safely.\"\"\"\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def as_list(x: Any) -> List[Any]:\n",
    "    \"\"\"Convert value to list.\"\"\"\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def normalize_text(s: Any) -> str:\n",
    "    \"\"\"Normalize text value.\"\"\"\n",
    "    if not s:\n",
    "        return ''\n",
    "    return str(s).strip()\n",
    "\n",
    "\n",
    "def extract_formats(resources: Any) -> List[str]:\n",
    "    \"\"\"Extract unique formats from resources.\"\"\"\n",
    "    fmts = []\n",
    "    for r in as_list(resources):\n",
    "        fmt = normalize_text(r.get('format') if isinstance(r, dict) else '')\n",
    "        if fmt:\n",
    "            fmts.append(fmt.upper())\n",
    "    # Deduplicate preserving order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for f in fmts:\n",
    "        if f not in seen:\n",
    "            out.append(f)\n",
    "            seen.add(f)\n",
    "    return out\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-3-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification function defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.2 Classification Dataclass and Function\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Classification:\n",
    "    \"\"\"Classification result for a dataset.\"\"\"\n",
    "    scores: Dict[str, int]\n",
    "    components: List[str]\n",
    "    rdls_candidate: bool\n",
    "    confidence: str\n",
    "    top_signals: List[str]\n",
    "\n",
    "\n",
    "def classify_dataset(meta: Dict[str, Any]) -> Classification:\n",
    "    \"\"\"\n",
    "    Classify a dataset into RDLS components.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    meta : Dict[str, Any]\n",
    "        Dataset metadata\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Classification\n",
    "        Classification result with scores and components\n",
    "    \"\"\"\n",
    "    # Extract text fields\n",
    "    title = normalize_text(meta.get('title'))\n",
    "    notes = normalize_text(meta.get('notes'))\n",
    "    org = normalize_text(meta.get('organization'))\n",
    "    text = f\"{title}\\n{notes}\".strip()\n",
    "    \n",
    "    # Extract and normalize tags\n",
    "    tags = [normalize_text(t) for t in as_list(meta.get('tags'))]\n",
    "    tags_lower = [t.lower() for t in tags if t]\n",
    "    \n",
    "    # Initialize scores\n",
    "    scores = {c: 0 for c in RDLS_COMPONENTS}\n",
    "    signals: List[Tuple[int, str]] = []\n",
    "    \n",
    "    # 1) Tag weights\n",
    "    for comp, weights in tag_weights.items():\n",
    "        if comp not in RDLS_COMPONENTS:\n",
    "            continue\n",
    "        for t in tags_lower:\n",
    "            w = weights.get(t)\n",
    "            if w:\n",
    "                scores[comp] += int(w)\n",
    "                signals.append((abs(int(w)), f\"tag:{t}(+{w})→{comp}\"))\n",
    "    \n",
    "    # 2) Keyword pattern matches\n",
    "    if text:\n",
    "        for comp in RDLS_COMPONENTS:\n",
    "            hits = 0\n",
    "            for pat in compiled_keywords.get(comp, []):\n",
    "                if pat.search(text):\n",
    "                    hits += 1\n",
    "                    signals.append((KEYWORD_HIT_WEIGHT, f\"kw:{pat.pattern}(+{KEYWORD_HIT_WEIGHT})→{comp}\"))\n",
    "            if hits:\n",
    "                scores[comp] += hits * KEYWORD_HIT_WEIGHT\n",
    "    \n",
    "    # 3) Organization hints\n",
    "    org_norm = org.lower()\n",
    "    for hint, comp_weights in org_hints.items():\n",
    "        if not hint:\n",
    "            continue\n",
    "        if hint.lower() in org_norm:\n",
    "            for comp, w in comp_weights.items():\n",
    "                if comp in RDLS_COMPONENTS and w:\n",
    "                    scores[comp] += int(w)\n",
    "                    signals.append((abs(int(w)), f\"org:{hint}(+{w})→{comp}\"))\n",
    "    \n",
    "    \n",
    "    # 4) H5: Apply exclusion patterns (false-positive reduction)\n",
    "    if text and compiled_exclusion_patterns:\n",
    "        for comp, excl_pats in compiled_exclusion_patterns.items():\n",
    "            if comp not in RDLS_COMPONENTS:\n",
    "                continue\n",
    "            for pat in excl_pats:\n",
    "                if pat.search(text):\n",
    "                    scores[comp] = max(0, scores[comp] - 3)\n",
    "                    signals.append((3, f\"excl:{pat.pattern}(-3)→{comp}\"))\n",
    "    \n",
    "    # Determine components\n",
    "    max_score = max(scores.values()) if scores else 0\n",
    "    components = [c for c, s in scores.items() if s >= CANDIDATE_MIN_SCORE]\n",
    "    \n",
    "    # Fallback: include best component if any signal exists\n",
    "    if not components and max_score > 0:\n",
    "        best = [c for c, s in scores.items() if s == max_score]\n",
    "        components = best[:1]\n",
    "    \n",
    "    rdls_candidate = max_score >= CANDIDATE_MIN_SCORE\n",
    "    \n",
    "    # Determine confidence\n",
    "    if max_score >= CONF_HIGH:\n",
    "        confidence = 'high'\n",
    "    elif max_score >= CONF_MED:\n",
    "        confidence = 'medium'\n",
    "    else:\n",
    "        confidence = 'low'\n",
    "    \n",
    "    # Top signals for debugging\n",
    "    signals_sorted = [s for _, s in sorted(signals, key=lambda x: x[0], reverse=True)]\n",
    "    top_signals = signals_sorted[:8]\n",
    "    \n",
    "    return Classification(\n",
    "        scores=scores,\n",
    "        components=components,\n",
    "        rdls_candidate=rdls_candidate,\n",
    "        confidence=confidence,\n",
    "        top_signals=top_signals\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Classification function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Run Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 26,246 dataset files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5810fdaf924c466aa88d83a3f4e68cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying datasets:   0%|          | 0/26246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 26,246\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.1 Process All Datasets\n",
    "\"\"\"\n",
    "\n",
    "files = list(iter_json_files(DATASET_DIR))\n",
    "total = len(files)\n",
    "\n",
    "print(f\"Processing {total:,} dataset files...\")\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "errors = 0\n",
    "\n",
    "# Reset errors log\n",
    "if OUT_ERRORS_JSONL.exists():\n",
    "    OUT_ERRORS_JSONL.unlink()\n",
    "\n",
    "# Create iterator with progress bar\n",
    "iterator = tqdm(files, desc=\"Classifying datasets\") if HAS_TQDM else files\n",
    "\n",
    "for i, fp in enumerate(iterator, start=1):\n",
    "    try:\n",
    "        meta = safe_load_json(fp)\n",
    "        \n",
    "        dataset_id = normalize_text(meta.get('id'))\n",
    "        cls = classify_dataset(meta)\n",
    "        \n",
    "        excluded_by_policy = dataset_id in osm_excluded\n",
    "        resources = as_list(meta.get('resources'))\n",
    "        formats = extract_formats(resources)\n",
    "        \n",
    "        row = {\n",
    "            'dataset_id': dataset_id,\n",
    "            'name': normalize_text(meta.get('name')),\n",
    "            'title': normalize_text(meta.get('title')),\n",
    "            'organization': normalize_text(meta.get('organization')),\n",
    "            'dataset_source': normalize_text(meta.get('dataset_source')),\n",
    "            'license_title': normalize_text(meta.get('license_title')),\n",
    "            'dataset_date': normalize_text(meta.get('dataset_date')),\n",
    "            'last_modified': normalize_text(meta.get('last_modified')),\n",
    "            'data_update_frequency': normalize_text(meta.get('data_update_frequency')),\n",
    "            'groups': ';'.join([normalize_text(g) for g in as_list(meta.get('groups')) if normalize_text(g)]),\n",
    "            'tags': ';'.join([t for t in as_list(meta.get('tags')) if normalize_text(t)]),\n",
    "            'resource_count': len(resources),\n",
    "            'formats': ';'.join(formats),\n",
    "            **{f'score_{k}': int(v) for k, v in cls.scores.items()},\n",
    "            'rdls_components': ';'.join(cls.components),\n",
    "            'rdls_candidate': bool(cls.rdls_candidate),\n",
    "            'confidence': cls.confidence,\n",
    "            'excluded_by_policy': bool(excluded_by_policy),\n",
    "            'exclusion_reason': 'osm_policy' if excluded_by_policy else '',\n",
    "            'top_signals': ' | '.join(cls.top_signals),\n",
    "            'source_file': fp.name,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        with OUT_ERRORS_JSONL.open('a', encoding='utf-8') as ef:\n",
    "            ef.write(json.dumps({'file': fp.name, 'error': str(e)}, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    # Progress for non-tqdm\n",
    "    if not HAS_TQDM and i % 5000 == 0:\n",
    "        print(f\"  Processed {i:,}/{total:,}\")\n",
    "\n",
    "print(f\"\\nTotal rows: {len(rows):,}\")\n",
    "print(f\"Errors: {errors:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/classification.csv\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/rdls_included_dataset_ids.txt (13,152 IDs)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.1 Save Classification Results\n",
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Stable column order\n",
    "base_cols = [\n",
    "    'dataset_id', 'name', 'title', 'organization', 'dataset_source', 'license_title',\n",
    "    'dataset_date', 'last_modified', 'data_update_frequency', 'groups', 'tags',\n",
    "    'resource_count', 'formats',\n",
    "    'score_hazard', 'score_exposure', 'score_vulnerability_proxy', 'score_loss_impact',\n",
    "    'rdls_components', 'rdls_candidate', 'confidence',\n",
    "    'excluded_by_policy', 'exclusion_reason',\n",
    "    'top_signals', 'source_file'\n",
    "]\n",
    "\n",
    "for c in base_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = ''\n",
    "\n",
    "df = df[base_cols]\n",
    "df.to_csv(OUT_CLASSIFICATION_CSV, index=False, encoding='utf-8')\n",
    "print(f\"Wrote: {OUT_CLASSIFICATION_CSV}\")\n",
    "\n",
    "# Included IDs = RDLS candidate AND not excluded\n",
    "included_ids = df.loc[\n",
    "    (df['rdls_candidate'] == True) & (df['excluded_by_policy'] == False), \n",
    "    'dataset_id'\n",
    "].dropna().tolist()\n",
    "\n",
    "OUT_INCLUDED_IDS_TXT.write_text(\n",
    "    '\\n'.join(included_ids) + ('\\n' if included_ids else ''), \n",
    "    encoding='utf-8'\n",
    ")\n",
    "print(f\"Wrote: {OUT_INCLUDED_IDS_TXT} ({len(included_ids):,} IDs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-5-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/classification_summary.json\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION SUMMARY\n",
      "============================================================\n",
      "Total datasets: 26,246\n",
      "RDLS candidates: 16,224\n",
      "Included (after policy): 13,152\n",
      "Excluded by OSM policy: 3,649\n",
      "\n",
      "Confidence distribution:\n",
      "  high: 12,308\n",
      "  medium: 7,760\n",
      "  low: 6,178\n",
      "\n",
      "Component coverage (non-zero scores):\n",
      "  hazard: 4,111\n",
      "  exposure: 19,858\n",
      "  vulnerability_proxy: 12,952\n",
      "  loss_impact: 2,745\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.2 Generate Summary Statistics\n",
    "\"\"\"\n",
    "\n",
    "summary = {\n",
    "    'total_datasets': int(len(df)),\n",
    "    'errors': int(errors),\n",
    "    'policy': {\n",
    "        'osm_excluded_ids_loaded': int(len(osm_excluded)),\n",
    "        'datasets_excluded_by_policy': int(df['excluded_by_policy'].sum()),\n",
    "    },\n",
    "    'rdls': {\n",
    "        'candidates_total': int(df['rdls_candidate'].sum()),\n",
    "        'included_total': int(((df['rdls_candidate'] == True) & (df['excluded_by_policy'] == False)).sum()),\n",
    "    },\n",
    "    'confidence_counts': df['confidence'].value_counts(dropna=False).to_dict(),\n",
    "    'component_nonzero_counts': {\n",
    "        comp: int((df[f'score_{comp}'] > 0).sum()) for comp in RDLS_COMPONENTS\n",
    "    },\n",
    "}\n",
    "\n",
    "OUT_SUMMARY_JSON.write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "print(f\"Wrote: {OUT_SUMMARY_JSON}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CLASSIFICATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total datasets: {summary['total_datasets']:,}\")\n",
    "print(f\"RDLS candidates: {summary['rdls']['candidates_total']:,}\")\n",
    "print(f\"Included (after policy): {summary['rdls']['included_total']:,}\")\n",
    "print(f\"Excluded by OSM policy: {summary['policy']['datasets_excluded_by_policy']:,}\")\n",
    "print(f\"\\nConfidence distribution:\")\n",
    "for conf, count in summary['confidence_counts'].items():\n",
    "    print(f\"  {conf}: {count:,}\")\n",
    "print(f\"\\nComponent coverage (non-zero scores):\")\n",
    "for comp, count in summary['component_nonzero_counts'].items():\n",
    "    print(f\"  {comp}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-6-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 RDLS Candidates (by score):\n",
      "                          dataset_id                                                                                                         title                        rdls_components  score_max confidence\n",
      "d58c8a8a-5334-4d71-85d1-bfee280bd13d                                                                           Maldives - Critical infrastructures                               exposure         33       high\n",
      "7362ef2d-7282-459f-bc1b-0347076fcc12                       Bangladesh - Hazards (Drought risk, Earthquake risk, Flood risk and River erosion risk)                                 hazard         32       high\n",
      "b94a55d0-0b8e-4458-9499-8bbc6987d396 Senegal - geodatabase on admin boundaries, settlements, hydrology, transporation, land use and infrastructure                               exposure         30       high\n",
      "a87f96f8-16e6-4d51-872c-cfa54a8251ec                                                                                         GDACS RSS Information                                 hazard         30       high\n",
      "001d5cac-9f29-4ad0-af6b-9b359ac12bd9                                                              Afghanistan - Natural Disaster Incidents in 2018                     hazard;loss_impact         27       high\n",
      "bb961da8-f354-4984-9e8e-a2655e5187cb                                                                      Philippines - Risk Assessment Indicators    hazard;exposure;vulnerability_proxy         26       high\n",
      "8650634c-3b83-4479-b1ec-931c41834913                                                                          Ecuador - Risk Assessment Indicators    hazard;exposure;vulnerability_proxy         26       high\n",
      "11309139-2eee-47da-a337-6771f65dbd30                                                                         Saudi Arabia - Transportation Network                               exposure         26       high\n",
      "73cd4d78-bc1e-4a8f-b9ff-32a3fe884b3b                                                                 United Arab Emirates - Transportation Network                               exposure         26       high\n",
      "191892b0-d982-448f-a925-45f22d30057d                                                                              Algeria - Transportation Network                               exposure         26       high\n",
      "dfb26495-118a-44cc-bac1-2e5df6949949                                                                            Egypt - Risk Assessment Indicators    hazard;exposure;vulnerability_proxy         26       high\n",
      "c20e57ca-dae8-4c65-896b-bd4ba25ca0a6                                                                                     Lao PDR - Equity Analysis hazard;vulnerability_proxy;loss_impact         26       high\n",
      "96cf7a4b-6ba9-47da-a990-2be20fab1965                                                                       Bangladesh - Risk Assessment Indicators    hazard;exposure;vulnerability_proxy         26       high\n",
      "7c55bb39-83b9-4908-b033-2edbf336d784                                                                        Sri Lanka - Risk Assessment Indicators    hazard;exposure;vulnerability_proxy         26       high\n",
      "7926b801-09e6-41c9-9aa8-0f775a1a961c                                                                              Tunisia - Transportation Network                               exposure         26       high\n",
      "\n",
      "Outputs:\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/classification.csv\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/classification_summary.json\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/derived/rdls_included_dataset_ids.txt\n",
      "\n",
      "Next: Run Notebook 05 to apply manual overrides.\n",
      "\n",
      "Notebook completed: 2026-02-10T21:39:50.082423\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.1 Display Top Candidates\n",
    "\"\"\"\n",
    "\n",
    "# Add max score column\n",
    "df['score_max'] = df[[f'score_{c}' for c in RDLS_COMPONENTS]].max(axis=1)\n",
    "\n",
    "# Show top candidates\n",
    "print(\"\\nTop 15 RDLS Candidates (by score):\")\n",
    "top_candidates = df.loc[\n",
    "    df['excluded_by_policy'] == False\n",
    "].sort_values('score_max', ascending=False).head(15)\n",
    "\n",
    "display_cols = ['dataset_id', 'title', 'rdls_components', 'score_max', 'confidence']\n",
    "print(top_candidates[display_cols].to_string(index=False))\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - {OUT_CLASSIFICATION_CSV}\")\n",
    "print(f\"  - {OUT_SUMMARY_JSON}\")\n",
    "print(f\"  - {OUT_INCLUDED_IDS_TXT}\")\n",
    "\n",
    "print(f\"\\nNext: Run Notebook 05 to apply manual overrides.\")\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e4772-c3ff-491e-a0e9-611141ba7cca",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
