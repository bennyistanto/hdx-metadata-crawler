{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 13: RDLS Validation and Quality Assurance\n",
    "\n",
    "**Purpose**: Validate integrated RDLS records against the v0.3 JSON schema and produce QA reports.\n",
    "\n",
    "**Process**:\n",
    "1. Load all generated RDLS records\n",
    "2. Validate against RDLS v0.3 JSON schema\n",
    "3. Check HEVL block completeness\n",
    "4. Generate validation reports\n",
    "5. Produce final distribution package\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import jsonschema\n",
    "    from jsonschema import validate, ValidationError, Draft7Validator\n",
    "    HAS_JSONSCHEMA = True\n",
    "except ImportError:\n",
    "    HAS_JSONSCHEMA = False\n",
    "    print(\"Warning: jsonschema not installed. Schema validation will be skipped.\")\n",
    "    print(\"Install with: pip install jsonschema\")\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"JSON Schema validation: {'Available' if HAS_JSONSCHEMA else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# RDLS schema\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "\n",
    "# Input: integrated records\n",
    "INTEGRATED_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'integrated'\n",
    "EXTRACTED_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'extracted'\n",
    "\n",
    "# Output: reports and dist\n",
    "REPORTS_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'reports'\n",
    "DIST_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'dist'\n",
    "\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base: {BASE_DIR}\")\n",
    "print(f\"Reports: {REPORTS_DIR}\")\n",
    "print(f\"Dist: {DIST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Load RDLS Schema\n",
    "\"\"\"\n",
    "\n",
    "with open(RDLS_SCHEMA_PATH, 'r', encoding='utf-8') as f:\n",
    "    RDLS_SCHEMA = json.load(f)\n",
    "\n",
    "print(f\"RDLS Schema loaded: {RDLS_SCHEMA.get('$id', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load RDLS Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Find All RDLS Records\n",
    "\"\"\"\n",
    "\n",
    "def find_rdls_records(*directories: Path) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Find all RDLS JSON files in given directories.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for directory in directories:\n",
    "        if directory.exists():\n",
    "            files.extend(directory.glob('rdls_*.json'))\n",
    "    return sorted(set(files))\n",
    "\n",
    "rdls_files = find_rdls_records(INTEGRATED_DIR, EXTRACTED_DIR)\n",
    "print(f\"Found {len(rdls_files)} RDLS record files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Load Records\n",
    "\"\"\"\n",
    "\n",
    "def load_rdls_records(files: List[Path]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load RDLS records from files.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for filepath in files:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            records.append({\n",
    "                'filepath': filepath,\n",
    "                'filename': filepath.name,\n",
    "                'data': data\n",
    "            })\n",
    "        except Exception as e:\n",
    "            records.append({\n",
    "                'filepath': filepath,\n",
    "                'filename': filepath.name,\n",
    "                'load_error': str(e)\n",
    "            })\n",
    "    \n",
    "    return records\n",
    "\n",
    "rdls_records = load_rdls_records(rdls_files)\n",
    "loaded = sum(1 for r in rdls_records if 'data' in r)\n",
    "errors = sum(1 for r in rdls_records if 'load_error' in r)\n",
    "\n",
    "print(f\"Loaded: {loaded}\")\n",
    "print(f\"Load errors: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Validate Against RDLS Schema\n",
    "\"\"\"\n",
    "\n",
    "def validate_rdls_record(data: Dict[str, Any], schema: Dict[str, Any]) -> Tuple[bool, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate RDLS record against schema.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[bool, List[str]]\n",
    "        (is_valid, list_of_error_messages)\n",
    "    \"\"\"\n",
    "    if not HAS_JSONSCHEMA:\n",
    "        return True, ['Schema validation skipped - jsonschema not installed']\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    try:\n",
    "        # Use Draft7 validator\n",
    "        validator = Draft7Validator(schema)\n",
    "        \n",
    "        for error in validator.iter_errors(data):\n",
    "            path = '/'.join(str(p) for p in error.absolute_path)\n",
    "            errors.append(f\"{path}: {error.message}\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, [f\"Validation exception: {str(e)}\"]\n",
    "\n",
    "print(\"Schema validator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.2 Run Schema Validation\n",
    "\"\"\"\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "iterator = tqdm(rdls_records, desc=\"Validating\") if HAS_TQDM else rdls_records\n",
    "\n",
    "for record in iterator:\n",
    "    result = {\n",
    "        'filename': record['filename'],\n",
    "        'filepath': str(record['filepath'])\n",
    "    }\n",
    "    \n",
    "    if 'load_error' in record:\n",
    "        result['status'] = 'load_error'\n",
    "        result['errors'] = [record['load_error']]\n",
    "    else:\n",
    "        is_valid, errors = validate_rdls_record(record['data'], RDLS_SCHEMA)\n",
    "        result['status'] = 'valid' if is_valid else 'invalid'\n",
    "        result['errors'] = errors\n",
    "        \n",
    "        # Extract basic info\n",
    "        if 'datasets' in record['data'] and record['data']['datasets']:\n",
    "            ds = record['data']['datasets'][0]\n",
    "            result['id'] = ds.get('id', '')\n",
    "            result['title'] = ds.get('title', '')[:80]\n",
    "            result['risk_data_type'] = '|'.join(ds.get('risk_data_type', []))\n",
    "    \n",
    "    validation_results.append(result)\n",
    "\n",
    "df_validation = pd.DataFrame(validation_results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SCHEMA VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nTotal records: {len(df_validation)}\")\n",
    "print(f\"\\nStatus distribution:\")\n",
    "print(df_validation['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.3 Analyze Validation Errors\n",
    "\"\"\"\n",
    "\n",
    "invalid_records = df_validation[df_validation['status'] == 'invalid']\n",
    "\n",
    "if len(invalid_records) > 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VALIDATION ERRORS ({len(invalid_records)} records)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Collect all errors\n",
    "    all_errors = []\n",
    "    for errors in invalid_records['errors']:\n",
    "        if isinstance(errors, list):\n",
    "            all_errors.extend(errors)\n",
    "    \n",
    "    # Group by error type\n",
    "    error_counts = pd.Series(all_errors).value_counts()\n",
    "    \n",
    "    print(f\"\\nTop 10 error types:\")\n",
    "    for error, count in error_counts.head(10).items():\n",
    "        print(f\"  [{count}] {error[:80]}\")\n",
    "else:\n",
    "    print(\"\\nAll records passed schema validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HEVL Completeness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Check HEVL Block Completeness\n",
    "\"\"\"\n",
    "\n",
    "def check_hevl_completeness(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Check completeness of HEVL blocks.\n",
    "    \"\"\"\n",
    "    if 'datasets' not in data or not data['datasets']:\n",
    "        return {'error': 'No datasets array'}\n",
    "    \n",
    "    ds = data['datasets'][0]\n",
    "    result = {\n",
    "        'has_hazard_block': 'hazard' in ds,\n",
    "        'has_exposure_block': 'exposure' in ds,\n",
    "        'has_vulnerability_block': 'vulnerability' in ds,\n",
    "        'has_loss_block': 'loss' in ds,\n",
    "    }\n",
    "    \n",
    "    # Hazard completeness\n",
    "    if result['has_hazard_block']:\n",
    "        hazard = ds['hazard']\n",
    "        event_sets = hazard.get('event_sets', [])\n",
    "        result['hazard_event_sets'] = len(event_sets)\n",
    "        if event_sets:\n",
    "            es = event_sets[0]\n",
    "            result['hazard_has_analysis_type'] = 'analysis_type' in es\n",
    "            result['hazard_has_events'] = 'events' in es and len(es.get('events', [])) > 0\n",
    "            result['hazard_hazards_count'] = len(es.get('hazards', []))\n",
    "    \n",
    "    # Exposure completeness\n",
    "    if result['has_exposure_block']:\n",
    "        exposure = ds['exposure']\n",
    "        result['exposure_items'] = len(exposure) if isinstance(exposure, list) else 0\n",
    "        if isinstance(exposure, list) and exposure:\n",
    "            result['exposure_has_metrics'] = 'metrics' in exposure[0]\n",
    "            result['exposure_has_taxonomy'] = 'taxonomy' in exposure[0]\n",
    "    \n",
    "    # Vulnerability completeness\n",
    "    if result['has_vulnerability_block']:\n",
    "        vuln = ds['vulnerability']\n",
    "        funcs = vuln.get('functions', {})\n",
    "        result['vuln_has_functions'] = bool(funcs)\n",
    "        result['vuln_has_socioeconomic'] = 'socio_economic' in vuln\n",
    "    \n",
    "    # Loss completeness\n",
    "    if result['has_loss_block']:\n",
    "        loss = ds['loss']\n",
    "        result['loss_items'] = len(loss.get('losses', []))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run completeness check\n",
    "completeness_results = []\n",
    "\n",
    "for record in rdls_records:\n",
    "    if 'data' in record:\n",
    "        result = check_hevl_completeness(record['data'])\n",
    "        result['filename'] = record['filename']\n",
    "        completeness_results.append(result)\n",
    "\n",
    "df_completeness = pd.DataFrame(completeness_results)\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"HEVL BLOCK COMPLETENESS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nRecords analyzed: {len(df_completeness)}\")\n",
    "print(f\"\\nBlock presence:\")\n",
    "for col in ['has_hazard_block', 'has_exposure_block', 'has_vulnerability_block', 'has_loss_block']:\n",
    "    if col in df_completeness:\n",
    "        count = df_completeness[col].sum()\n",
    "        print(f\"  {col}: {count} ({count/len(df_completeness)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 Hazard Block Quality\n",
    "\"\"\"\n",
    "\n",
    "hazard_records = df_completeness[df_completeness.get('has_hazard_block', False) == True]\n",
    "\n",
    "if len(hazard_records) > 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HAZARD BLOCK QUALITY ({len(hazard_records)} records)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if 'hazard_has_analysis_type' in hazard_records:\n",
    "        print(f\"\\nWith analysis_type: {hazard_records['hazard_has_analysis_type'].sum()}\")\n",
    "    if 'hazard_has_events' in hazard_records:\n",
    "        print(f\"With events: {hazard_records['hazard_has_events'].sum()}\")\n",
    "    if 'hazard_hazards_count' in hazard_records:\n",
    "        print(f\"Avg hazards per record: {hazard_records['hazard_hazards_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Save Validation Report\n",
    "\"\"\"\n",
    "\n",
    "# Prepare validation report\n",
    "validation_export = df_validation.copy()\n",
    "validation_export['errors'] = validation_export['errors'].apply(\n",
    "    lambda x: '|'.join(x) if isinstance(x, list) else str(x)\n",
    ")\n",
    "\n",
    "validation_file = REPORTS_DIR / 'schema_validation_report.csv'\n",
    "validation_export.to_csv(validation_file, index=False)\n",
    "print(f\"Saved: {validation_file}\")\n",
    "\n",
    "# Save completeness report\n",
    "completeness_file = REPORTS_DIR / 'hevl_completeness_report.csv'\n",
    "df_completeness.to_csv(completeness_file, index=False)\n",
    "print(f\"Saved: {completeness_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Generate Summary Markdown Report\n",
    "\"\"\"\n",
    "\n",
    "def generate_summary_report(\n",
    "    validation_df: pd.DataFrame,\n",
    "    completeness_df: pd.DataFrame,\n",
    "    output_path: Path\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate markdown summary report.\n",
    "    \"\"\"\n",
    "    total = len(validation_df)\n",
    "    valid = (validation_df['status'] == 'valid').sum()\n",
    "    invalid = (validation_df['status'] == 'invalid').sum()\n",
    "    \n",
    "    report = f\"\"\"# RDLS Validation and QA Report\n",
    "\n",
    "**Generated**: {datetime.now().isoformat()}\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Total Records | {total} |\n",
    "| Schema Valid | {valid} ({valid/total*100:.1f}%) |\n",
    "| Schema Invalid | {invalid} ({invalid/total*100:.1f}%) |\n",
    "\n",
    "## HEVL Block Coverage\n",
    "\n",
    "| Component | Records | Percentage |\n",
    "|-----------|---------|------------|\n",
    "| Hazard | {completeness_df.get('has_hazard_block', pd.Series([False])).sum()} | {completeness_df.get('has_hazard_block', pd.Series([False])).mean()*100:.1f}% |\n",
    "| Exposure | {completeness_df.get('has_exposure_block', pd.Series([False])).sum()} | {completeness_df.get('has_exposure_block', pd.Series([False])).mean()*100:.1f}% |\n",
    "| Vulnerability | {completeness_df.get('has_vulnerability_block', pd.Series([False])).sum()} | {completeness_df.get('has_vulnerability_block', pd.Series([False])).mean()*100:.1f}% |\n",
    "| Loss | {completeness_df.get('has_loss_block', pd.Series([False])).sum()} | {completeness_df.get('has_loss_block', pd.Series([False])).mean()*100:.1f}% |\n",
    "\n",
    "## Validation Status\n",
    "\n",
    "### Valid Records\n",
    "Records that pass RDLS v0.3 JSON schema validation.\n",
    "\n",
    "### Invalid Records\n",
    "Records with schema validation errors. See `schema_validation_report.csv` for details.\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "- `schema_validation_report.csv` - Detailed validation results\n",
    "- `hevl_completeness_report.csv` - HEVL block completeness analysis\n",
    "- `rdls_index.csv` - Index of all RDLS records\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review invalid records and fix schema errors\n",
    "2. Enrich records with manual data where needed\n",
    "3. Re-run validation after corrections\n",
    "\n",
    "---\n",
    "*Report generated by HDX-RDLS Pipeline Notebook 13*\n",
    "\"\"\"\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"Saved: {output_path}\")\n",
    "\n",
    "summary_file = REPORTS_DIR / 'rdls_validation_summary.md'\n",
    "generate_summary_report(df_validation, df_completeness, summary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Distribution Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Copy Valid Records to Dist\n",
    "\"\"\"\n",
    "\n",
    "def create_distribution(\n",
    "    validation_df: pd.DataFrame,\n",
    "    rdls_records: List[Dict],\n",
    "    dist_dir: Path,\n",
    "    include_invalid: bool = False\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create distribution package with valid records.\n",
    "    \"\"\"\n",
    "    records_dir = dist_dir / 'records'\n",
    "    records_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    stats = {'copied': 0, 'skipped': 0}\n",
    "    \n",
    "    # Get valid filenames\n",
    "    if include_invalid:\n",
    "        valid_files = set(validation_df['filename'])\n",
    "    else:\n",
    "        valid_files = set(validation_df[validation_df['status'] == 'valid']['filename'])\n",
    "    \n",
    "    for record in rdls_records:\n",
    "        if record['filename'] in valid_files and 'data' in record:\n",
    "            dest = records_dir / record['filename']\n",
    "            shutil.copy2(record['filepath'], dest)\n",
    "            stats['copied'] += 1\n",
    "        else:\n",
    "            stats['skipped'] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Create distribution (include all for now)\n",
    "dist_stats = create_distribution(df_validation, rdls_records, DIST_DIR, include_invalid=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DISTRIBUTION PACKAGE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Records copied: {dist_stats['copied']}\")\n",
    "print(f\"Records skipped: {dist_stats['skipped']}\")\n",
    "print(f\"Location: {DIST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.2 Copy Reports and Index\n",
    "\"\"\"\n",
    "\n",
    "# Copy reports to dist\n",
    "dist_reports = DIST_DIR / 'reports'\n",
    "dist_reports.mkdir(exist_ok=True)\n",
    "\n",
    "for report_file in REPORTS_DIR.glob('*'):\n",
    "    shutil.copy2(report_file, dist_reports / report_file.name)\n",
    "\n",
    "# Copy index if exists\n",
    "for source_dir in [INTEGRATED_DIR, EXTRACTED_DIR]:\n",
    "    for index_file in source_dir.glob('rdls_index.*'):\n",
    "        shutil.copy2(index_file, DIST_DIR / index_file.name)\n",
    "\n",
    "print(f\"Reports copied to: {dist_reports}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.3 Create ZIP Archive (Optional)\n",
    "\"\"\"\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def create_zip_archive(dist_dir: Path, output_name: str = 'rdls_hdx_package') -> Path:\n",
    "    \"\"\"\n",
    "    Create ZIP archive of distribution.\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d')\n",
    "    zip_path = dist_dir.parent / f\"{output_name}_{timestamp}.zip\"\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for file_path in dist_dir.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                arcname = file_path.relative_to(dist_dir)\n",
    "                zf.write(file_path, arcname)\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "# Create ZIP\n",
    "zip_path = create_zip_archive(DIST_DIR)\n",
    "print(f\"\\nZIP archive created: {zip_path}\")\n",
    "print(f\"Size: {zip_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7.1 Pipeline Summary\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HDX â†’ RDLS PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "SUMMARY\n",
    "-------\n",
    "Total RDLS records generated: {len(rdls_records)}\n",
    "Schema valid: {(df_validation['status'] == 'valid').sum()}\n",
    "Schema invalid: {(df_validation['status'] == 'invalid').sum()}\n",
    "\n",
    "HEVL COVERAGE\n",
    "-------------\n",
    "Records with Hazard block: {df_completeness.get('has_hazard_block', pd.Series([False])).sum()}\n",
    "Records with Exposure block: {df_completeness.get('has_exposure_block', pd.Series([False])).sum()}\n",
    "Records with Vulnerability block: {df_completeness.get('has_vulnerability_block', pd.Series([False])).sum()}\n",
    "Records with Loss block: {df_completeness.get('has_loss_block', pd.Series([False])).sum()}\n",
    "\n",
    "OUTPUT LOCATIONS\n",
    "----------------\n",
    "Distribution: {DIST_DIR}\n",
    "Reports: {REPORTS_DIR}\n",
    "ZIP Archive: {zip_path}\n",
    "\n",
    "NEXT STEPS\n",
    "----------\n",
    "1. Review validation errors in schema_validation_report.csv\n",
    "2. Enrich high-priority records manually using RDLS Metadata Editor\n",
    "3. Merge with manually curated RDLS records\n",
    "4. Publish to RDLS catalog\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
