{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 08: HDX Metadata Signal Analysis for HEVL Inference\n",
    "\n",
    "**Purpose**: Analyze 26,246 HDX metadata files to identify extractable signals for populating RDLS v0.3 HEVL (Hazard, Exposure, Vulnerability, Loss) component blocks.\n",
    "\n",
    "**Outputs**:\n",
    "- Signal frequency analysis (hazard types, exposure categories, etc.)\n",
    "- Duplication pattern report\n",
    "- HEVL coverage potential assessment\n",
    "- Signal dictionary foundation for downstream extraction\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis started: 2026-02-11T06:37:39.927614\n",
      "Python packages loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\n",
    "Standard data science stack for metadata analysis.\n",
    "All packages are commonly available via pip/conda.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Optional: progress bar for long operations\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not available. Install with 'pip install tqdm' for progress bars.\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(f\"Analysis started: {datetime.now().isoformat()}\")\n",
    "print(f\"Python packages loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler\n",
      "Dataset metadata: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/dataset_metadata\n",
      "Output directory: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/analysis\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Define Paths and Constants\n",
    "\n",
    "All paths are relative to the repository root for reproducibility.\n",
    "Adjust BASE_DIR if running from a different location.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# PATH CONFIGURATION - Adjust if needed\n",
    "# ============================================================================\n",
    "\n",
    "# Repository root (parent of 'notebook' folder)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input paths\n",
    "DATASET_METADATA_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'dataset_metadata'\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'analysis'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify paths exist\n",
    "assert DATASET_METADATA_DIR.exists(), f\"Dataset metadata directory not found: {DATASET_METADATA_DIR}\"\n",
    "assert RDLS_SCHEMA_PATH.exists(), f\"RDLS schema not found: {RDLS_SCHEMA_PATH}\"\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Dataset metadata: {DATASET_METADATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "CLEANUP_MODE = \"replace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output cleanup [NB 08 Signal Analysis]:\n",
      "  hdx_hevl_signal_analysis.csv            : 1 files\n",
      "  hdx_hevl_signal_summary.json            : 1 files\n",
      "  hdx_high_signal_records.csv             : 1 files\n",
      "  Cleaned 3 files. Ready for fresh output.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deleted': 3, 'skipped': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Clean Previous Outputs\n",
    "\n",
    "Remove stale output files from previous runs (controlled by CLEANUP_MODE).\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice, defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "# ── Run cleanup ────────────────────────────────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    OUTPUT_DIR,\n",
    "    patterns=[\n",
    "        \"hdx_hevl_signal_analysis.csv\",\n",
    "        \"hdx_hevl_signal_summary.json\",\n",
    "        \"hdx_high_signal_records.csv\",\n",
    "    ],\n",
    "    label=\"NB 08 Signal Analysis\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RDLS Key Codelists (closed - must match exactly)\n",
      "============================================================\n",
      "\n",
      "hazard_type:\n",
      "  ['coastal_flood', 'convective_storm', 'drought', 'extreme_temperature', 'flood', 'wildfire', 'strong_wind', 'earthquake', 'landslide', 'tsunami', 'volcanic']\n",
      "\n",
      "process_type:\n",
      "  ['coastal_flood', 'storm_surge', 'tornado', 'agricultural_drought', 'hydrological_drought', 'meteorological_drought', 'socioeconomic_drought', 'primary_rupture', 'secondary_rupture', 'ground_motion', 'liquefaction', 'extreme_cold', 'extreme_heat', 'fluvial_flood', 'pluvial_flood', 'groundwater_flood', 'snow_avalanche', 'landslide_general', 'landslide_rockslide', 'landslide_mudflow', 'landslide_rockfall', 'tsunami', 'ashfall', 'volcano_ballistics', 'lahar', 'lava', 'pyroclastic_flow', 'wildfire', 'extratropical_cyclone', 'tropical_cyclone']\n",
      "\n",
      "exposure_category:\n",
      "  ['agriculture', 'buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "\n",
      "analysis_type:\n",
      "  ['probabilistic', 'deterministic', 'empirical']\n",
      "\n",
      "risk_data_type:\n",
      "  ['hazard', 'exposure', 'vulnerability', 'loss']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.3 Load RDLS Schema Codelists\n",
    "\n",
    "Extract closed codelists from RDLS v0.3 schema to use as reference\n",
    "for signal matching. These are the valid values we need to map to.\n",
    "\"\"\"\n",
    "\n",
    "def load_rdls_codelists(schema_path: Path) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract codelist values from RDLS schema.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    schema_path : Path\n",
    "        Path to rdls_schema_v0.3.json\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[str]]\n",
    "        Dictionary mapping codelist names to their valid values\n",
    "    \"\"\"\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    codelists = {}\n",
    "    defs = schema.get('$defs', {})\n",
    "    \n",
    "    # Extract enum values from $defs\n",
    "    for name, definition in defs.items():\n",
    "        if 'enum' in definition:\n",
    "            codelists[name] = definition['enum']\n",
    "        elif definition.get('type') == 'string' and 'enum' in definition:\n",
    "            codelists[name] = definition['enum']\n",
    "    \n",
    "    return codelists\n",
    "\n",
    "# Load codelists\n",
    "RDLS_CODELISTS = load_rdls_codelists(RDLS_SCHEMA_PATH)\n",
    "\n",
    "# Display key codelists for HEVL\n",
    "key_codelists = ['hazard_type', 'process_type', 'exposure_category', 'analysis_type', 'risk_data_type']\n",
    "print(\"=\" * 60)\n",
    "print(\"RDLS Key Codelists (closed - must match exactly)\")\n",
    "print(\"=\" * 60)\n",
    "for cl in key_codelists:\n",
    "    if cl in RDLS_CODELISTS:\n",
    "        print(f\"\\n{cl}:\")\n",
    "        print(f\"  {RDLS_CODELISTS[cl]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HDX metadata files...\n",
      "Limit: All files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885bf870961842169b00f3c0498867dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading metadata:   0%|          | 0/26246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded: 26,246 records\n",
      "Errors: 0 files\n",
      "\n",
      "Total records available for analysis: 26,246\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 Load All HDX Metadata Files\n",
    "\n",
    "Read all JSON files from dataset_metadata directory.\n",
    "This may take a few minutes for 26,000+ files.\n",
    "\"\"\"\n",
    "\n",
    "def load_hdx_metadata(metadata_dir: Path, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load HDX metadata JSON files from directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata_dir : Path\n",
    "        Directory containing HDX metadata JSON files\n",
    "    limit : Optional[int]\n",
    "        Maximum number of files to load (for testing). None = all files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, Any]]\n",
    "        List of parsed metadata dictionaries\n",
    "    \"\"\"\n",
    "    json_files = list(metadata_dir.glob('*.json'))\n",
    "    \n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "    \n",
    "    records = []\n",
    "    errors = []\n",
    "    \n",
    "    iterator = tqdm(json_files, desc=\"Loading metadata\") if HAS_TQDM else json_files\n",
    "    \n",
    "    for filepath in iterator:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                data['_source_file'] = filepath.name\n",
    "                records.append(data)\n",
    "        except Exception as e:\n",
    "            errors.append({'file': filepath.name, 'error': str(e)})\n",
    "    \n",
    "    print(f\"\\nLoaded: {len(records):,} records\")\n",
    "    print(f\"Errors: {len(errors):,} files\")\n",
    "    \n",
    "    return records, errors\n",
    "\n",
    "# Load all metadata (set limit=1000 for faster testing)\n",
    "LOAD_LIMIT = None  # Set to integer for testing, None for full load\n",
    "\n",
    "print(f\"Loading HDX metadata files...\")\n",
    "print(f\"Limit: {'All files' if LOAD_LIMIT is None else f'{LOAD_LIMIT:,} files'}\")\n",
    "\n",
    "hdx_records, load_errors = load_hdx_metadata(DATASET_METADATA_DIR, limit=LOAD_LIMIT)\n",
    "\n",
    "# Store for later use\n",
    "print(f\"\\nTotal records available for analysis: {len(hdx_records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (26246, 20)\n",
      "\n",
      "Columns: ['id', 'name', 'title', 'notes', 'organization', 'dataset_source', 'groups', 'tags', 'license_title', 'methodology', 'methodology_other', 'caveats', 'dataset_date', 'last_modified', 'data_update_frequency', 'resource_count', 'resource_formats', 'resource_names', '_source_file', '_all_text']\n",
      "\n",
      "Memory usage: 399.3 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Convert to DataFrame for Analysis\n",
    "\n",
    "Flatten key fields into a DataFrame for efficient analysis.\n",
    "\"\"\"\n",
    "\n",
    "def extract_flat_record(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract key fields from HDX record into flat dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    record : Dict[str, Any]\n",
    "        Raw HDX metadata record\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Flattened record with key fields\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'id': record.get('id', ''),\n",
    "        'name': record.get('name', ''),\n",
    "        'title': record.get('title', ''),\n",
    "        'notes': record.get('notes', ''),\n",
    "        'organization': record.get('organization', ''),\n",
    "        'dataset_source': record.get('dataset_source', ''),\n",
    "        'groups': '|'.join(record.get('groups', [])),\n",
    "        'tags': '|'.join(record.get('tags', [])),\n",
    "        'license_title': record.get('license_title', ''),\n",
    "        'methodology': record.get('methodology', ''),\n",
    "        'methodology_other': record.get('methodology_other', ''),\n",
    "        'caveats': record.get('caveats', ''),\n",
    "        'dataset_date': record.get('dataset_date', ''),\n",
    "        'last_modified': record.get('last_modified', ''),\n",
    "        'data_update_frequency': record.get('data_update_frequency', ''),\n",
    "        'resource_count': len(record.get('resources', [])),\n",
    "        'resource_formats': '|'.join(set(r.get('format', '') for r in record.get('resources', []))),\n",
    "        'resource_names': '|'.join(r.get('name', '') for r in record.get('resources', [])),\n",
    "        '_source_file': record.get('_source_file', ''),\n",
    "        # Concatenate all text fields for pattern matching\n",
    "        '_all_text': ' '.join(filter(None, [\n",
    "            record.get('title', ''),\n",
    "            record.get('name', ''),\n",
    "            record.get('notes', ''),\n",
    "            ' '.join(record.get('tags', [])),\n",
    "            ' '.join(r.get('name', '') for r in record.get('resources', [])),\n",
    "            ' '.join(r.get('description', '') for r in record.get('resources', []))\n",
    "        ])).lower()\n",
    "    }\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([extract_flat_record(r) for r in hdx_records])\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HDX METADATA CORPUS OVERVIEW\n",
      "============================================================\n",
      "\n",
      "Total datasets: 26,246\n",
      "Unique dataset IDs: 26,246\n",
      "Unique dataset names: 26,246\n",
      "Unique organizations: 358\n",
      "\n",
      "--- Top 15 Organizations by Dataset Count ---\n",
      "  World Bank Group: 4,792\n",
      "  Humanitarian OpenStreetMap Team (HOT): 2,593\n",
      "  WorldPop: 1,569\n",
      "  United Nations Satellite Centre (UNOSAT): 1,452\n",
      "  UNHCR - The UN Refugee Agency: 1,132\n",
      "  FEWS NET: 833\n",
      "  World Health Organization: 678\n",
      "  HeiGIT (Heidelberg Institute for Geoinformation Technology): 661\n",
      "  HDX: 603\n",
      "  Kontur: 502\n",
      "  WFP - World Food Programme: 492\n",
      "  Copernicus: 478\n",
      "  Food and Agriculture Organization (FAO) of the United Nations: 441\n",
      "  Internal Displacement Monitoring Centre (IDMC): 426\n",
      "  UNICEF Data and Analytics (HQ): 292\n",
      "\n",
      "--- Resource Format Distribution ---\n",
      "  CSV: 12,603\n",
      "  SHP: 6,248\n",
      "  GeoJSON: 4,770\n",
      "  Geopackage: 3,892\n",
      "  KML: 3,337\n",
      "  XLSX: 3,336\n",
      "  GeoTIFF: 2,433\n",
      "  Geodatabase: 1,310\n",
      "  PDF: 1,199\n",
      "  Web App: 925\n",
      "  Garmin IMG: 560\n",
      "  XML: 293\n",
      "  XLS: 289\n",
      "  JSON: 275\n",
      "  PNG: 107\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.3 Basic Statistics Overview\n",
    "\n",
    "Summary statistics for the HDX metadata corpus.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HDX METADATA CORPUS OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal datasets: {len(df):,}\")\n",
    "print(f\"Unique dataset IDs: {df['id'].nunique():,}\")\n",
    "print(f\"Unique dataset names: {df['name'].nunique():,}\")\n",
    "print(f\"Unique organizations: {df['organization'].nunique():,}\")\n",
    "\n",
    "# Top organizations\n",
    "print(f\"\\n--- Top 15 Organizations by Dataset Count ---\")\n",
    "org_counts = df['organization'].value_counts().head(15)\n",
    "for org, count in org_counts.items():\n",
    "    print(f\"  {org}: {count:,}\")\n",
    "\n",
    "# Resource format distribution\n",
    "print(f\"\\n--- Resource Format Distribution ---\")\n",
    "all_formats = []\n",
    "for formats in df['resource_formats'].dropna():\n",
    "    all_formats.extend(formats.split('|'))\n",
    "format_counts = Counter(f for f in all_formats if f)\n",
    "for fmt, count in format_counts.most_common(15):\n",
    "    print(f\"  {fmt}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HEVL Signal Pattern Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEVL signal patterns defined (hardcoded):\n",
      "  - Hazard types: 11 patterns\n",
      "  - Process types: 12 patterns\n",
      "  - Exposure categories: 7 patterns\n",
      "  - Analysis types: 3 patterns\n",
      "  - Vulnerability/Loss indicators: 3 patterns\n",
      "\n",
      "Loaded signal dictionary: signal_dictionary.yaml\n",
      "  Hazard types: 11\n",
      "  Process types: 12\n",
      "  Exposure categories: 7\n",
      "  Exclusion patterns: 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.1 Define HEVL Signal Patterns\n",
    "\n",
    "Regular expression patterns to detect HEVL-relevant signals in text.\n",
    "These patterns are designed to map to RDLS codelist values.\n",
    "\n",
    "NOTE: These patterns should be validated with unit tests in a future iteration\n",
    "to ensure precision/recall against a labeled ground truth set.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# HAZARD TYPE PATTERNS\n",
    "# Maps to: RDLS hazard_type codelist\n",
    "# ============================================================================\n",
    "HAZARD_TYPE_PATTERNS = {\n",
    "    'flood': r'\\b(flood|flooding|fluvial|pluvial|inundation)\\b',\n",
    "    'coastal_flood': r'\\b(coastal.?flood|storm.?surge|tidal.?flood|sea.?level)\\b',\n",
    "    'earthquake': r'\\b(earthquake|seismic|quake|tremor|ground.?motion)\\b',\n",
    "    'tsunami': r'\\b(tsunami|tidal.?wave)\\b',\n",
    "    'landslide': r'\\b(landslide|mudslide|rockfall|debris.?flow|mass.?movement)\\b',\n",
    "    'volcanic': r'\\b(volcan|lava|pyroclastic|ash.?fall|eruption)\\b',\n",
    "    'drought': r'\\b(drought|water.?scarcity|aridity)\\b',\n",
    "    'wildfire': r'\\b(wildfire|forest.?fire|bushfire|fire.?hazard)\\b',\n",
    "    'strong_wind': r'\\b(wind|gust|gale)\\b',\n",
    "    'convective_storm': r'\\b(cyclone|typhoon|hurricane|tropical.?storm|tornado|thunderstorm)\\b',\n",
    "    'extreme_temperature': r'\\b(heat.?wave|cold.?wave|extreme.?temperature|frost|freeze)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# HAZARD PROCESS TYPE PATTERNS (more specific)\n",
    "# Maps to: RDLS process_type codelist\n",
    "# ============================================================================\n",
    "PROCESS_TYPE_PATTERNS = {\n",
    "    'fluvial_flood': r'\\b(fluvial|river.?flood|riverine)\\b',\n",
    "    'pluvial_flood': r'\\b(pluvial|flash.?flood|surface.?water|urban.?flood)\\b',\n",
    "    'coastal_flood': r'\\b(coastal.?flood|storm.?surge|tidal)\\b',\n",
    "    'ground_motion': r'\\b(ground.?motion|pga|pgv|shaking|intensity)\\b',\n",
    "    'liquefaction': r'\\b(liquefaction)\\b',\n",
    "    'tornado': r'\\b(tornado|twister)\\b',\n",
    "    'tropical_cyclone': r'\\b(tropical.?cyclone|typhoon|hurricane|cyclone)\\b',\n",
    "    'storm_surge': r'\\b(storm.?surge|sea.?surge)\\b',\n",
    "    'meteorological_drought': r'\\b(meteorological.?drought|rainfall.?deficit|precipitation.?deficit)\\b',\n",
    "    'agricultural_drought': r'\\b(agricultural.?drought|crop.?drought|soil.?moisture.?deficit)\\b',\n",
    "    'surface_rupture': r'\\b(surface.?rupture|fault.?rupture)\\b',\n",
    "    'ash_fall': r'\\b(ash.?fall|tephra|volcanic.?ash)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# EXPOSURE CATEGORY PATTERNS\n",
    "# Maps to: RDLS exposure_category codelist\n",
    "# ============================================================================\n",
    "EXPOSURE_CATEGORY_PATTERNS = {\n",
    "    'buildings': r'\\b(building|structure|dwelling|house|residential|commercial|industrial)\\b',\n",
    "    'infrastructure': r'\\b(infrastructure|road|bridge|railway|transport|power.?line|utility|airport|port)\\b',\n",
    "    'population': r'\\b(population|people|inhabitant|resident|demographic|census)\\b',\n",
    "    'agriculture': r'\\b(agriculture|crop|farm|livestock|agricultural|cultivation)\\b',\n",
    "    'natural_environment': r'\\b(environment|ecosystem|forest|wetland|biodiversity|natural.?resource)\\b',\n",
    "    'economic_indicator': r'\\b(gdp|gnp|gni|economic.?indicator|economic.?loss|damage.?cost|financial.?indicator|economic.?value|trade|export|import)\\b',\n",
    "    'development_index': r'\\b(hdi|human.?development|poverty.?index|vulnerability.?index|svi|social.?vulnerability|development.?indicator|gini|inequality)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS TYPE PATTERNS\n",
    "# Maps to: RDLS analysis_type codelist\n",
    "# ============================================================================\n",
    "ANALYSIS_TYPE_PATTERNS = {\n",
    "    'probabilistic': r'\\b(probabilistic|return.?period|rp\\d+|annual.?exceedance|aep|frequency|stochastic|\\d+.?year.?event)\\b',\n",
    "    'deterministic': r'\\b(deterministic|index|susceptibility|ranking|score|classification)\\b',\n",
    "    'empirical': r'\\b(empirical|historical|observed|actual|recorded|past.?event)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# RETURN PERIOD EXTRACTION PATTERN\n",
    "# ============================================================================\n",
    "RETURN_PERIOD_PATTERN = r'(?:return.?period|rp|recurrence).?(?:of)?\\s*(\\d+)\\s*(?:year|yr)?|(?:(\\d+).?year.?(?:return|event|flood|storm))|(\\d+)\\s*yr'\n",
    "\n",
    "# ============================================================================\n",
    "# VULNERABILITY/LOSS INDICATORS\n",
    "# ============================================================================\n",
    "VULNERABILITY_PATTERNS = {\n",
    "    'vulnerability': r'\\b(vulnerability|fragility|damage.?function|loss.?function|susceptibility)\\b',\n",
    "    'loss': r'\\b(loss|damage|impact|economic.?loss|casualty|fatality|injury)\\b',\n",
    "    'risk_assessment': r'\\b(risk.?assessment|risk.?analysis|risk.?model|cat.?model)\\b',\n",
    "}\n",
    "\n",
    "print(\"HEVL signal patterns defined (hardcoded):\")\n",
    "print(f\"  - Hazard types: {len(HAZARD_TYPE_PATTERNS)} patterns\")\n",
    "print(f\"  - Process types: {len(PROCESS_TYPE_PATTERNS)} patterns\")\n",
    "print(f\"  - Exposure categories: {len(EXPOSURE_CATEGORY_PATTERNS)} patterns\")\n",
    "print(f\"  - Analysis types: {len(ANALYSIS_TYPE_PATTERNS)} patterns\")\n",
    "print(f\"  - Vulnerability/Loss indicators: {len(VULNERABILITY_PATTERNS)} patterns\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD SIGNAL DICTIONARY (centralized pattern source)\n",
    "# Merges additional patterns from signal_dictionary.yaml without overwriting\n",
    "# existing hardcoded patterns. This ensures UNOSAT event codes (H8) and other\n",
    "# extended patterns are available for signal detection.\n",
    "# ============================================================================\n",
    "SIGNAL_DICT_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'config' / 'signal_dictionary.yaml'\n",
    "if SIGNAL_DICT_PATH.exists():\n",
    "    try:\n",
    "        import yaml\n",
    "        with open(SIGNAL_DICT_PATH, 'r', encoding='utf-8') as f:\n",
    "            signal_dict = yaml.safe_load(f)\n",
    "\n",
    "        # Merge hazard type patterns\n",
    "        if 'hazard_type' in signal_dict:\n",
    "            for htype, info in signal_dict['hazard_type'].items():\n",
    "                if htype not in HAZARD_TYPE_PATTERNS:\n",
    "                    # Combine patterns into one OR group\n",
    "                    combined = '|'.join(info.get('patterns', []))\n",
    "                    if combined:\n",
    "                        HAZARD_TYPE_PATTERNS[htype] = combined\n",
    "\n",
    "        # Merge process type patterns\n",
    "        if 'process_type' in signal_dict:\n",
    "            for ptype, info in signal_dict['process_type'].items():\n",
    "                if ptype not in PROCESS_TYPE_PATTERNS:\n",
    "                    combined = '|'.join(info.get('patterns', []))\n",
    "                    if combined:\n",
    "                        PROCESS_TYPE_PATTERNS[ptype] = combined\n",
    "\n",
    "        # Merge exposure category patterns\n",
    "        if 'exposure_category' in signal_dict:\n",
    "            for cat, info in signal_dict['exposure_category'].items():\n",
    "                if cat not in EXPOSURE_CATEGORY_PATTERNS:\n",
    "                    combined = '|'.join(info.get('patterns', []))\n",
    "                    if combined:\n",
    "                        EXPOSURE_CATEGORY_PATTERNS[cat] = combined\n",
    "\n",
    "        # Load exclusion patterns for false positive filtering\n",
    "        EXCLUSION_PATTERNS = {}\n",
    "        if 'exclusion_patterns' in signal_dict:\n",
    "            for category, patterns in signal_dict['exclusion_patterns'].items():\n",
    "                EXCLUSION_PATTERNS[category] = [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "\n",
    "        print(f\"\\nLoaded signal dictionary: {SIGNAL_DICT_PATH.name}\")\n",
    "        print(f\"  Hazard types: {len(HAZARD_TYPE_PATTERNS)}\")\n",
    "        print(f\"  Process types: {len(PROCESS_TYPE_PATTERNS)}\")\n",
    "        print(f\"  Exposure categories: {len(EXPOSURE_CATEGORY_PATTERNS)}\")\n",
    "        print(f\"  Exclusion patterns: {len(EXCLUSION_PATTERNS)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not load signal dictionary: {e}\")\n",
    "        EXCLUSION_PATTERNS = {}\n",
    "else:\n",
    "    print(f\"\\nNOTE: Signal dictionary not found at {SIGNAL_DICT_PATH}\")\n",
    "    EXCLUSION_PATTERNS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting HEVL signals from metadata...\n",
      "Signal extraction complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.2 Apply Pattern Matching to Corpus\n",
    "\n",
    "Scan all records for HEVL signals using defined patterns.\n",
    "\"\"\"\n",
    "\n",
    "def extract_patterns(text: str, patterns: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all matching pattern names in text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to search (should be lowercase)\n",
    "    patterns : Dict[str, str]\n",
    "        Dictionary of {name: regex_pattern}\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of matched pattern names\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for name, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            matches.append(name)\n",
    "    return matches\n",
    "\n",
    "def extract_return_periods(text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Extract return period values from text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to search\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        List of extracted return period values (years)\n",
    "    \"\"\"\n",
    "    # Filter out year-like values (1900-2100) that are likely dates, not return periods\n",
    "    YEAR_RANGE = range(1900, 2101)\n",
    "\n",
    "    rp_values = []\n",
    "    for match in re.finditer(RETURN_PERIOD_PATTERN, text, re.IGNORECASE):\n",
    "        for group in match.groups():\n",
    "            if group:\n",
    "                try:\n",
    "                    rp = int(group)\n",
    "                    if 1 <= rp <= 100000:  # Reasonable range\n",
    "                        if rp not in YEAR_RANGE:\n",
    "                            rp_values.append(rp)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    return sorted(set(rp_values))\n",
    "\n",
    "# Apply pattern matching to all records\n",
    "print(\"Extracting HEVL signals from metadata...\")\n",
    "\n",
    "df['hazard_types'] = df['_all_text'].apply(lambda x: extract_patterns(x, HAZARD_TYPE_PATTERNS))\n",
    "df['process_types'] = df['_all_text'].apply(lambda x: extract_patterns(x, PROCESS_TYPE_PATTERNS))\n",
    "df['exposure_categories'] = df['_all_text'].apply(lambda x: extract_patterns(x, EXPOSURE_CATEGORY_PATTERNS))\n",
    "df['analysis_types'] = df['_all_text'].apply(lambda x: extract_patterns(x, ANALYSIS_TYPE_PATTERNS))\n",
    "df['vuln_loss_indicators'] = df['_all_text'].apply(lambda x: extract_patterns(x, VULNERABILITY_PATTERNS))\n",
    "df['return_periods'] = df['_all_text'].apply(extract_return_periods)\n",
    "\n",
    "# Create binary flags\n",
    "df['has_hazard'] = df['hazard_types'].apply(lambda x: len(x) > 0)\n",
    "df['has_exposure'] = df['exposure_categories'].apply(lambda x: len(x) > 0)\n",
    "df['has_vulnerability'] = df['vuln_loss_indicators'].apply(lambda x: 'vulnerability' in x)\n",
    "df['has_loss'] = df['vuln_loss_indicators'].apply(lambda x: 'loss' in x or 'risk_assessment' in x)\n",
    "df['has_return_period'] = df['return_periods'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "print(\"Signal extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HEVL SIGNAL DETECTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "--- Component Detection Rates ---\n",
      "Component                 Count   Percentage\n",
      "---------------------------------------------\n",
      "Hazard signal             2,517         9.6%\n",
      "Exposure signal          18,673        71.1%\n",
      "Vulnerability signal        205         0.8%\n",
      "Loss signal               2,407         9.2%\n",
      "Return period found          56         0.2%\n",
      "\n",
      "--- HEVL Component Combinations ---\n",
      "Combination          Count   Percentage  Description\n",
      "----------------------------------------------------------------------\n",
      "-E--                15,356        58.5%  Exposure\n",
      "----                 6,298        24.0%  No HEVL signals\n",
      "-E-L                 1,667         6.4%  Exposure+Loss\n",
      "HE--                 1,240         4.7%  Hazard+Exposure\n",
      "H---                   776         3.0%  Hazard\n",
      "HE-L                   250         1.0%  Hazard+Exposure+Loss\n",
      "---L                   241         0.9%  Loss\n",
      "H--L                   213         0.8%  Hazard+Loss\n",
      "-EV-                   103         0.4%  Exposure+Vulnerability\n",
      "--V-                    43         0.2%  Vulnerability\n",
      "HEV-                    22         0.1%  Hazard+Exposure+Vulnerability\n",
      "-EVL                    20         0.1%  Exposure+Vulnerability+Loss\n",
      "HEVL                    15         0.1%  Hazard+Exposure+Vulnerability+Loss\n",
      "--VL                     1         0.0%  Vulnerability+Loss\n",
      "H-V-                     1         0.0%  Hazard+Vulnerability\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.3 Signal Detection Summary Statistics\n",
    "\n",
    "Analyze coverage of HEVL signals across the corpus.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HEVL SIGNAL DETECTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "# Overall component detection rates\n",
    "print(f\"\\n--- Component Detection Rates ---\")\n",
    "print(f\"{'Component':<20} {'Count':>10} {'Percentage':>12}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Hazard signal':<20} {df['has_hazard'].sum():>10,} {df['has_hazard'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Exposure signal':<20} {df['has_exposure'].sum():>10,} {df['has_exposure'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Vulnerability signal':<20} {df['has_vulnerability'].sum():>10,} {df['has_vulnerability'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Loss signal':<20} {df['has_loss'].sum():>10,} {df['has_loss'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Return period found':<20} {df['has_return_period'].sum():>10,} {df['has_return_period'].mean()*100:>11.1f}%\")\n",
    "\n",
    "# HEVL combination analysis\n",
    "print(f\"\\n--- HEVL Component Combinations ---\")\n",
    "df['hevl_combo'] = df.apply(\n",
    "    lambda r: ''.join([\n",
    "        'H' if r['has_hazard'] else '-',\n",
    "        'E' if r['has_exposure'] else '-',\n",
    "        'V' if r['has_vulnerability'] else '-',\n",
    "        'L' if r['has_loss'] else '-'\n",
    "    ]), axis=1\n",
    ")\n",
    "\n",
    "combo_counts = df['hevl_combo'].value_counts()\n",
    "print(f\"{'Combination':<15} {'Count':>10} {'Percentage':>12}  Description\")\n",
    "print(\"-\" * 70)\n",
    "for combo, count in combo_counts.head(15).items():\n",
    "    desc = []\n",
    "    if combo[0] == 'H': desc.append('Hazard')\n",
    "    if combo[1] == 'E': desc.append('Exposure')\n",
    "    if combo[2] == 'V': desc.append('Vulnerability')\n",
    "    if combo[3] == 'L': desc.append('Loss')\n",
    "    desc_str = '+'.join(desc) if desc else 'No HEVL signals'\n",
    "    print(f\"{combo:<15} {count:>10,} {count/total*100:>11.1f}%  {desc_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HAZARD TYPE DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Hazard Type                    Count  % of Corpus RDLS Code           \n",
      "----------------------------------------------------------------------\n",
      "flood                          1,315         5.0% flood               \n",
      "convective_storm                 534         2.0% convective_storm    \n",
      "drought                          370         1.4% drought             \n",
      "earthquake                       365         1.4% earthquake          \n",
      "tsunami                          312         1.2% tsunami             \n",
      "strong_wind                      282         1.1% strong_wind         \n",
      "landslide                         76         0.3% landslide           \n",
      "volcanic                          26         0.1% volcanic            \n",
      "coastal_flood                      5         0.0% coastal_flood       \n",
      "wildfire                           5         0.0% wildfire            \n",
      "extreme_temperature                4         0.0% extreme_temperature \n",
      "\n",
      "* = May need mapping to official RDLS hazard_type code\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.4 Detailed Hazard Type Distribution\n",
    "\n",
    "Frequency analysis of specific hazard types detected.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HAZARD TYPE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count each hazard type\n",
    "hazard_counter = Counter()\n",
    "for hazards in df['hazard_types']:\n",
    "    hazard_counter.update(hazards)\n",
    "\n",
    "print(f\"\\n{'Hazard Type':<25} {'Count':>10} {'% of Corpus':>12} {'RDLS Code':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for hazard, count in hazard_counter.most_common():\n",
    "    # Check if in RDLS codelist\n",
    "    rdls_code = hazard if hazard in RDLS_CODELISTS.get('hazard_type', []) else f\"{hazard}*\"\n",
    "    print(f\"{hazard:<25} {count:>10,} {count/total*100:>11.1f}% {rdls_code:<20}\")\n",
    "\n",
    "print(\"\\n* = May need mapping to official RDLS hazard_type code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPOSURE CATEGORY DISTRIBUTION\n",
      "============================================================\n",
      "\n",
      "Exposure Category              Count  % of Corpus\n",
      "--------------------------------------------------\n",
      "population                    11,213        42.7%\n",
      "infrastructure                 6,750        25.7%\n",
      "economic_indicator             6,490        24.7%\n",
      "buildings                      5,489        20.9%\n",
      "agriculture                    3,637        13.9%\n",
      "natural_environment            3,063        11.7%\n",
      "development_index              1,482         5.6%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.5 Detailed Exposure Category Distribution\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPOSURE CATEGORY DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count each exposure category\n",
    "exposure_counter = Counter()\n",
    "for categories in df['exposure_categories']:\n",
    "    exposure_counter.update(categories)\n",
    "\n",
    "print(f\"\\n{'Exposure Category':<25} {'Count':>10} {'% of Corpus':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for category, count in exposure_counter.most_common():\n",
    "    print(f\"{category:<25} {count:>10,} {count/total*100:>11.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RETURN PERIOD EXTRACTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Datasets with return period: 56\n",
      "Total return period values found: 89\n",
      "Unique return period values: 15\n",
      "\n",
      "Return Period (years)      Occurrences\n",
      "----------------------------------------\n",
      "100                                 27\n",
      "5                                   12\n",
      "4                                   11\n",
      "2                                    9\n",
      "1                                    8\n",
      "3                                    8\n",
      "21                                   3\n",
      "500                                  2\n",
      "25                                   2\n",
      "24                                   2\n",
      "50                                   1\n",
      "250                                  1\n",
      "1000                                 1\n",
      "26                                   1\n",
      "16                                   1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.6 Return Period Analysis\n",
    "\n",
    "Distribution of extracted return period values.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RETURN PERIOD EXTRACTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Flatten all return periods\n",
    "all_rp = []\n",
    "for rps in df['return_periods']:\n",
    "    all_rp.extend(rps)\n",
    "\n",
    "rp_counter = Counter(all_rp)\n",
    "\n",
    "print(f\"\\nDatasets with return period: {df['has_return_period'].sum():,}\")\n",
    "print(f\"Total return period values found: {len(all_rp):,}\")\n",
    "print(f\"Unique return period values: {len(rp_counter):,}\")\n",
    "\n",
    "print(f\"\\n{'Return Period (years)':<25} {'Occurrences':>12}\")\n",
    "print(\"-\" * 40)\n",
    "for rp, count in rp_counter.most_common(20):\n",
    "    print(f\"{rp:<25} {count:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Duplication and Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUPLICATION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total datasets: 26,246\n",
      "Unique normalized titles: 24,194\n",
      "Potential duplicate groups: 895\n",
      "Records in duplicate groups: 2,947\n",
      "\n",
      "--- Largest Duplicate Groups (by normalized title) ---\n",
      "Normalized Title (truncated)                          Count\n",
      "------------------------------------------------------------\n",
      "gar15 global exposure dataset                           166\n",
      "hdx hapi data                                           153\n",
      "daily summaries of precipitation indicators              52\n",
      "kenya medium term projection fews net acute foo...       12\n",
      "uganda current situation fews net acute food in...       12\n",
      "kenya near term projection fews net acute food ...       12\n",
      "zimbabwe medium term projection fews net acute ...       12\n",
      "mozambique near term projection fews net acute ...       12\n",
      "chad medium term projection fews net acute food...       12\n",
      "malawi current situation fews net acute food in...       12\n",
      "kenya current situation fews net acute food ins...       12\n",
      "guatemala near term projection fews net acute f...       12\n",
      "somalia near term projection fews net acute foo...       12\n",
      "malawi medium term projection fews net acute fo...       12\n",
      "mali current situation fews net acute food inse...       12\n",
      "somalia medium term projection fews net acute f...       12\n",
      "niger medium term projection fews net acute foo...       12\n",
      "uganda near term projection fews net acute food...       12\n",
      "mozambique medium term projection fews net acut...       12\n",
      "malawi near term projection fews net acute food...       12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.1 Identify Potential Duplicates\n",
    "\n",
    "Detect datasets that appear to be versions/variants of each other.\n",
    "Uses title similarity and organization matching.\n",
    "\"\"\"\n",
    "\n",
    "def normalize_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize title for comparison by removing common variations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Original title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Normalized title\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        return ''\n",
    "    \n",
    "    # Lowercase\n",
    "    t = title.lower()\n",
    "    \n",
    "    # Remove country-specific suffixes (for GAR15 type datasets)\n",
    "    t = re.sub(r'\\s+for\\s+[\\w\\s-]+$', '', t)\n",
    "    \n",
    "    # Remove year references\n",
    "    t = re.sub(r'\\b(19|20)\\d{2}\\b', '', t)\n",
    "    \n",
    "    # Remove common version indicators\n",
    "    t = re.sub(r'\\b(v\\d+|version\\s*\\d+|rev\\s*\\d+)\\b', '', t)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    t = ' '.join(t.split())\n",
    "    \n",
    "    return t.strip()\n",
    "\n",
    "# Create normalized title column\n",
    "df['title_normalized'] = df['title'].apply(normalize_title)\n",
    "\n",
    "# Count duplicates by normalized title\n",
    "title_counts = df['title_normalized'].value_counts()\n",
    "duplicate_titles = title_counts[title_counts > 1]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal datasets: {len(df):,}\")\n",
    "print(f\"Unique normalized titles: {df['title_normalized'].nunique():,}\")\n",
    "print(f\"Potential duplicate groups: {len(duplicate_titles):,}\")\n",
    "print(f\"Records in duplicate groups: {df[df['title_normalized'].isin(duplicate_titles.index)].shape[0]:,}\")\n",
    "\n",
    "# Show largest duplicate groups\n",
    "print(f\"\\n--- Largest Duplicate Groups (by normalized title) ---\")\n",
    "print(f\"{'Normalized Title (truncated)':<50} {'Count':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for title, count in duplicate_titles.head(20).items():\n",
    "    display_title = title[:47] + '...' if len(title) > 50 else title\n",
    "    print(f\"{display_title:<50} {count:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET SERIES ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Datasets identified as part of series: 1,360\n",
      "\n",
      "Series Name                         Count\n",
      "---------------------------------------------\n",
      "Population Data                     1,132\n",
      "GAR15 Exposure                        181\n",
      "Level 1 Exposure                       47\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.2 Identify Dataset Series (Country Variants)\n",
    "\n",
    "Detect systematic series like \"GAR15 Global Exposure Dataset for [Country]\".\n",
    "\"\"\"\n",
    "\n",
    "# Known series patterns\n",
    "SERIES_PATTERNS = [\n",
    "    (r'^gar15\\s+global\\s+exposure\\s+dataset', 'GAR15 Exposure'),\n",
    "    (r'^\\w{3}\\s+requirements\\s+and\\s+funding\\s+data', 'Requirements & Funding'),\n",
    "    (r'level\\s+1\\s+exposure\\s+data', 'Level 1 Exposure'),\n",
    "    (r'admin\\s*\\d+\\s+(boundaries|administrative)', 'Admin Boundaries'),\n",
    "    (r'flood\\s+hazard.*return\\s+period', 'Flood Hazard RP'),\n",
    "    (r'earthquake.*hazard.*pga', 'Earthquake PGA'),\n",
    "    (r'population\\s+(density|count|statistics)', 'Population Data'),\n",
    "]\n",
    "\n",
    "def identify_series(title: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Identify if title belongs to a known dataset series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Dataset title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Optional[str]\n",
    "        Series name if matched, None otherwise\n",
    "    \"\"\"\n",
    "    title_lower = title.lower() if title else ''\n",
    "    for pattern, series_name in SERIES_PATTERNS:\n",
    "        if re.search(pattern, title_lower):\n",
    "            return series_name\n",
    "    return None\n",
    "\n",
    "df['series'] = df['title'].apply(identify_series)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SERIES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "series_counts = df['series'].value_counts()\n",
    "print(f\"\\nDatasets identified as part of series: {df['series'].notna().sum():,}\")\n",
    "print(f\"\\n{'Series Name':<30} {'Count':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for series, count in series_counts.items():\n",
    "    print(f\"{series:<30} {count:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Organization and Source Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RISK-RELEVANT ORGANIZATIONS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Organizations with significant risk data (>=10 datasets, >=30% HEVL rate):\n",
      "\n",
      "Organization                                     Total   Hazard Exposure    HEVL%\n",
      "--------------------------------------------------------------------------------\n",
      "World Bank Group                                 4,792        0    4,506    94.0%\n",
      "Humanitarian OpenStreetMap Team (HOT)            2,593       10    2,587   100.2%\n",
      "United Nations Satellite Centre (UNOSAT)         1,452    1,019      724   120.0%\n",
      "WorldPop                                         1,569        3    1,247    79.7%\n",
      "UNHCR - The UN Refugee Agency                    1,132        9      873    77.9%\n",
      "HeiGIT (Heidelberg Institute for Geoinform...      661      192      661   129.0%\n",
      "Copernicus                                         478      240      478   150.2%\n",
      "Kontur                                             502        2      502   100.4%\n",
      "Internal Displacement Monitoring Centre (I...      426       56      423   112.4%\n",
      "World Health Organization                          678        1      463    68.4%\n",
      "Food and Agriculture Organization (FAO) of...      441        1      430    97.7%\n",
      "WFP - World Food Programme                         492       47      337    78.0%\n",
      "HDX                                                603      257      113    61.4%\n",
      "United Nations Human Settlements Programme...      284        0      281    98.9%\n",
      "Who's On First                                     248        0      248   100.0%\n",
      "Armed Conflict Location & Event Data Proje...      246        1      246   100.4%\n",
      "UNESCO                                             251        0      246    98.0%\n",
      "OurAirports                                        225        1      225   100.4%\n",
      "UNDP Human Development Reports Office (HDRO)       228        0      217    95.2%\n",
      "AI and Data for Good at Meta                       223        1      212    95.5%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.1 Risk-Relevant Organizations\n",
    "\n",
    "Identify organizations that publish HEVL-relevant data.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RISK-RELEVANT ORGANIZATIONS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Organizations with high HEVL signal rates\n",
    "org_hevl_stats = df.groupby('organization').agg({\n",
    "    'id': 'count',\n",
    "    'has_hazard': 'sum',\n",
    "    'has_exposure': 'sum',\n",
    "    'has_vulnerability': 'sum',\n",
    "    'has_loss': 'sum',\n",
    "}).rename(columns={'id': 'total_datasets'})\n",
    "\n",
    "# Calculate rates\n",
    "org_hevl_stats['hazard_rate'] = org_hevl_stats['has_hazard'] / org_hevl_stats['total_datasets']\n",
    "org_hevl_stats['exposure_rate'] = org_hevl_stats['has_exposure'] / org_hevl_stats['total_datasets']\n",
    "\n",
    "# Filter to orgs with significant HEVL content (at least 10 datasets and 30% HEVL rate)\n",
    "org_hevl_stats['any_hevl'] = org_hevl_stats['has_hazard'] + org_hevl_stats['has_exposure']\n",
    "org_hevl_stats['hevl_rate'] = org_hevl_stats['any_hevl'] / org_hevl_stats['total_datasets']\n",
    "\n",
    "risk_orgs = org_hevl_stats[\n",
    "    (org_hevl_stats['total_datasets'] >= 10) & \n",
    "    (org_hevl_stats['hevl_rate'] >= 0.3)\n",
    "].sort_values('any_hevl', ascending=False)\n",
    "\n",
    "print(f\"\\nOrganizations with significant risk data (>=10 datasets, >=30% HEVL rate):\")\n",
    "print(f\"\\n{'Organization':<45} {'Total':>8} {'Hazard':>8} {'Exposure':>8} {'HEVL%':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for org, row in risk_orgs.head(20).iterrows():\n",
    "    org_display = org[:42] + '...' if len(org) > 45 else org\n",
    "    print(f\"{org_display:<45} {row['total_datasets']:>8,.0f} {row['has_hazard']:>8,.0f} {row['has_exposure']:>8,.0f} {row['hevl_rate']*100:>7.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TAG ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Total unique tags: 143\n",
      "Risk-relevant tags: 11\n",
      "\n",
      "--- Top Risk-Relevant Tags ---\n",
      "Tag                                                Count\n",
      "----------------------------------------------------------\n",
      "flooding                                             843\n",
      "cyclones-hurricanes-typhoons                         806\n",
      "natural disasters                                    592\n",
      "hazards and risk                                     567\n",
      "disaster risk reduction-drr                          357\n",
      "earthquake-tsunami                                   306\n",
      "drought                                              298\n",
      "climate hazards                                      158\n",
      "crisis-myanmar-earthquake                             33\n",
      "libya-floods                                          20\n",
      "morocco-earthquake                                    14\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.2 Tag Analysis for HEVL Signals\n",
    "\n",
    "Analyze HDX tags to identify risk-relevant categorization.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TAG ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count all tags\n",
    "all_tags = []\n",
    "for tags_str in df['tags'].dropna():\n",
    "    all_tags.extend(tags_str.split('|'))\n",
    "\n",
    "tag_counter = Counter(t.strip() for t in all_tags if t.strip())\n",
    "\n",
    "# Risk-relevant tags\n",
    "risk_keywords = ['hazard', 'risk', 'disaster', 'flood', 'earthquake', 'cyclone', \n",
    "                 'drought', 'exposure', 'vulnerability', 'tsunami', 'storm']\n",
    "\n",
    "risk_tags = {tag: count for tag, count in tag_counter.items() \n",
    "             if any(kw in tag.lower() for kw in risk_keywords)}\n",
    "\n",
    "print(f\"\\nTotal unique tags: {len(tag_counter):,}\")\n",
    "print(f\"Risk-relevant tags: {len(risk_tags):,}\")\n",
    "\n",
    "print(f\"\\n--- Top Risk-Relevant Tags ---\")\n",
    "print(f\"{'Tag':<45} {'Count':>10}\")\n",
    "print(\"-\" * 58)\n",
    "for tag, count in sorted(risk_tags.items(), key=lambda x: x[1], reverse=True)[:25]:\n",
    "    print(f\"{tag:<45} {count:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample High-Quality HEVL Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HIGH-QUALITY HEVL RECORD SAMPLES\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.1 Identify High-Quality Records for Each Component\n",
    "\n",
    "Find records with strong HEVL signals for manual review and pattern validation.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_signal_strength(row: pd.Series) -> int:\n",
    "    \"\"\"\n",
    "    Calculate overall HEVL signal strength score.\n",
    "    \n",
    "    NOTE: These confidence scores are heuristic weights and have not been\n",
    "    calibrated against ground truth labels. The weights (2x for hazard/exposure,\n",
    "    3x for return periods) reflect assumed relative informativeness but should\n",
    "    be validated empirically in a future iteration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        DataFrame row\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Signal strength score (0-20, heuristic)\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    score += len(row['hazard_types']) * 2  # Weight hazard signals\n",
    "    score += len(row['exposure_categories']) * 2\n",
    "    score += len(row['process_types'])\n",
    "    score += len(row['analysis_types']) * 2\n",
    "    score += len(row['return_periods']) * 3  # Return periods are very specific\n",
    "    return min(score, 20)  # Cap at 20\n",
    "\n",
    "df['signal_strength'] = df.apply(calculate_signal_strength, axis=1)\n",
    "\n",
    "# Find top records for each component\n",
    "print(\"=\" * 70)\n",
    "print(\"HIGH-QUALITY HEVL RECORD SAMPLES\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Hazard-Rich Records (with return periods) ---\n",
      "\n",
      "Title: Global Drought Hazard\n",
      "  Organization: Institute for International Law of Peace and Armed Conflict\n",
      "  Hazard types: ['drought']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic']\n",
      "  Return periods: [25, 50, 100, 250, 500, 1000]\n",
      "  Signal strength: 20\n",
      "\n",
      "Title: United Republic of Tanzania: Integrated Context Analysis (ICA), 2015\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'landslide', 'drought']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 19\n",
      "\n",
      "Title: Burkina Faso: Integrated Context Analysis (ICA), 2018\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'drought']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 17\n",
      "\n",
      "Title: Zambia: Response Plan projects\n",
      "  Organization: OCHA Humanitarian Programme Cycle Tools (HPC Tools)\n",
      "  Hazard types: ['drought']\n",
      "  Process types: []\n",
      "  Analysis types: []\n",
      "  Return periods: [1, 2, 3, 4, 5]\n",
      "  Signal strength: 17\n",
      "\n",
      "Title: Chad: Integrated Context Analysis (ICA), 2017\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'landslide', 'drought']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 17\n",
      "\n",
      "Title: Syrian Arab Republic: Integrated Context Analysis (ICA), 2020\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'drought']\n",
      "  Process types: ['ground_motion']\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 16\n",
      "\n",
      "Title: Niger: Integrated Context Analysis (ICA), 2018\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'drought']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 15\n",
      "\n",
      "Title: Mali: Integrated Context Analysis (ICA), 2014\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'drought']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 15\n",
      "\n",
      "Title: Sierra Leone: Integrated Context Analysis (ICA), 2017\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'landslide']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 15\n",
      "\n",
      "Title: Sudan: Integrated Context Analysis (ICA), 2018\n",
      "  Organization: WFP - World Food Programme\n",
      "  Hazard types: ['flood', 'drought']\n",
      "  Process types: []\n",
      "  Analysis types: ['probabilistic', 'deterministic', 'empirical']\n",
      "  Return periods: [100]\n",
      "  Signal strength: 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.2 Sample Hazard-Rich Records\n",
    "\"\"\"\n",
    "\n",
    "hazard_rich = df[df['has_hazard'] & df['has_return_period']].nlargest(10, 'signal_strength')\n",
    "\n",
    "print(\"\\n--- Top 10 Hazard-Rich Records (with return periods) ---\\n\")\n",
    "for idx, row in hazard_rich.iterrows():\n",
    "    print(f\"Title: {row['title'][:80]}\")\n",
    "    print(f\"  Organization: {row['organization']}\")\n",
    "    print(f\"  Hazard types: {row['hazard_types']}\")\n",
    "    print(f\"  Process types: {row['process_types']}\")\n",
    "    print(f\"  Analysis types: {row['analysis_types']}\")\n",
    "    print(f\"  Return periods: {row['return_periods']}\")\n",
    "    print(f\"  Signal strength: {row['signal_strength']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top 10 Exposure-Rich Records (multiple categories) ---\n",
      "\n",
      "Title: United Republic of Tanzania: Integrated Context Analysis (ICA), 2015\n",
      "  Organization: WFP - World Food Programme\n",
      "  Exposure categories: ['population', 'agriculture']\n",
      "  Tags: geodata|hazards and risk\n",
      "  Signal strength: 19\n",
      "\n",
      "Title: Mozambique - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|malaria\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Mauritius - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|malaria\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Tonga - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|materni\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Japan - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|materni\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Burundi - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|malaria\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Bangladesh - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|malaria\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Mauritania - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|malaria\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Haiti - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|malaria\n",
      "  Signal strength: 18\n",
      "\n",
      "Title: Croatia - Health Indicators\n",
      "  Organization: World Health Organization\n",
      "  Exposure categories: ['buildings', 'infrastructure', 'population', 'natural_environment', 'economic_indicator', 'development_index']\n",
      "  Tags: disability|disease|environment|health|hxl|indicators|materni\n",
      "  Signal strength: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "6.3 Sample Exposure-Rich Records\n",
    "\"\"\"\n",
    "\n",
    "exposure_rich = df[df['has_exposure'] & (df['exposure_categories'].apply(len) >= 2)].nlargest(10, 'signal_strength')\n",
    "\n",
    "print(\"\\n--- Top 10 Exposure-Rich Records (multiple categories) ---\\n\")\n",
    "for idx, row in exposure_rich.iterrows():\n",
    "    print(f\"Title: {row['title'][:80]}\")\n",
    "    print(f\"  Organization: {row['organization']}\")\n",
    "    print(f\"  Exposure categories: {row['exposure_categories']}\")\n",
    "    print(f\"  Tags: {row['tags'][:60]}\")\n",
    "    print(f\"  Signal strength: {row['signal_strength']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/analysis/hdx_hevl_signal_analysis.csv\n",
      "  Records: 26,246\n",
      "  Columns: 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "7.1 Export Analysis DataFrames\n",
    "\n",
    "Save analysis results for downstream notebooks.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare export DataFrame (exclude large text columns)\n",
    "export_cols = [\n",
    "    'id', 'name', 'title', 'organization', 'groups', 'tags',\n",
    "    'hazard_types', 'process_types', 'exposure_categories', 'analysis_types',\n",
    "    'vuln_loss_indicators', 'return_periods',\n",
    "    'has_hazard', 'has_exposure', 'has_vulnerability', 'has_loss',\n",
    "    'has_return_period', 'hevl_combo', 'signal_strength', 'series'\n",
    "]\n",
    "\n",
    "df_export = df[export_cols].copy()\n",
    "\n",
    "# Convert lists to pipe-separated strings for CSV compatibility\n",
    "list_cols = ['hazard_types', 'process_types', 'exposure_categories', \n",
    "             'analysis_types', 'vuln_loss_indicators', 'return_periods']\n",
    "for col in list_cols:\n",
    "    df_export[col] = df_export[col].apply(lambda x: '|'.join(map(str, x)) if x else '')\n",
    "\n",
    "# Save full analysis\n",
    "output_file = OUTPUT_DIR / 'hdx_hevl_signal_analysis.csv'\n",
    "df_export.to_csv(output_file, index=False)\n",
    "print(f\"Saved: {output_file}\")\n",
    "print(f\"  Records: {len(df_export):,}\")\n",
    "print(f\"  Columns: {len(df_export.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/analysis/hdx_hevl_signal_summary.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "7.2 Export Summary Statistics\n",
    "\"\"\"\n",
    "\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert NumPy/pandas types to native Python types for JSON serialization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : Any\n",
    "        Object to convert (can be dict, list, numpy type, etc.)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        Object with all NumPy types converted to native Python types\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_to_native(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return convert_to_native(obj.tolist())\n",
    "    elif isinstance(obj, pd.Series):\n",
    "        return convert_to_native(obj.to_dict())\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "summary = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'total_datasets': int(len(df)),\n",
    "    'unique_organizations': int(df['organization'].nunique()),\n",
    "    \n",
    "    'hevl_detection': {\n",
    "        'hazard_signal_count': int(df['has_hazard'].sum()),\n",
    "        'hazard_signal_rate': float(round(df['has_hazard'].mean(), 4)),\n",
    "        'exposure_signal_count': int(df['has_exposure'].sum()),\n",
    "        'exposure_signal_rate': float(round(df['has_exposure'].mean(), 4)),\n",
    "        'vulnerability_signal_count': int(df['has_vulnerability'].sum()),\n",
    "        'vulnerability_signal_rate': float(round(df['has_vulnerability'].mean(), 4)),\n",
    "        'loss_signal_count': int(df['has_loss'].sum()),\n",
    "        'loss_signal_rate': float(round(df['has_loss'].mean(), 4)),\n",
    "        'return_period_count': int(df['has_return_period'].sum()),\n",
    "        'return_period_rate': float(round(df['has_return_period'].mean(), 4)),\n",
    "    },\n",
    "    \n",
    "    'hazard_type_counts': convert_to_native(dict(hazard_counter.most_common())),\n",
    "    'exposure_category_counts': convert_to_native(dict(exposure_counter.most_common())),\n",
    "    'return_period_counts': convert_to_native(dict(rp_counter.most_common(20))),\n",
    "    \n",
    "    'hevl_combinations': convert_to_native(dict(df['hevl_combo'].value_counts().head(10))),\n",
    "    \n",
    "    'duplication': {\n",
    "        'unique_normalized_titles': int(df['title_normalized'].nunique()),\n",
    "        'duplicate_groups': int(len(duplicate_titles)),\n",
    "    },\n",
    "    \n",
    "    'series_counts': convert_to_native(dict(series_counts)) if len(series_counts) > 0 else {},\n",
    "}\n",
    "\n",
    "summary_file = OUTPUT_DIR / 'hdx_hevl_signal_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"\\nSaved: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/analysis/hdx_high_signal_records.csv\n",
      "  Records: 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "7.3 Export High-Signal Records for Manual Review\n",
    "\"\"\"\n",
    "\n",
    "# Top 500 records by signal strength\n",
    "high_signal = df.nlargest(500, 'signal_strength')[[\n",
    "    'id', 'title', 'organization', 'hazard_types', 'exposure_categories',\n",
    "    'analysis_types', 'return_periods', 'signal_strength', 'hevl_combo'\n",
    "]].copy()\n",
    "\n",
    "for col in ['hazard_types', 'exposure_categories', 'analysis_types', 'return_periods']:\n",
    "    high_signal[col] = high_signal[col].apply(lambda x: '|'.join(map(str, x)) if x else '')\n",
    "\n",
    "high_signal_file = OUTPUT_DIR / 'hdx_high_signal_records.csv'\n",
    "high_signal.to_csv(high_signal_file, index=False)\n",
    "print(f\"Saved: {high_signal_file}\")\n",
    "print(f\"  Records: {len(high_signal):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYSIS SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "CORPUS OVERVIEW\n",
      "---------------\n",
      "Total HDX datasets analyzed: 26,246\n",
      "Unique organizations: 358\n",
      "\n",
      "HEVL SIGNAL COVERAGE\n",
      "--------------------\n",
      "Datasets with Hazard signals:        2,517 (9.6%)\n",
      "Datasets with Exposure signals:     18,673 (71.1%)\n",
      "Datasets with Vulnerability signals:   205 (0.8%)\n",
      "Datasets with Loss signals:          2,407 (9.2%)\n",
      "Datasets with Return Period info:       56 (0.2%)\n",
      "\n",
      "INFERENCE POTENTIAL\n",
      "-------------------\n",
      "Datasets suitable for Hazard block population: ~2,517\n",
      "  - With specific hazard type: 2,517\n",
      "  - With process type detail:  1,320\n",
      "  - With analysis type:        7,095\n",
      "  - With return periods:       56\n",
      "\n",
      "Datasets suitable for Exposure block population: ~18,673\n",
      "  - With category detected:    18,673\n",
      "\n",
      "DUPLICATION STATUS\n",
      "------------------\n",
      "Potential duplicate groups: 895\n",
      "Records in duplicate groups: 2,947\n",
      "Identified dataset series: 1,360\n",
      "\n",
      "KEY INSIGHTS\n",
      "------------\n",
      "1. Hazard data is well-represented (10% of corpus)\n",
      "2. Return period extraction is feasible for 56 datasets\n",
      "3. Major risk data publishers: UNDRR, GEM, OCHA, WFP, UNOSAT\n",
      "4. Significant series duplication exists (GAR15 Exposure: ~190 country variants)\n",
      "\n",
      "RECOMMENDED NEXT STEPS\n",
      "----------------------\n",
      "1. Build Signal Dictionary with confident mappings to RDLS codelists\n",
      "2. Develop Hazard block extractor (highest coverage potential)\n",
      "3. Handle series deduplication to avoid redundant processing\n",
      "4. Develop Exposure block extractor\n",
      "\n",
      "\n",
      "Analysis completed: 2026-02-11T06:51:06.159834\n",
      "Output files saved to: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/analysis\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "8.1 Analysis Summary Report\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "CORPUS OVERVIEW\n",
    "---------------\n",
    "Total HDX datasets analyzed: {len(df):,}\n",
    "Unique organizations: {df['organization'].nunique():,}\n",
    "\n",
    "HEVL SIGNAL COVERAGE\n",
    "--------------------\n",
    "Datasets with Hazard signals:       {df['has_hazard'].sum():>6,} ({df['has_hazard'].mean()*100:.1f}%)\n",
    "Datasets with Exposure signals:     {df['has_exposure'].sum():>6,} ({df['has_exposure'].mean()*100:.1f}%)\n",
    "Datasets with Vulnerability signals:{df['has_vulnerability'].sum():>6,} ({df['has_vulnerability'].mean()*100:.1f}%)\n",
    "Datasets with Loss signals:         {df['has_loss'].sum():>6,} ({df['has_loss'].mean()*100:.1f}%)\n",
    "Datasets with Return Period info:   {df['has_return_period'].sum():>6,} ({df['has_return_period'].mean()*100:.1f}%)\n",
    "\n",
    "INFERENCE POTENTIAL\n",
    "-------------------\n",
    "Datasets suitable for Hazard block population: ~{df['has_hazard'].sum():,}\n",
    "  - With specific hazard type: {(df['hazard_types'].apply(len) >= 1).sum():,}\n",
    "  - With process type detail:  {(df['process_types'].apply(len) >= 1).sum():,}\n",
    "  - With analysis type:        {(df['analysis_types'].apply(len) >= 1).sum():,}\n",
    "  - With return periods:       {df['has_return_period'].sum():,}\n",
    "\n",
    "Datasets suitable for Exposure block population: ~{df['has_exposure'].sum():,}\n",
    "  - With category detected:    {(df['exposure_categories'].apply(len) >= 1).sum():,}\n",
    "\n",
    "DUPLICATION STATUS\n",
    "------------------\n",
    "Potential duplicate groups: {len(duplicate_titles):,}\n",
    "Records in duplicate groups: {df[df['title_normalized'].isin(duplicate_titles.index)].shape[0]:,}\n",
    "Identified dataset series: {df['series'].notna().sum():,}\n",
    "\n",
    "KEY INSIGHTS\n",
    "------------\n",
    "1. Hazard data is well-represented ({df['has_hazard'].mean()*100:.0f}% of corpus)\n",
    "2. Return period extraction is feasible for {df['has_return_period'].sum():,} datasets\n",
    "3. Major risk data publishers: UNDRR, GEM, OCHA, WFP, UNOSAT\n",
    "4. Significant series duplication exists (GAR15 Exposure: ~190 country variants)\n",
    "\n",
    "RECOMMENDED NEXT STEPS\n",
    "----------------------\n",
    "1. Build Signal Dictionary with confident mappings to RDLS codelists\n",
    "2. Develop Hazard block extractor (highest coverage potential)\n",
    "3. Handle series deduplication to avoid redundant processing\n",
    "4. Develop Exposure block extractor\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nAnalysis completed: {datetime.now().isoformat()}\")\n",
    "print(f\"Output files saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
