{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 08: HDX Metadata Signal Analysis for HEVL Inference\n",
    "\n",
    "**Purpose**: Analyze 26,246 HDX metadata files to identify extractable signals for populating RDLS v0.3 HEVL (Hazard, Exposure, Vulnerability, Loss) component blocks.\n",
    "\n",
    "**Outputs**:\n",
    "- Signal frequency analysis (hazard types, exposure categories, etc.)\n",
    "- Duplication pattern report\n",
    "- HEVL coverage potential assessment\n",
    "- Signal dictionary foundation for downstream extraction\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\n",
    "Standard data science stack for metadata analysis.\n",
    "All packages are commonly available via pip/conda.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Optional: progress bar for long operations\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not available. Install with 'pip install tqdm' for progress bars.\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(f\"Analysis started: {datetime.now().isoformat()}\")\n",
    "print(f\"Python packages loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Define Paths and Constants\n",
    "\n",
    "All paths are relative to the repository root for reproducibility.\n",
    "Adjust BASE_DIR if running from a different location.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# PATH CONFIGURATION - Adjust if needed\n",
    "# ============================================================================\n",
    "\n",
    "# Repository root (parent of 'notebook' folder)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input paths\n",
    "DATASET_METADATA_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'dataset_metadata'\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'analysis'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify paths exist\n",
    "assert DATASET_METADATA_DIR.exists(), f\"Dataset metadata directory not found: {DATASET_METADATA_DIR}\"\n",
    "assert RDLS_SCHEMA_PATH.exists(), f\"RDLS schema not found: {RDLS_SCHEMA_PATH}\"\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Dataset metadata: {DATASET_METADATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Load RDLS Schema Codelists\n",
    "\n",
    "Extract closed codelists from RDLS v0.3 schema to use as reference\n",
    "for signal matching. These are the valid values we need to map to.\n",
    "\"\"\"\n",
    "\n",
    "def load_rdls_codelists(schema_path: Path) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract codelist values from RDLS schema.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    schema_path : Path\n",
    "        Path to rdls_schema_v0.3.json\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[str]]\n",
    "        Dictionary mapping codelist names to their valid values\n",
    "    \"\"\"\n",
    "    with open(schema_path, 'r', encoding='utf-8') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    codelists = {}\n",
    "    defs = schema.get('$defs', {})\n",
    "    \n",
    "    # Extract enum values from $defs\n",
    "    for name, definition in defs.items():\n",
    "        if 'enum' in definition:\n",
    "            codelists[name] = definition['enum']\n",
    "        elif definition.get('type') == 'string' and 'enum' in definition:\n",
    "            codelists[name] = definition['enum']\n",
    "    \n",
    "    return codelists\n",
    "\n",
    "# Load codelists\n",
    "RDLS_CODELISTS = load_rdls_codelists(RDLS_SCHEMA_PATH)\n",
    "\n",
    "# Display key codelists for HEVL\n",
    "key_codelists = ['hazard_type', 'process_type', 'exposure_category', 'analysis_type', 'risk_data_type']\n",
    "print(\"=\" * 60)\n",
    "print(\"RDLS Key Codelists (closed - must match exactly)\")\n",
    "print(\"=\" * 60)\n",
    "for cl in key_codelists:\n",
    "    if cl in RDLS_CODELISTS:\n",
    "        print(f\"\\n{cl}:\")\n",
    "        print(f\"  {RDLS_CODELISTS[cl]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Load All HDX Metadata Files\n",
    "\n",
    "Read all JSON files from dataset_metadata directory.\n",
    "This may take a few minutes for 26,000+ files.\n",
    "\"\"\"\n",
    "\n",
    "def load_hdx_metadata(metadata_dir: Path, limit: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load HDX metadata JSON files from directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata_dir : Path\n",
    "        Directory containing HDX metadata JSON files\n",
    "    limit : Optional[int]\n",
    "        Maximum number of files to load (for testing). None = all files.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, Any]]\n",
    "        List of parsed metadata dictionaries\n",
    "    \"\"\"\n",
    "    json_files = list(metadata_dir.glob('*.json'))\n",
    "    \n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "    \n",
    "    records = []\n",
    "    errors = []\n",
    "    \n",
    "    iterator = tqdm(json_files, desc=\"Loading metadata\") if HAS_TQDM else json_files\n",
    "    \n",
    "    for filepath in iterator:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                data['_source_file'] = filepath.name\n",
    "                records.append(data)\n",
    "        except Exception as e:\n",
    "            errors.append({'file': filepath.name, 'error': str(e)})\n",
    "    \n",
    "    print(f\"\\nLoaded: {len(records):,} records\")\n",
    "    print(f\"Errors: {len(errors):,} files\")\n",
    "    \n",
    "    return records, errors\n",
    "\n",
    "# Load all metadata (set limit=1000 for faster testing)\n",
    "LOAD_LIMIT = None  # Set to integer for testing, None for full load\n",
    "\n",
    "print(f\"Loading HDX metadata files...\")\n",
    "print(f\"Limit: {'All files' if LOAD_LIMIT is None else f'{LOAD_LIMIT:,} files'}\")\n",
    "\n",
    "hdx_records, load_errors = load_hdx_metadata(DATASET_METADATA_DIR, limit=LOAD_LIMIT)\n",
    "\n",
    "# Store for later use\n",
    "print(f\"\\nTotal records available for analysis: {len(hdx_records):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Convert to DataFrame for Analysis\n",
    "\n",
    "Flatten key fields into a DataFrame for efficient analysis.\n",
    "\"\"\"\n",
    "\n",
    "def extract_flat_record(record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract key fields from HDX record into flat dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    record : Dict[str, Any]\n",
    "        Raw HDX metadata record\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Flattened record with key fields\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'id': record.get('id', ''),\n",
    "        'name': record.get('name', ''),\n",
    "        'title': record.get('title', ''),\n",
    "        'notes': record.get('notes', ''),\n",
    "        'organization': record.get('organization', ''),\n",
    "        'dataset_source': record.get('dataset_source', ''),\n",
    "        'groups': '|'.join(record.get('groups', [])),\n",
    "        'tags': '|'.join(record.get('tags', [])),\n",
    "        'license_title': record.get('license_title', ''),\n",
    "        'methodology': record.get('methodology', ''),\n",
    "        'methodology_other': record.get('methodology_other', ''),\n",
    "        'caveats': record.get('caveats', ''),\n",
    "        'dataset_date': record.get('dataset_date', ''),\n",
    "        'last_modified': record.get('last_modified', ''),\n",
    "        'data_update_frequency': record.get('data_update_frequency', ''),\n",
    "        'resource_count': len(record.get('resources', [])),\n",
    "        'resource_formats': '|'.join(set(r.get('format', '') for r in record.get('resources', []))),\n",
    "        'resource_names': '|'.join(r.get('name', '') for r in record.get('resources', [])),\n",
    "        '_source_file': record.get('_source_file', ''),\n",
    "        # Concatenate all text fields for pattern matching\n",
    "        '_all_text': ' '.join(filter(None, [\n",
    "            record.get('title', ''),\n",
    "            record.get('name', ''),\n",
    "            record.get('notes', ''),\n",
    "            ' '.join(record.get('tags', [])),\n",
    "            ' '.join(r.get('name', '') for r in record.get('resources', [])),\n",
    "            ' '.join(r.get('description', '') for r in record.get('resources', []))\n",
    "        ])).lower()\n",
    "    }\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([extract_flat_record(r) for r in hdx_records])\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.3 Basic Statistics Overview\n",
    "\n",
    "Summary statistics for the HDX metadata corpus.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HDX METADATA CORPUS OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal datasets: {len(df):,}\")\n",
    "print(f\"Unique dataset IDs: {df['id'].nunique():,}\")\n",
    "print(f\"Unique dataset names: {df['name'].nunique():,}\")\n",
    "print(f\"Unique organizations: {df['organization'].nunique():,}\")\n",
    "\n",
    "# Top organizations\n",
    "print(f\"\\n--- Top 15 Organizations by Dataset Count ---\")\n",
    "org_counts = df['organization'].value_counts().head(15)\n",
    "for org, count in org_counts.items():\n",
    "    print(f\"  {org}: {count:,}\")\n",
    "\n",
    "# Resource format distribution\n",
    "print(f\"\\n--- Resource Format Distribution ---\")\n",
    "all_formats = []\n",
    "for formats in df['resource_formats'].dropna():\n",
    "    all_formats.extend(formats.split('|'))\n",
    "format_counts = Counter(f for f in all_formats if f)\n",
    "for fmt, count in format_counts.most_common(15):\n",
    "    print(f\"  {fmt}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HEVL Signal Pattern Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Define HEVL Signal Patterns\n",
    "\n",
    "Regular expression patterns to detect HEVL-relevant signals in text.\n",
    "These patterns are designed to map to RDLS codelist values.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# HAZARD TYPE PATTERNS\n",
    "# Maps to: RDLS hazard_type codelist\n",
    "# ============================================================================\n",
    "HAZARD_TYPE_PATTERNS = {\n",
    "    'flood': r'\\b(flood|flooding|fluvial|pluvial|inundation)\\b',\n",
    "    'coastal_flood': r'\\b(coastal.?flood|storm.?surge|tidal.?flood|sea.?level)\\b',\n",
    "    'earthquake': r'\\b(earthquake|seismic|quake|tremor|ground.?motion)\\b',\n",
    "    'tsunami': r'\\b(tsunami|tidal.?wave)\\b',\n",
    "    'landslide': r'\\b(landslide|mudslide|rockfall|debris.?flow|mass.?movement)\\b',\n",
    "    'volcanic': r'\\b(volcan|lava|pyroclastic|ash.?fall|eruption)\\b',\n",
    "    'drought': r'\\b(drought|water.?scarcity|aridity)\\b',\n",
    "    'wildfire': r'\\b(wildfire|forest.?fire|bushfire|fire.?hazard)\\b',\n",
    "    'strong_wind': r'\\b(wind|gust|gale)\\b',\n",
    "    'convective_storm': r'\\b(cyclone|typhoon|hurricane|tropical.?storm|tornado|thunderstorm)\\b',\n",
    "    'extreme_temperature': r'\\b(heat.?wave|cold.?wave|extreme.?temperature|frost|freeze)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# HAZARD PROCESS TYPE PATTERNS (more specific)\n",
    "# Maps to: RDLS process_type codelist\n",
    "# ============================================================================\n",
    "PROCESS_TYPE_PATTERNS = {\n",
    "    'fluvial_flood': r'\\b(fluvial|river.?flood|riverine)\\b',\n",
    "    'pluvial_flood': r'\\b(pluvial|flash.?flood|surface.?water|urban.?flood)\\b',\n",
    "    'coastal_flood': r'\\b(coastal.?flood|storm.?surge|tidal)\\b',\n",
    "    'ground_motion': r'\\b(ground.?motion|pga|pgv|shaking|intensity)\\b',\n",
    "    'liquefaction': r'\\b(liquefaction)\\b',\n",
    "    'tornado': r'\\b(tornado|twister)\\b',\n",
    "    'tropical_cyclone': r'\\b(tropical.?cyclone|typhoon|hurricane|cyclone)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# EXPOSURE CATEGORY PATTERNS\n",
    "# Maps to: RDLS exposure_category codelist\n",
    "# ============================================================================\n",
    "EXPOSURE_CATEGORY_PATTERNS = {\n",
    "    'buildings': r'\\b(building|structure|dwelling|house|residential|commercial|industrial)\\b',\n",
    "    'infrastructure': r'\\b(infrastructure|road|bridge|railway|transport|power.?line|utility|airport|port)\\b',\n",
    "    'population': r'\\b(population|people|inhabitant|resident|demographic|census)\\b',\n",
    "    'agriculture': r'\\b(agriculture|crop|farm|livestock|agricultural|cultivation)\\b',\n",
    "    'natural_environment': r'\\b(environment|ecosystem|forest|wetland|biodiversity|natural.?resource)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS TYPE PATTERNS\n",
    "# Maps to: RDLS analysis_type codelist\n",
    "# ============================================================================\n",
    "ANALYSIS_TYPE_PATTERNS = {\n",
    "    'probabilistic': r'\\b(probabilistic|return.?period|rp\\d+|annual.?exceedance|aep|frequency|stochastic|\\d+.?year.?event)\\b',\n",
    "    'deterministic': r'\\b(deterministic|index|susceptibility|ranking|score|classification)\\b',\n",
    "    'empirical': r'\\b(empirical|historical|observed|actual|recorded|past.?event)\\b',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# RETURN PERIOD EXTRACTION PATTERN\n",
    "# ============================================================================\n",
    "RETURN_PERIOD_PATTERN = r'(?:return.?period|rp|recurrence).?(?:of)?\\s*(\\d+)\\s*(?:year|yr)?|(?:(\\d+).?year.?(?:return|event|flood|storm))|(\\d+)\\s*yr'\n",
    "\n",
    "# ============================================================================\n",
    "# VULNERABILITY/LOSS INDICATORS\n",
    "# ============================================================================\n",
    "VULNERABILITY_PATTERNS = {\n",
    "    'vulnerability': r'\\b(vulnerability|fragility|damage.?function|loss.?function|susceptibility)\\b',\n",
    "    'loss': r'\\b(loss|damage|impact|economic.?loss|casualty|fatality|injury)\\b',\n",
    "    'risk_assessment': r'\\b(risk.?assessment|risk.?analysis|risk.?model|cat.?model)\\b',\n",
    "}\n",
    "\n",
    "print(\"HEVL signal patterns defined:\")\n",
    "print(f\"  - Hazard types: {len(HAZARD_TYPE_PATTERNS)} patterns\")\n",
    "print(f\"  - Process types: {len(PROCESS_TYPE_PATTERNS)} patterns\")\n",
    "print(f\"  - Exposure categories: {len(EXPOSURE_CATEGORY_PATTERNS)} patterns\")\n",
    "print(f\"  - Analysis types: {len(ANALYSIS_TYPE_PATTERNS)} patterns\")\n",
    "print(f\"  - Vulnerability/Loss indicators: {len(VULNERABILITY_PATTERNS)} patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.2 Apply Pattern Matching to Corpus\n",
    "\n",
    "Scan all records for HEVL signals using defined patterns.\n",
    "\"\"\"\n",
    "\n",
    "def extract_patterns(text: str, patterns: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all matching pattern names in text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to search (should be lowercase)\n",
    "    patterns : Dict[str, str]\n",
    "        Dictionary of {name: regex_pattern}\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        List of matched pattern names\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for name, pattern in patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            matches.append(name)\n",
    "    return matches\n",
    "\n",
    "def extract_return_periods(text: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    Extract return period values from text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to search\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        List of extracted return period values (years)\n",
    "    \"\"\"\n",
    "    rp_values = []\n",
    "    for match in re.finditer(RETURN_PERIOD_PATTERN, text, re.IGNORECASE):\n",
    "        for group in match.groups():\n",
    "            if group:\n",
    "                try:\n",
    "                    rp = int(group)\n",
    "                    if 1 <= rp <= 100000:  # Reasonable range\n",
    "                        rp_values.append(rp)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    return sorted(set(rp_values))\n",
    "\n",
    "# Apply pattern matching to all records\n",
    "print(\"Extracting HEVL signals from metadata...\")\n",
    "\n",
    "df['hazard_types'] = df['_all_text'].apply(lambda x: extract_patterns(x, HAZARD_TYPE_PATTERNS))\n",
    "df['process_types'] = df['_all_text'].apply(lambda x: extract_patterns(x, PROCESS_TYPE_PATTERNS))\n",
    "df['exposure_categories'] = df['_all_text'].apply(lambda x: extract_patterns(x, EXPOSURE_CATEGORY_PATTERNS))\n",
    "df['analysis_types'] = df['_all_text'].apply(lambda x: extract_patterns(x, ANALYSIS_TYPE_PATTERNS))\n",
    "df['vuln_loss_indicators'] = df['_all_text'].apply(lambda x: extract_patterns(x, VULNERABILITY_PATTERNS))\n",
    "df['return_periods'] = df['_all_text'].apply(extract_return_periods)\n",
    "\n",
    "# Create binary flags\n",
    "df['has_hazard'] = df['hazard_types'].apply(lambda x: len(x) > 0)\n",
    "df['has_exposure'] = df['exposure_categories'].apply(lambda x: len(x) > 0)\n",
    "df['has_vulnerability'] = df['vuln_loss_indicators'].apply(lambda x: 'vulnerability' in x)\n",
    "df['has_loss'] = df['vuln_loss_indicators'].apply(lambda x: 'loss' in x or 'risk_assessment' in x)\n",
    "df['has_return_period'] = df['return_periods'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "print(\"Signal extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.3 Signal Detection Summary Statistics\n",
    "\n",
    "Analyze coverage of HEVL signals across the corpus.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HEVL SIGNAL DETECTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "# Overall component detection rates\n",
    "print(f\"\\n--- Component Detection Rates ---\")\n",
    "print(f\"{'Component':<20} {'Count':>10} {'Percentage':>12}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Hazard signal':<20} {df['has_hazard'].sum():>10,} {df['has_hazard'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Exposure signal':<20} {df['has_exposure'].sum():>10,} {df['has_exposure'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Vulnerability signal':<20} {df['has_vulnerability'].sum():>10,} {df['has_vulnerability'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Loss signal':<20} {df['has_loss'].sum():>10,} {df['has_loss'].mean()*100:>11.1f}%\")\n",
    "print(f\"{'Return period found':<20} {df['has_return_period'].sum():>10,} {df['has_return_period'].mean()*100:>11.1f}%\")\n",
    "\n",
    "# HEVL combination analysis\n",
    "print(f\"\\n--- HEVL Component Combinations ---\")\n",
    "df['hevl_combo'] = df.apply(\n",
    "    lambda r: ''.join([\n",
    "        'H' if r['has_hazard'] else '-',\n",
    "        'E' if r['has_exposure'] else '-',\n",
    "        'V' if r['has_vulnerability'] else '-',\n",
    "        'L' if r['has_loss'] else '-'\n",
    "    ]), axis=1\n",
    ")\n",
    "\n",
    "combo_counts = df['hevl_combo'].value_counts()\n",
    "print(f\"{'Combination':<15} {'Count':>10} {'Percentage':>12}  Description\")\n",
    "print(\"-\" * 70)\n",
    "for combo, count in combo_counts.head(15).items():\n",
    "    desc = []\n",
    "    if combo[0] == 'H': desc.append('Hazard')\n",
    "    if combo[1] == 'E': desc.append('Exposure')\n",
    "    if combo[2] == 'V': desc.append('Vulnerability')\n",
    "    if combo[3] == 'L': desc.append('Loss')\n",
    "    desc_str = '+'.join(desc) if desc else 'No HEVL signals'\n",
    "    print(f\"{combo:<15} {count:>10,} {count/total*100:>11.1f}%  {desc_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.4 Detailed Hazard Type Distribution\n",
    "\n",
    "Frequency analysis of specific hazard types detected.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HAZARD TYPE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count each hazard type\n",
    "hazard_counter = Counter()\n",
    "for hazards in df['hazard_types']:\n",
    "    hazard_counter.update(hazards)\n",
    "\n",
    "print(f\"\\n{'Hazard Type':<25} {'Count':>10} {'% of Corpus':>12} {'RDLS Code':<20}\")\n",
    "print(\"-\" * 70)\n",
    "for hazard, count in hazard_counter.most_common():\n",
    "    # Check if in RDLS codelist\n",
    "    rdls_code = hazard if hazard in RDLS_CODELISTS.get('hazard_type', []) else f\"{hazard}*\"\n",
    "    print(f\"{hazard:<25} {count:>10,} {count/total*100:>11.1f}% {rdls_code:<20}\")\n",
    "\n",
    "print(\"\\n* = May need mapping to official RDLS hazard_type code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.5 Detailed Exposure Category Distribution\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPOSURE CATEGORY DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count each exposure category\n",
    "exposure_counter = Counter()\n",
    "for categories in df['exposure_categories']:\n",
    "    exposure_counter.update(categories)\n",
    "\n",
    "print(f\"\\n{'Exposure Category':<25} {'Count':>10} {'% of Corpus':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for category, count in exposure_counter.most_common():\n",
    "    print(f\"{category:<25} {count:>10,} {count/total*100:>11.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.6 Return Period Analysis\n",
    "\n",
    "Distribution of extracted return period values.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RETURN PERIOD EXTRACTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Flatten all return periods\n",
    "all_rp = []\n",
    "for rps in df['return_periods']:\n",
    "    all_rp.extend(rps)\n",
    "\n",
    "rp_counter = Counter(all_rp)\n",
    "\n",
    "print(f\"\\nDatasets with return period: {df['has_return_period'].sum():,}\")\n",
    "print(f\"Total return period values found: {len(all_rp):,}\")\n",
    "print(f\"Unique return period values: {len(rp_counter):,}\")\n",
    "\n",
    "print(f\"\\n{'Return Period (years)':<25} {'Occurrences':>12}\")\n",
    "print(\"-\" * 40)\n",
    "for rp, count in rp_counter.most_common(20):\n",
    "    print(f\"{rp:<25} {count:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Duplication and Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Identify Potential Duplicates\n",
    "\n",
    "Detect datasets that appear to be versions/variants of each other.\n",
    "Uses title similarity and organization matching.\n",
    "\"\"\"\n",
    "\n",
    "def normalize_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize title for comparison by removing common variations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Original title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Normalized title\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        return ''\n",
    "    \n",
    "    # Lowercase\n",
    "    t = title.lower()\n",
    "    \n",
    "    # Remove country-specific suffixes (for GAR15 type datasets)\n",
    "    t = re.sub(r'\\s+for\\s+[\\w\\s-]+$', '', t)\n",
    "    \n",
    "    # Remove year references\n",
    "    t = re.sub(r'\\b(19|20)\\d{2}\\b', '', t)\n",
    "    \n",
    "    # Remove common version indicators\n",
    "    t = re.sub(r'\\b(v\\d+|version\\s*\\d+|rev\\s*\\d+)\\b', '', t)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    t = ' '.join(t.split())\n",
    "    \n",
    "    return t.strip()\n",
    "\n",
    "# Create normalized title column\n",
    "df['title_normalized'] = df['title'].apply(normalize_title)\n",
    "\n",
    "# Count duplicates by normalized title\n",
    "title_counts = df['title_normalized'].value_counts()\n",
    "duplicate_titles = title_counts[title_counts > 1]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTotal datasets: {len(df):,}\")\n",
    "print(f\"Unique normalized titles: {df['title_normalized'].nunique():,}\")\n",
    "print(f\"Potential duplicate groups: {len(duplicate_titles):,}\")\n",
    "print(f\"Records in duplicate groups: {df[df['title_normalized'].isin(duplicate_titles.index)].shape[0]:,}\")\n",
    "\n",
    "# Show largest duplicate groups\n",
    "print(f\"\\n--- Largest Duplicate Groups (by normalized title) ---\")\n",
    "print(f\"{'Normalized Title (truncated)':<50} {'Count':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for title, count in duplicate_titles.head(20).items():\n",
    "    display_title = title[:47] + '...' if len(title) > 50 else title\n",
    "    print(f\"{display_title:<50} {count:>8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 Identify Dataset Series (Country Variants)\n",
    "\n",
    "Detect systematic series like \"GAR15 Global Exposure Dataset for [Country]\".\n",
    "\"\"\"\n",
    "\n",
    "# Known series patterns\n",
    "SERIES_PATTERNS = [\n",
    "    (r'^gar15\\s+global\\s+exposure\\s+dataset', 'GAR15 Exposure'),\n",
    "    (r'^\\w{3}\\s+requirements\\s+and\\s+funding\\s+data', 'Requirements & Funding'),\n",
    "    (r'level\\s+1\\s+exposure\\s+data', 'Level 1 Exposure'),\n",
    "    (r'admin\\s*\\d+\\s+(boundaries|administrative)', 'Admin Boundaries'),\n",
    "    (r'flood\\s+hazard.*return\\s+period', 'Flood Hazard RP'),\n",
    "    (r'earthquake.*hazard.*pga', 'Earthquake PGA'),\n",
    "    (r'population\\s+(density|count|statistics)', 'Population Data'),\n",
    "]\n",
    "\n",
    "def identify_series(title: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Identify if title belongs to a known dataset series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    title : str\n",
    "        Dataset title\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Optional[str]\n",
    "        Series name if matched, None otherwise\n",
    "    \"\"\"\n",
    "    title_lower = title.lower() if title else ''\n",
    "    for pattern, series_name in SERIES_PATTERNS:\n",
    "        if re.search(pattern, title_lower):\n",
    "            return series_name\n",
    "    return None\n",
    "\n",
    "df['series'] = df['title'].apply(identify_series)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET SERIES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "series_counts = df['series'].value_counts()\n",
    "print(f\"\\nDatasets identified as part of series: {df['series'].notna().sum():,}\")\n",
    "print(f\"\\n{'Series Name':<30} {'Count':>10}\")\n",
    "print(\"-\" * 45)\n",
    "for series, count in series_counts.items():\n",
    "    print(f\"{series:<30} {count:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Organization and Source Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Risk-Relevant Organizations\n",
    "\n",
    "Identify organizations that publish HEVL-relevant data.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RISK-RELEVANT ORGANIZATIONS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Organizations with high HEVL signal rates\n",
    "org_hevl_stats = df.groupby('organization').agg({\n",
    "    'id': 'count',\n",
    "    'has_hazard': 'sum',\n",
    "    'has_exposure': 'sum',\n",
    "    'has_vulnerability': 'sum',\n",
    "    'has_loss': 'sum',\n",
    "}).rename(columns={'id': 'total_datasets'})\n",
    "\n",
    "# Calculate rates\n",
    "org_hevl_stats['hazard_rate'] = org_hevl_stats['has_hazard'] / org_hevl_stats['total_datasets']\n",
    "org_hevl_stats['exposure_rate'] = org_hevl_stats['has_exposure'] / org_hevl_stats['total_datasets']\n",
    "\n",
    "# Filter to orgs with significant HEVL content (at least 10 datasets and 30% HEVL rate)\n",
    "org_hevl_stats['any_hevl'] = org_hevl_stats['has_hazard'] + org_hevl_stats['has_exposure']\n",
    "org_hevl_stats['hevl_rate'] = org_hevl_stats['any_hevl'] / org_hevl_stats['total_datasets']\n",
    "\n",
    "risk_orgs = org_hevl_stats[\n",
    "    (org_hevl_stats['total_datasets'] >= 10) & \n",
    "    (org_hevl_stats['hevl_rate'] >= 0.3)\n",
    "].sort_values('any_hevl', ascending=False)\n",
    "\n",
    "print(f\"\\nOrganizations with significant risk data (>=10 datasets, >=30% HEVL rate):\")\n",
    "print(f\"\\n{'Organization':<45} {'Total':>8} {'Hazard':>8} {'Exposure':>8} {'HEVL%':>8}\")\n",
    "print(\"-\" * 80)\n",
    "for org, row in risk_orgs.head(20).iterrows():\n",
    "    org_display = org[:42] + '...' if len(org) > 45 else org\n",
    "    print(f\"{org_display:<45} {row['total_datasets']:>8,.0f} {row['has_hazard']:>8,.0f} {row['has_exposure']:>8,.0f} {row['hevl_rate']*100:>7.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Tag Analysis for HEVL Signals\n",
    "\n",
    "Analyze HDX tags to identify risk-relevant categorization.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TAG ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count all tags\n",
    "all_tags = []\n",
    "for tags_str in df['tags'].dropna():\n",
    "    all_tags.extend(tags_str.split('|'))\n",
    "\n",
    "tag_counter = Counter(t.strip() for t in all_tags if t.strip())\n",
    "\n",
    "# Risk-relevant tags\n",
    "risk_keywords = ['hazard', 'risk', 'disaster', 'flood', 'earthquake', 'cyclone', \n",
    "                 'drought', 'exposure', 'vulnerability', 'tsunami', 'storm']\n",
    "\n",
    "risk_tags = {tag: count for tag, count in tag_counter.items() \n",
    "             if any(kw in tag.lower() for kw in risk_keywords)}\n",
    "\n",
    "print(f\"\\nTotal unique tags: {len(tag_counter):,}\")\n",
    "print(f\"Risk-relevant tags: {len(risk_tags):,}\")\n",
    "\n",
    "print(f\"\\n--- Top Risk-Relevant Tags ---\")\n",
    "print(f\"{'Tag':<45} {'Count':>10}\")\n",
    "print(\"-\" * 58)\n",
    "for tag, count in sorted(risk_tags.items(), key=lambda x: x[1], reverse=True)[:25]:\n",
    "    print(f\"{tag:<45} {count:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample High-Quality HEVL Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Identify High-Quality Records for Each Component\n",
    "\n",
    "Find records with strong HEVL signals for manual review and pattern validation.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_signal_strength(row: pd.Series) -> int:\n",
    "    \"\"\"\n",
    "    Calculate overall HEVL signal strength score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row : pd.Series\n",
    "        DataFrame row\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Signal strength score (0-10)\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    score += len(row['hazard_types']) * 2  # Weight hazard signals\n",
    "    score += len(row['exposure_categories']) * 2\n",
    "    score += len(row['process_types'])\n",
    "    score += len(row['analysis_types']) * 2\n",
    "    score += len(row['return_periods']) * 3  # Return periods are very specific\n",
    "    return min(score, 20)  # Cap at 20\n",
    "\n",
    "df['signal_strength'] = df.apply(calculate_signal_strength, axis=1)\n",
    "\n",
    "# Find top records for each component\n",
    "print(\"=\" * 70)\n",
    "print(\"HIGH-QUALITY HEVL RECORD SAMPLES\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.2 Sample Hazard-Rich Records\n",
    "\"\"\"\n",
    "\n",
    "hazard_rich = df[df['has_hazard'] & df['has_return_period']].nlargest(10, 'signal_strength')\n",
    "\n",
    "print(\"\\n--- Top 10 Hazard-Rich Records (with return periods) ---\\n\")\n",
    "for idx, row in hazard_rich.iterrows():\n",
    "    print(f\"Title: {row['title'][:80]}\")\n",
    "    print(f\"  Organization: {row['organization']}\")\n",
    "    print(f\"  Hazard types: {row['hazard_types']}\")\n",
    "    print(f\"  Process types: {row['process_types']}\")\n",
    "    print(f\"  Analysis types: {row['analysis_types']}\")\n",
    "    print(f\"  Return periods: {row['return_periods']}\")\n",
    "    print(f\"  Signal strength: {row['signal_strength']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.3 Sample Exposure-Rich Records\n",
    "\"\"\"\n",
    "\n",
    "exposure_rich = df[df['has_exposure'] & (df['exposure_categories'].apply(len) >= 2)].nlargest(10, 'signal_strength')\n",
    "\n",
    "print(\"\\n--- Top 10 Exposure-Rich Records (multiple categories) ---\\n\")\n",
    "for idx, row in exposure_rich.iterrows():\n",
    "    print(f\"Title: {row['title'][:80]}\")\n",
    "    print(f\"  Organization: {row['organization']}\")\n",
    "    print(f\"  Exposure categories: {row['exposure_categories']}\")\n",
    "    print(f\"  Tags: {row['tags'][:60]}\")\n",
    "    print(f\"  Signal strength: {row['signal_strength']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7.1 Export Analysis DataFrames\n",
    "\n",
    "Save analysis results for downstream notebooks.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare export DataFrame (exclude large text columns)\n",
    "export_cols = [\n",
    "    'id', 'name', 'title', 'organization', 'groups', 'tags',\n",
    "    'hazard_types', 'process_types', 'exposure_categories', 'analysis_types',\n",
    "    'vuln_loss_indicators', 'return_periods',\n",
    "    'has_hazard', 'has_exposure', 'has_vulnerability', 'has_loss',\n",
    "    'has_return_period', 'hevl_combo', 'signal_strength', 'series'\n",
    "]\n",
    "\n",
    "df_export = df[export_cols].copy()\n",
    "\n",
    "# Convert lists to pipe-separated strings for CSV compatibility\n",
    "list_cols = ['hazard_types', 'process_types', 'exposure_categories', \n",
    "             'analysis_types', 'vuln_loss_indicators', 'return_periods']\n",
    "for col in list_cols:\n",
    "    df_export[col] = df_export[col].apply(lambda x: '|'.join(map(str, x)) if x else '')\n",
    "\n",
    "# Save full analysis\n",
    "output_file = OUTPUT_DIR / 'hdx_hevl_signal_analysis.csv'\n",
    "df_export.to_csv(output_file, index=False)\n",
    "print(f\"Saved: {output_file}\")\n",
    "print(f\"  Records: {len(df_export):,}\")\n",
    "print(f\"  Columns: {len(df_export.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7.2 Export Summary Statistics\n",
    "\"\"\"\n",
    "\n",
    "summary = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'total_datasets': len(df),\n",
    "    'unique_organizations': df['organization'].nunique(),\n",
    "    \n",
    "    'hevl_detection': {\n",
    "        'hazard_signal_count': int(df['has_hazard'].sum()),\n",
    "        'hazard_signal_rate': round(df['has_hazard'].mean(), 4),\n",
    "        'exposure_signal_count': int(df['has_exposure'].sum()),\n",
    "        'exposure_signal_rate': round(df['has_exposure'].mean(), 4),\n",
    "        'vulnerability_signal_count': int(df['has_vulnerability'].sum()),\n",
    "        'vulnerability_signal_rate': round(df['has_vulnerability'].mean(), 4),\n",
    "        'loss_signal_count': int(df['has_loss'].sum()),\n",
    "        'loss_signal_rate': round(df['has_loss'].mean(), 4),\n",
    "        'return_period_count': int(df['has_return_period'].sum()),\n",
    "        'return_period_rate': round(df['has_return_period'].mean(), 4),\n",
    "    },\n",
    "    \n",
    "    'hazard_type_counts': dict(hazard_counter.most_common()),\n",
    "    'exposure_category_counts': dict(exposure_counter.most_common()),\n",
    "    'return_period_counts': dict(rp_counter.most_common(20)),\n",
    "    \n",
    "    'hevl_combinations': dict(df['hevl_combo'].value_counts().head(10)),\n",
    "    \n",
    "    'duplication': {\n",
    "        'unique_normalized_titles': int(df['title_normalized'].nunique()),\n",
    "        'duplicate_groups': int(len(duplicate_titles)),\n",
    "    },\n",
    "    \n",
    "    'series_counts': dict(series_counts) if len(series_counts) > 0 else {},\n",
    "}\n",
    "\n",
    "summary_file = OUTPUT_DIR / 'hdx_hevl_signal_summary.json'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"\\nSaved: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7.3 Export High-Signal Records for Manual Review\n",
    "\"\"\"\n",
    "\n",
    "# Top 500 records by signal strength\n",
    "high_signal = df.nlargest(500, 'signal_strength')[[\n",
    "    'id', 'title', 'organization', 'hazard_types', 'exposure_categories',\n",
    "    'analysis_types', 'return_periods', 'signal_strength', 'hevl_combo'\n",
    "]].copy()\n",
    "\n",
    "for col in ['hazard_types', 'exposure_categories', 'analysis_types', 'return_periods']:\n",
    "    high_signal[col] = high_signal[col].apply(lambda x: '|'.join(map(str, x)) if x else '')\n",
    "\n",
    "high_signal_file = OUTPUT_DIR / 'hdx_high_signal_records.csv'\n",
    "high_signal.to_csv(high_signal_file, index=False)\n",
    "print(f\"Saved: {high_signal_file}\")\n",
    "print(f\"  Records: {len(high_signal):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "8.1 Analysis Summary Report\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "CORPUS OVERVIEW\n",
    "---------------\n",
    "Total HDX datasets analyzed: {len(df):,}\n",
    "Unique organizations: {df['organization'].nunique():,}\n",
    "\n",
    "HEVL SIGNAL COVERAGE\n",
    "--------------------\n",
    "Datasets with Hazard signals:       {df['has_hazard'].sum():>6,} ({df['has_hazard'].mean()*100:.1f}%)\n",
    "Datasets with Exposure signals:     {df['has_exposure'].sum():>6,} ({df['has_exposure'].mean()*100:.1f}%)\n",
    "Datasets with Vulnerability signals:{df['has_vulnerability'].sum():>6,} ({df['has_vulnerability'].mean()*100:.1f}%)\n",
    "Datasets with Loss signals:         {df['has_loss'].sum():>6,} ({df['has_loss'].mean()*100:.1f}%)\n",
    "Datasets with Return Period info:   {df['has_return_period'].sum():>6,} ({df['has_return_period'].mean()*100:.1f}%)\n",
    "\n",
    "INFERENCE POTENTIAL\n",
    "-------------------\n",
    "Datasets suitable for Hazard block population: ~{df['has_hazard'].sum():,}\n",
    "  - With specific hazard type: {(df['hazard_types'].apply(len) >= 1).sum():,}\n",
    "  - With process type detail:  {(df['process_types'].apply(len) >= 1).sum():,}\n",
    "  - With analysis type:        {(df['analysis_types'].apply(len) >= 1).sum():,}\n",
    "  - With return periods:       {df['has_return_period'].sum():,}\n",
    "\n",
    "Datasets suitable for Exposure block population: ~{df['has_exposure'].sum():,}\n",
    "  - With category detected:    {(df['exposure_categories'].apply(len) >= 1).sum():,}\n",
    "\n",
    "DUPLICATION STATUS\n",
    "------------------\n",
    "Potential duplicate groups: {len(duplicate_titles):,}\n",
    "Records in duplicate groups: {df[df['title_normalized'].isin(duplicate_titles.index)].shape[0]:,}\n",
    "Identified dataset series: {df['series'].notna().sum():,}\n",
    "\n",
    "KEY INSIGHTS\n",
    "------------\n",
    "1. Hazard data is well-represented ({df['has_hazard'].mean()*100:.0f}% of corpus)\n",
    "2. Return period extraction is feasible for {df['has_return_period'].sum():,} datasets\n",
    "3. Major risk data publishers: UNDRR, GEM, OCHA, WFP, UNOSAT\n",
    "4. Significant series duplication exists (GAR15 Exposure: ~190 country variants)\n",
    "\n",
    "RECOMMENDED NEXT STEPS\n",
    "----------------------\n",
    "1. Build Signal Dictionary with confident mappings to RDLS codelists\n",
    "2. Develop Hazard block extractor (highest coverage potential)\n",
    "3. Handle series deduplication to avoid redundant processing\n",
    "4. Develop Exposure block extractor\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nAnalysis completed: {datetime.now().isoformat()}\")\n",
    "print(f\"Output files saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
