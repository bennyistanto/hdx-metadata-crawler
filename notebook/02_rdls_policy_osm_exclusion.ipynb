{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 02: OSM Policy Exclusion Index\n",
    "\n",
    "**Purpose**: Scan HDX dataset metadata and flag datasets derived from OpenStreetMap (OSM) for exclusion.\n",
    "\n",
    "**Process**:\n",
    "1. Load HDX dataset-level metadata JSON files\n",
    "2. Detect OSM-derived datasets using multiple signals\n",
    "3. Produce exclusion list (dataset UUIDs)\n",
    "4. Generate audit report with detection reasons\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook started: 2026-02-08T20:25:11.548106\n",
      "Progress bars: Available\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/dataset_metadata\n",
      "Output: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/policy\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input: HDX dataset metadata from Notebook 01\n",
    "DUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DATASET_DIR = DUMP_DIR / 'dataset_metadata'\n",
    "\n",
    "# Output: Policy artifacts\n",
    "POLICY_DIR = DUMP_DIR / 'policy'\n",
    "OUT_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n",
    "OUT_REPORT_CSV = POLICY_DIR / 'osm_exclusion_report.csv'\n",
    "OUT_PILOT_CSV = POLICY_DIR / 'osm_candidates_for_pilot.csv'\n",
    "\n",
    "# Configuration\n",
    "USE_FAST_PREFILTER = True  # Text scan before JSON parse\n",
    "PILOT_MAX_PER_BUCKET = 10  # Limit candidates per org/theme\n",
    "\n",
    "# Create output directory\n",
    "POLICY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {DATASET_DIR}\")\n",
    "print(f\"Output: {POLICY_DIR}\")",
    "\n",
    "\n",
    "# ── Output cleanup mode ───────────────────────────────────────────────\n",
    "CLEANUP_MODE = \"replace\"\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-3-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Clean Previous Outputs\n",
    "\n",
    "Remove stale output files from previous runs (controlled by CLEANUP_MODE).\n",
    "\"\"\"\n",
    "\n",
    "def clean_previous_outputs(output_dir, patterns, label, mode=\"replace\"):\n",
    "    \"\"\"\n",
    "    Remove previous output files matching the given glob patterns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : Path\n",
    "        Directory containing old outputs.\n",
    "    patterns : list[str]\n",
    "        Glob patterns to match.\n",
    "    label : str\n",
    "        Human-readable label for log messages.\n",
    "    mode : str\n",
    "        One of: \"replace\" (auto-delete), \"prompt\" (ask user),\n",
    "        \"skip\" (keep old files), \"abort\" (error if stale files exist).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  with keys 'deleted' (int) and 'skipped' (bool)\n",
    "    \"\"\"\n",
    "    result = {'deleted': 0, 'skipped': False}\n",
    "    targets = {}\n",
    "    for pattern in patterns:\n",
    "        matches = sorted(output_dir.glob(pattern))\n",
    "        if matches:\n",
    "            targets[pattern] = matches\n",
    "    total = sum(len(files) for files in targets.values())\n",
    "\n",
    "    if total == 0:\n",
    "        print(f'Output cleanup [{label}]: Directory is clean.')\n",
    "        return result\n",
    "\n",
    "    summary = []\n",
    "    for pattern, files in targets.items():\n",
    "        summary.append(f'  {pattern:40s}: {len(files):,} files')\n",
    "\n",
    "    if mode == 'skip':\n",
    "        print(f'Output cleanup [{label}]: SKIPPED ({total:,} existing files kept)')\n",
    "        result['skipped'] = True\n",
    "        return result\n",
    "\n",
    "    if mode == 'abort':\n",
    "        raise RuntimeError(\n",
    "            f'Output cleanup [{label}]: ABORT -- {total:,} stale files found. '\n",
    "            f'Delete manually or change CLEANUP_MODE.'\n",
    "        )\n",
    "\n",
    "    if mode == 'prompt':\n",
    "        print(f'Output cleanup [{label}]: Found {total:,} existing output files:')\n",
    "        for line in summary:\n",
    "            print(line)\n",
    "        choice = input('Choose [R]eplace / [S]kip / [A]bort: ').strip().lower()\n",
    "        if choice in ('s', 'skip'):\n",
    "            print('  Skipped.')\n",
    "            result['skipped'] = True\n",
    "            return result\n",
    "        elif choice in ('a', 'abort'):\n",
    "            raise RuntimeError('User chose to abort.')\n",
    "        elif choice not in ('r', 'replace', ''):\n",
    "            print(f'  Unknown choice, defaulting to Replace.')\n",
    "\n",
    "    # Mode: replace (default)\n",
    "    print(f'Output cleanup [{label}]:')\n",
    "    for line in summary:\n",
    "        print(line)\n",
    "    for pattern, files in targets.items():\n",
    "        for f in files:\n",
    "            try:\n",
    "                f.unlink()\n",
    "                result['deleted'] += 1\n",
    "            except Exception as e:\n",
    "                print(f'  WARNING: Could not delete {f.name}: {e}')\n",
    "    deleted_count = result['deleted']\n",
    "    print(f'  Cleaned {deleted_count:,} files. Ready for fresh output.')\n",
    "    print()\n",
    "    return result\n",
    "\n",
    "# ── Run cleanup ────────────────────────────────────────────────────────\n",
    "clean_previous_outputs(\n",
    "    POLICY_DIR,\n",
    "    patterns=[\n",
    "        \"osm_excluded_dataset_ids.txt\",\n",
    "        \"osm_exclusion_report.csv\",\n",
    "        \"osm_candidates_for_pilot.csv\",\n",
    "    ],\n",
    "    label=\"NB 02 OSM Exclusion\",\n",
    "    mode=CLEANUP_MODE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Detection Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.1 Helper Functions\n",
    "\n",
    "Utilities for robust JSON access and data extraction.\n",
    "\"\"\"\n",
    "\n",
    "def read_text(path: Path) -> str:\n",
    "    \"\"\"Read text safely with UTF-8 encoding.\"\"\"\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file into dict.\"\"\"\n",
    "    return json.loads(read_text(path))\n",
    "\n",
    "\n",
    "def normalize_dataset_record(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize dataset record shape.\n",
    "    \n",
    "    Handles both direct HDX export and CKAN fallback wrapper.\n",
    "    \"\"\"\n",
    "    if isinstance(raw, dict) and \"id\" in raw:\n",
    "        return raw\n",
    "    if isinstance(raw, dict) and \"dataset\" in raw and isinstance(raw[\"dataset\"], dict):\n",
    "        return raw[\"dataset\"]\n",
    "    return raw\n",
    "\n",
    "\n",
    "def norm_str(x: Any) -> str:\n",
    "    \"\"\"Normalize value to lowercase stripped string with Unicode normalization.\"\"\"\n",
    "    s = (x or \"\").__str__().strip().lower()\n",
    "    return unicodedata.normalize('NFKD', s)\n",
    "\n",
    "\n",
    "def get_org_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract organization title/name.\"\"\"\n",
    "    org = ds.get(\"organization\")\n",
    "    if isinstance(org, dict):\n",
    "        return (org.get(\"title\") or org.get(\"name\") or \"\").strip()\n",
    "    return (org or \"\").strip()\n",
    "\n",
    "\n",
    "def get_tags(ds: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Extract tags as lowercase strings.\"\"\"\n",
    "    tags = ds.get(\"tags\") or []\n",
    "    out: List[str] = []\n",
    "    if isinstance(tags, list):\n",
    "        for t in tags:\n",
    "            if isinstance(t, dict):\n",
    "                name = t.get(\"name\") or \"\"\n",
    "                if name:\n",
    "                    out.append(name.strip().lower())\n",
    "            elif isinstance(t, str):\n",
    "                out.append(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_resources(ds: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract resources list.\"\"\"\n",
    "    res = ds.get(\"resources\") or []\n",
    "    return res if isinstance(res, list) else []\n",
    "\n",
    "\n",
    "def get_license_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract normalized license string.\"\"\"\n",
    "    lt = ds.get(\"license_title\") or ds.get(\"license_id\") or \"\"\n",
    "    return (lt or \"\").strip()\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-2-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSM detection rules defined.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 OSM Detection Rules\n",
    "\n",
    "Policy-based detection with traceable reasons.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OSMDetectionResult:\n",
    "    \"\"\"\n",
    "    Result of OSM detection for a dataset.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    is_osm : bool\n",
    "        Whether dataset is OSM-derived\n",
    "    reasons : Tuple[str, ...]\n",
    "        Detection rule IDs that fired\n",
    "    signals : Dict[str, Any]\n",
    "        Evidence fields for auditing\n",
    "    \"\"\"\n",
    "    is_osm: bool\n",
    "    reasons: Tuple[str, ...]\n",
    "    signals: Dict[str, Any]\n",
    "\n",
    "\n",
    "# Fast prefilter markers\n",
    "FAST_MARKERS = (\n",
    "    \"openstreetmap contributors\",\n",
    "    '\"dataset_source\":',\n",
    "    '\"license_title\": \"odbl\"',\n",
    "    '\"license_title\":\"odbl\"',\n",
    "    \"open database license\",\n",
    "    \"hotosm\",\n",
    "    \"export.hotosm.org\",\n",
    "    \"openstreetmap.org\",\n",
    "    \"openstreetmap\",\n",
    ")\n",
    "\n",
    "# URL markers for OSM/HOT\n",
    "OSM_URL_MARKERS = (\n",
    "    \"openstreetmap.org\",\n",
    "    \"hotosm.org\",\n",
    "    \"export.hotosm.org\",\n",
    "    \"exports-stage.hotosm.org\",\n",
    "    \"production-raw-data-api\",\n",
    ")\n",
    "\n",
    "# Organization markers\n",
    "OSM_ORG_MARKERS = (\n",
    "    \"humanitarian openstreetmap\",\n",
    "    \"hotosm\",\n",
    "    \"openstreetmap\",\n",
    ")\n",
    "\n",
    "# Title markers\n",
    "OSM_TITLE_MARKERS = (\n",
    "    \"openstreetmap export\",\n",
    "    \"(openstreetmap export)\",\n",
    "    \"openstreetmap\",\n",
    ")\n",
    "\n",
    "# Notes markers\n",
    "OSM_NOTES_MARKERS = (\n",
    "    \"openstreetmap\",\n",
    "    \"wiki.openstreetmap.org\",\n",
    "    \"osm\",\n",
    ")\n",
    "\n",
    "# Minimum number of supporting (non-strong) signals to flag borderline cases\n",
    "SUPPORTING_EVIDENCE_THRESHOLD = 2\n",
    "\n",
    "\n",
    "def prefilter_maybe_osm(text: str) -> bool:\n",
    "    \"\"\"Quick text scan for OSM indicators.\"\"\"\n",
    "    t = text.lower()\n",
    "    return any(m in t for m in FAST_MARKERS)\n",
    "\n",
    "\n",
    "def detect_osm(ds: Dict[str, Any]) -> OSMDetectionResult:\n",
    "    \"\"\"\n",
    "    Detect whether a dataset is derived from OpenStreetMap.\n",
    "    \n",
    "    Uses multiple signals with policy-based scoring:\n",
    "    1. dataset_source (strongest)\n",
    "    2. license evidence (ODbL) + OSM cues\n",
    "    3. resource URL evidence\n",
    "    4. organization/title/notes evidence\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : Dict[str, Any]\n",
    "        Dataset metadata dict\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    OSMDetectionResult\n",
    "        Detection result with reasons\n",
    "    \"\"\"\n",
    "    title = ds.get(\"title\") or \"\"\n",
    "    notes = ds.get(\"notes\") or \"\"\n",
    "    dataset_source = ds.get(\"dataset_source\") or \"\"\n",
    "    org_title = get_org_title(ds)\n",
    "    license_title = get_license_title(ds)\n",
    "    tags = get_tags(ds)\n",
    "    resources = get_resources(ds)\n",
    "    \n",
    "    title_l = norm_str(title)\n",
    "    notes_l = norm_str(notes)\n",
    "    dataset_source_l = norm_str(dataset_source)\n",
    "    org_l = norm_str(org_title)\n",
    "    license_l = norm_str(license_title)\n",
    "    \n",
    "    reasons: List[str] = []\n",
    "    \n",
    "    # Rule 1: dataset_source references OpenStreetMap\n",
    "    if \"openstreetmap\" in dataset_source_l:\n",
    "        reasons.append(\"dataset_source_mentions_openstreetmap\")\n",
    "    \n",
    "    # Rule 2: ODbL license with OSM cues\n",
    "    if license_l in {\"odbl\", \"odc-odbl\"} or \"odbl\" in license_l or \"open database license\" in license_l:\n",
    "        if (\"openstreetmap\" in title_l) or (\"openstreetmap\" in notes_l) or (\"openstreetmap\" in dataset_source_l):\n",
    "            reasons.append(\"odbl_license_plus_osm_cue\")\n",
    "    \n",
    "    # Rule 3: Resource URLs point to HOT/OSM\n",
    "    for r in resources:\n",
    "        url = norm_str(r.get(\"download_url\") or r.get(\"url\") or \"\")\n",
    "        if url and any(m in url for m in OSM_URL_MARKERS):\n",
    "            reasons.append(\"resource_url_osm_domain\")\n",
    "            break\n",
    "    \n",
    "    # Rule 4: Organization suggests OSM/HOT\n",
    "    # NOTE: Broad substring match is intentional -- any org with \"openstreetmap\"\n",
    "    # in its name is likely producing OSM-derived data.  The supporting-evidence\n",
    "    # threshold (2+ signals) already gates borderline cases.\n",
    "    if any(m in org_l for m in OSM_ORG_MARKERS):\n",
    "        reasons.append(\"organization_mentions_osm_or_hot\")\n",
    "    \n",
    "    # Rule 5: Title suggests OSM export\n",
    "    if any(m in title_l for m in OSM_TITLE_MARKERS):\n",
    "        reasons.append(\"title_mentions_osm_export\")\n",
    "    \n",
    "    # Rule 6: Tags include openstreetmap\n",
    "    if \"openstreetmap\" in tags:\n",
    "        reasons.append(\"tag_openstreetmap_present\")\n",
    "    \n",
    "    # Rule 7: Notes include OSM references\n",
    "    if any(m in notes_l for m in OSM_NOTES_MARKERS) and \"openstreetmap\" in notes_l:\n",
    "        reasons.append(\"notes_mentions_openstreetmap\")\n",
    "    \n",
    "    # Policy: mark OSM if any strong rules fire\n",
    "    strong = {\n",
    "        \"dataset_source_mentions_openstreetmap\",\n",
    "        \"resource_url_osm_domain\",\n",
    "        \"odbl_license_plus_osm_cue\",\n",
    "        \"tag_openstreetmap_present\",\n",
    "    }\n",
    "    is_osm = any(r in strong for r in reasons)\n",
    "    \n",
    "    # Allow supporting evidence to upgrade borderline cases\n",
    "    supporting = set(reasons) - strong\n",
    "    if not is_osm and len(supporting) >= SUPPORTING_EVIDENCE_THRESHOLD:\n",
    "        is_osm = True\n",
    "        reasons.append(\"supporting_evidence_threshold_met\")\n",
    "    \n",
    "    signals = {\n",
    "        \"dataset_source\": dataset_source,\n",
    "        \"license_title\": license_title,\n",
    "        \"organization\": org_title,\n",
    "        \"tags\": tags,\n",
    "        \"resource_url_sample\": (resources[0].get(\"download_url\") if resources else None),\n",
    "    }\n",
    "    \n",
    "    return OSMDetectionResult(\n",
    "        is_osm=is_osm,\n",
    "        reasons=tuple(sorted(set(reasons))),\n",
    "        signals=signals\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"OSM detection rules defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Scan Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 26,246 JSON files in: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/dataset_metadata\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4619492897864983b11fbd97405d8a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning for OSM:   0%|          | 0/26246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilter skipped (no OSM markers): 0 / 26,246\n",
      "\n",
      "Flagged OSM-derived datasets: 3,649\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.1 Scan All Dataset Files\n",
    "\n",
    "Process all JSON files and detect OSM datasets.\n",
    "\"\"\"\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in folder, sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Input folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "def scan_folder_for_osm(input_dir: Path) -> Tuple[List[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Scan folder for OSM-derived datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : Path\n",
    "        Folder containing dataset JSON files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[Dict], List[str]]\n",
    "        (report_rows, excluded_ids)\n",
    "    \"\"\"\n",
    "    report_rows: List[Dict[str, Any]] = []\n",
    "    excluded_ids: List[str] = []\n",
    "    prefilter_skipped: int = 0\n",
    "    \n",
    "    files = list(iter_json_files(input_dir))\n",
    "    total = len(files)\n",
    "    \n",
    "    print(f\"Scanning {total:,} JSON files in: {input_dir}\")\n",
    "    \n",
    "    # Use tqdm if available\n",
    "    iterator = tqdm(files, desc=\"Scanning for OSM\") if HAS_TQDM else files\n",
    "    \n",
    "    for i, path in enumerate(iterator, start=1):\n",
    "        # Progress for non-tqdm\n",
    "        if not HAS_TQDM and i % 2000 == 0:\n",
    "            print(f\"  Processed {i:,}/{total:,}\")\n",
    "        \n",
    "        try:\n",
    "            txt = read_text(path) if USE_FAST_PREFILTER else \"\"\n",
    "            if USE_FAST_PREFILTER and not prefilter_maybe_osm(txt):\n",
    "                prefilter_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            raw = json.loads(txt) if USE_FAST_PREFILTER else load_json(path)\n",
    "            ds = normalize_dataset_record(raw)\n",
    "            \n",
    "            ds_id = ds.get(\"id\") or \"\"\n",
    "            title = ds.get(\"title\") or \"\"\n",
    "            name = ds.get(\"name\") or \"\"\n",
    "            org = get_org_title(ds)\n",
    "            \n",
    "            result = detect_osm(ds)\n",
    "            \n",
    "            if result.is_osm:\n",
    "                excluded_ids.append(ds_id)\n",
    "                \n",
    "                report_rows.append({\n",
    "                    \"dataset_id\": ds_id,\n",
    "                    \"name\": name,\n",
    "                    \"title\": title,\n",
    "                    \"organization\": org,\n",
    "                    \"dataset_source\": ds.get(\"dataset_source\"),\n",
    "                    \"license_title\": get_license_title(ds),\n",
    "                    \"reasons\": \";\".join(result.reasons),\n",
    "                    \"tags\": \";\".join(get_tags(ds)),\n",
    "                    \"n_resources\": len(get_resources(ds)),\n",
    "                    \"file\": str(path),\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            report_rows.append({\n",
    "                \"dataset_id\": \"\",\n",
    "                \"name\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"organization\": \"\",\n",
    "                \"dataset_source\": \"\",\n",
    "                \"license_title\": \"\",\n",
    "                \"reasons\": f\"ERROR:{type(e).__name__}:{e}\",\n",
    "                \"tags\": \"\",\n",
    "                \"n_resources\": \"\",\n",
    "                \"file\": str(path),\n",
    "            })\n",
    "    \n",
    "    # Deduplicate IDs\n",
    "    excluded_ids = sorted(set([x for x in excluded_ids if x]))\n",
    "    \n",
    "    if USE_FAST_PREFILTER:\n",
    "        print(f\"Prefilter skipped (no OSM markers): {prefilter_skipped:,} / {total:,}\")\n",
    "    \n",
    "    return report_rows, excluded_ids\n",
    "\n",
    "\n",
    "# Run scan\n",
    "report_rows, excluded_ids = scan_folder_for_osm(DATASET_DIR)\n",
    "\n",
    "print(f\"\\nFlagged OSM-derived datasets: {len(excluded_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/policy/osm_excluded_dataset_ids.txt\n",
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/policy/osm_exclusion_report.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.1 Save Exclusion List and Report\n",
    "\"\"\"\n",
    "\n",
    "def write_ids_txt(path: Path, ids: Sequence[str]) -> None:\n",
    "    \"\"\"Write IDs to text file, one per line.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for x in ids:\n",
    "            f.write(f\"{x}\\n\")\n",
    "\n",
    "\n",
    "def write_report_csv(path: Path, rows: Sequence[Dict[str, Any]]) -> None:\n",
    "    \"\"\"Write report to CSV file.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    header = [\n",
    "        \"dataset_id\", \"name\", \"title\", \"organization\",\n",
    "        \"dataset_source\", \"license_title\", \"reasons\",\n",
    "        \"tags\", \"n_resources\", \"file\"\n",
    "    ]\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in header})\n",
    "\n",
    "\n",
    "# Write outputs\n",
    "write_ids_txt(OUT_IDS_TXT, excluded_ids)\n",
    "write_report_csv(OUT_REPORT_CSV, report_rows)\n",
    "\n",
    "print(f\"Wrote: {OUT_IDS_TXT}\")\n",
    "print(f\"Wrote: {OUT_REPORT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-4-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/policy/osm_candidates_for_pilot.csv (138 rows)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.2 Create Pilot Shortlist (Optional)\n",
    "\n",
    "Small sample for future OSM pilot experiments.\n",
    "\"\"\"\n",
    "\n",
    "def derive_theme(tags: List[str]) -> str:\n",
    "    \"\"\"Infer theme category from tags.\"\"\"\n",
    "    if any(t in tags for t in (\"roads\", \"railways\", \"transportation\", \"aviation\")):\n",
    "        return \"transport\"\n",
    "    if any(t in tags for t in (\"health facilities\", \"health\")):\n",
    "        return \"health_facilities\"\n",
    "    if any(t in tags for t in (\"waterways\", \"rivers\", \"hydrology\")):\n",
    "        return \"hydrology\"\n",
    "    if any(t in tags for t in (\"administrative boundaries-divisions\", \"gazetteer\")):\n",
    "        return \"boundaries_gazetteer\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def make_pilot_shortlist(rows: List[Dict[str, Any]], max_per_bucket: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create small pilot shortlist grouped by org/theme.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rows : List[Dict]\n",
    "        Report rows\n",
    "    max_per_bucket : int\n",
    "        Maximum samples per org/theme combination\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict]\n",
    "        Pilot shortlist\n",
    "    \"\"\"\n",
    "    clean = [r for r in rows if r.get(\"dataset_id\")]\n",
    "    \n",
    "    for r in clean:\n",
    "        tags = (r.get(\"tags\") or \"\").split(\";\") if r.get(\"tags\") else []\n",
    "        tags = [t.strip().lower() for t in tags if t.strip()]\n",
    "        r[\"theme\"] = derive_theme(tags)\n",
    "    \n",
    "    # Group by (org, theme)\n",
    "    buckets: Dict[Tuple[str, str], List[Dict[str, Any]]] = {}\n",
    "    for r in clean:\n",
    "        key = ((r.get(\"organization\") or \"unknown\").strip(), (r.get(\"theme\") or \"other\").strip())\n",
    "        buckets.setdefault(key, []).append(r)\n",
    "    \n",
    "    pilot: List[Dict[str, Any]] = []\n",
    "    for (org, theme), items in sorted(buckets.items(), key=lambda x: (-len(x[1]), x[0])):\n",
    "        items_sorted = sorted(items, key=lambda r: (r.get(\"title\") or \"\"))\n",
    "        pilot.extend(items_sorted[:max_per_bucket])\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"dataset_id\": r[\"dataset_id\"],\n",
    "            \"title\": r[\"title\"],\n",
    "            \"organization\": r[\"organization\"],\n",
    "            \"theme\": r[\"theme\"],\n",
    "            \"reasons\": r[\"reasons\"],\n",
    "        }\n",
    "        for r in pilot\n",
    "    ]\n",
    "\n",
    "\n",
    "pilot_rows = make_pilot_shortlist(report_rows, max_per_bucket=PILOT_MAX_PER_BUCKET)\n",
    "\n",
    "if pilot_rows:\n",
    "    with OUT_PILOT_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"dataset_id\", \"title\", \"organization\", \"theme\", \"reasons\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(pilot_rows)\n",
    "    print(f\"Wrote: {OUT_PILOT_CSV} ({len(pilot_rows)} rows)\")\n",
    "else:\n",
    "    print(\"No pilot rows produced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OSM EXCLUSION SUMMARY\n",
      "============================================================\n",
      "Total flagged: 3,649\n",
      "Pilot candidates: 138\n",
      "\n",
      "Example excluded IDs (first 10):\n",
      "  - 003b676c-3e72-4ab2-a33b-2d3b9f9d7857\n",
      "  - 004790b4-4ddd-4d7e-9288-73ae46e643a3\n",
      "  - 0048d3d1-50eb-4428-a506-23882f4ce7ac\n",
      "  - 00503aef-1f17-44fd-8664-679c5ad6e05c\n",
      "  - 00a377de-920d-43c4-8fa1-590e2b369dcf\n",
      "  - 00ad2859-55c5-4e91-bbe6-d79d3fde2dc2\n",
      "  - 00b5e80c-1e32-4603-ae37-04b54655389c\n",
      "  - 00d7616a-c748-41c1-ae80-e65669293924\n",
      "  - 00e423c7-70dd-4a0c-ace7-8996e782a99e\n",
      "  - 00e5fec7-756d-4b1d-8816-21fb89320e48\n",
      "\n",
      "Outputs:\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/policy/osm_excluded_dataset_ids.txt\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/policy/osm_exclusion_report.csv\n",
      "  - /mnt/c/Users/benny/OneDrive/Documents/Github/hdx-metadata-crawler/hdx_dataset_metadata_dump/policy/osm_candidates_for_pilot.csv\n",
      "\n",
      "Notebook completed: 2026-02-08T20:31:45.941349\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "5.1 Display Summary\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OSM EXCLUSION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total flagged: {len(excluded_ids):,}\")\n",
    "print(f\"Pilot candidates: {len(pilot_rows):,}\")\n",
    "\n",
    "print(f\"\\nExample excluded IDs (first 10):\")\n",
    "for ds_id in excluded_ids[:10]:\n",
    "    print(f\"  - {ds_id}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - {OUT_IDS_TXT}\")\n",
    "print(f\"  - {OUT_REPORT_CSV}\")\n",
    "print(f\"  - {OUT_PILOT_CSV}\")\n",
    "\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d96cff-204e-426c-be0d-e957f2e24e70",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
