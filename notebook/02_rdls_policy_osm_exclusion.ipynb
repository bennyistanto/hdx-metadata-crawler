{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 02: OSM Policy Exclusion Index\n",
    "\n",
    "**Purpose**: Scan HDX dataset metadata and flag datasets derived from OpenStreetMap (OSM) for exclusion.\n",
    "\n",
    "**Process**:\n",
    "1. Load HDX dataset-level metadata JSON files\n",
    "2. Detect OSM-derived datasets using multiple signals\n",
    "3. Produce exclusion list (dataset UUIDs)\n",
    "4. Generate audit report with detection reasons\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n",
    "\n",
    "# Optional: tqdm for progress bars\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "    print(\"Note: tqdm not installed. Install with: pip install tqdm\")\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")\n",
    "print(f\"Progress bars: {'Available' if HAS_TQDM else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Configure Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "# Input: HDX dataset metadata from Notebook 01\n",
    "DUMP_DIR = BASE_DIR / 'hdx_dataset_metadata_dump'\n",
    "DATASET_DIR = DUMP_DIR / 'dataset_metadata'\n",
    "\n",
    "# Output: Policy artifacts\n",
    "POLICY_DIR = DUMP_DIR / 'policy'\n",
    "OUT_IDS_TXT = POLICY_DIR / 'osm_excluded_dataset_ids.txt'\n",
    "OUT_REPORT_CSV = POLICY_DIR / 'osm_exclusion_report.csv'\n",
    "OUT_PILOT_CSV = POLICY_DIR / 'osm_candidates_for_pilot.csv'\n",
    "\n",
    "# Configuration\n",
    "USE_FAST_PREFILTER = True  # Text scan before JSON parse\n",
    "PILOT_MAX_PER_BUCKET = 10  # Limit candidates per org/theme\n",
    "\n",
    "# Create output directory\n",
    "POLICY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Input: {DATASET_DIR}\")\n",
    "print(f\"Output: {POLICY_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Detection Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Helper Functions\n",
    "\n",
    "Utilities for robust JSON access and data extraction.\n",
    "\"\"\"\n",
    "\n",
    "def read_text(path: Path) -> str:\n",
    "    \"\"\"Read text safely with UTF-8 encoding.\"\"\"\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "def load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load JSON file into dict.\"\"\"\n",
    "    return json.loads(read_text(path))\n",
    "\n",
    "\n",
    "def normalize_dataset_record(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize dataset record shape.\n",
    "    \n",
    "    Handles both direct HDX export and CKAN fallback wrapper.\n",
    "    \"\"\"\n",
    "    if isinstance(raw, dict) and \"id\" in raw:\n",
    "        return raw\n",
    "    if isinstance(raw, dict) and \"dataset\" in raw and isinstance(raw[\"dataset\"], dict):\n",
    "        return raw[\"dataset\"]\n",
    "    return raw\n",
    "\n",
    "\n",
    "def norm_str(x: Any) -> str:\n",
    "    \"\"\"Normalize value to lowercase stripped string.\"\"\"\n",
    "    return (x or \"\").__str__().strip().lower()\n",
    "\n",
    "\n",
    "def get_org_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract organization title/name.\"\"\"\n",
    "    org = ds.get(\"organization\")\n",
    "    if isinstance(org, dict):\n",
    "        return (org.get(\"title\") or org.get(\"name\") or \"\").strip()\n",
    "    return (org or \"\").strip()\n",
    "\n",
    "\n",
    "def get_tags(ds: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Extract tags as lowercase strings.\"\"\"\n",
    "    tags = ds.get(\"tags\") or []\n",
    "    out: List[str] = []\n",
    "    if isinstance(tags, list):\n",
    "        for t in tags:\n",
    "            if isinstance(t, dict):\n",
    "                name = t.get(\"name\") or \"\"\n",
    "                if name:\n",
    "                    out.append(name.strip().lower())\n",
    "            elif isinstance(t, str):\n",
    "                out.append(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_resources(ds: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract resources list.\"\"\"\n",
    "    res = ds.get(\"resources\") or []\n",
    "    return res if isinstance(res, list) else []\n",
    "\n",
    "\n",
    "def get_license_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract normalized license string.\"\"\"\n",
    "    lt = ds.get(\"license_title\") or ds.get(\"license_id\") or \"\"\n",
    "    return (lt or \"\").strip()\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 OSM Detection Rules\n",
    "\n",
    "Policy-based detection with traceable reasons.\n",
    "\"\"\"\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OSMDetectionResult:\n",
    "    \"\"\"\n",
    "    Result of OSM detection for a dataset.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    is_osm : bool\n",
    "        Whether dataset is OSM-derived\n",
    "    reasons : Tuple[str, ...]\n",
    "        Detection rule IDs that fired\n",
    "    signals : Dict[str, Any]\n",
    "        Evidence fields for auditing\n",
    "    \"\"\"\n",
    "    is_osm: bool\n",
    "    reasons: Tuple[str, ...]\n",
    "    signals: Dict[str, Any]\n",
    "\n",
    "\n",
    "# Fast prefilter markers\n",
    "FAST_MARKERS = (\n",
    "    \"openstreetmap contributors\",\n",
    "    '\"dataset_source\":',\n",
    "    '\"license_title\": \"odbl\"',\n",
    "    '\"license_title\":\"odbl\"',\n",
    "    \"open database license\",\n",
    "    \"hotosm\",\n",
    "    \"export.hotosm.org\",\n",
    "    \"openstreetmap.org\",\n",
    ")\n",
    "\n",
    "# URL markers for OSM/HOT\n",
    "OSM_URL_MARKERS = (\n",
    "    \"openstreetmap.org\",\n",
    "    \"hotosm.org\",\n",
    "    \"export.hotosm.org\",\n",
    "    \"exports-stage.hotosm.org\",\n",
    "    \"production-raw-data-api\",\n",
    ")\n",
    "\n",
    "# Organization markers\n",
    "OSM_ORG_MARKERS = (\n",
    "    \"humanitarian openstreetmap\",\n",
    "    \"hotosm\",\n",
    "    \"openstreetmap\",\n",
    ")\n",
    "\n",
    "# Title markers\n",
    "OSM_TITLE_MARKERS = (\n",
    "    \"openstreetmap export\",\n",
    "    \"(openstreetmap export)\",\n",
    "    \"openstreetmap\",\n",
    ")\n",
    "\n",
    "# Notes markers\n",
    "OSM_NOTES_MARKERS = (\n",
    "    \"openstreetmap\",\n",
    "    \"wiki.openstreetmap.org\",\n",
    "    \"osm\",\n",
    ")\n",
    "\n",
    "\n",
    "def prefilter_maybe_osm(text: str) -> bool:\n",
    "    \"\"\"Quick text scan for OSM indicators.\"\"\"\n",
    "    t = text.lower()\n",
    "    return any(m in t for m in FAST_MARKERS)\n",
    "\n",
    "\n",
    "def detect_osm(ds: Dict[str, Any]) -> OSMDetectionResult:\n",
    "    \"\"\"\n",
    "    Detect whether a dataset is derived from OpenStreetMap.\n",
    "    \n",
    "    Uses multiple signals with policy-based scoring:\n",
    "    1. dataset_source (strongest)\n",
    "    2. license evidence (ODbL) + OSM cues\n",
    "    3. resource URL evidence\n",
    "    4. organization/title/notes evidence\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : Dict[str, Any]\n",
    "        Dataset metadata dict\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    OSMDetectionResult\n",
    "        Detection result with reasons\n",
    "    \"\"\"\n",
    "    title = ds.get(\"title\") or \"\"\n",
    "    notes = ds.get(\"notes\") or \"\"\n",
    "    dataset_source = ds.get(\"dataset_source\") or \"\"\n",
    "    org_title = get_org_title(ds)\n",
    "    license_title = get_license_title(ds)\n",
    "    tags = get_tags(ds)\n",
    "    resources = get_resources(ds)\n",
    "    \n",
    "    title_l = norm_str(title)\n",
    "    notes_l = norm_str(notes)\n",
    "    dataset_source_l = norm_str(dataset_source)\n",
    "    org_l = norm_str(org_title)\n",
    "    license_l = norm_str(license_title)\n",
    "    \n",
    "    reasons: List[str] = []\n",
    "    \n",
    "    # Rule 1: dataset_source references OpenStreetMap\n",
    "    if \"openstreetmap\" in dataset_source_l:\n",
    "        reasons.append(\"dataset_source_mentions_openstreetmap\")\n",
    "    \n",
    "    # Rule 2: ODbL license with OSM cues\n",
    "    if license_l in {\"odbl\", \"odc-odbl\"} or \"odbl\" in license_l or \"open database license\" in license_l:\n",
    "        if (\"openstreetmap\" in title_l) or (\"openstreetmap\" in notes_l) or (\"openstreetmap\" in dataset_source_l):\n",
    "            reasons.append(\"odbl_license_plus_osm_cue\")\n",
    "    \n",
    "    # Rule 3: Resource URLs point to HOT/OSM\n",
    "    for r in resources:\n",
    "        url = norm_str(r.get(\"download_url\") or r.get(\"url\") or \"\")\n",
    "        if url and any(m in url for m in OSM_URL_MARKERS):\n",
    "            reasons.append(\"resource_url_osm_domain\")\n",
    "            break\n",
    "    \n",
    "    # Rule 4: Organization suggests OSM/HOT\n",
    "    if any(m in org_l for m in OSM_ORG_MARKERS):\n",
    "        reasons.append(\"organization_mentions_osm_or_hot\")\n",
    "    \n",
    "    # Rule 5: Title suggests OSM export\n",
    "    if any(m in title_l for m in OSM_TITLE_MARKERS):\n",
    "        reasons.append(\"title_mentions_osm_export\")\n",
    "    \n",
    "    # Rule 6: Tags include openstreetmap\n",
    "    if \"openstreetmap\" in tags:\n",
    "        reasons.append(\"tag_openstreetmap_present\")\n",
    "    \n",
    "    # Rule 7: Notes include OSM references\n",
    "    if any(m in notes_l for m in OSM_NOTES_MARKERS) and \"openstreetmap\" in notes_l:\n",
    "        reasons.append(\"notes_mentions_openstreetmap\")\n",
    "    \n",
    "    # Policy: mark OSM if any strong rules fire\n",
    "    strong = {\n",
    "        \"dataset_source_mentions_openstreetmap\",\n",
    "        \"resource_url_osm_domain\",\n",
    "        \"odbl_license_plus_osm_cue\",\n",
    "        \"tag_openstreetmap_present\",\n",
    "    }\n",
    "    is_osm = any(r in strong for r in reasons)\n",
    "    \n",
    "    # Allow supporting evidence to upgrade borderline cases\n",
    "    supporting = set(reasons) - strong\n",
    "    if not is_osm and len(supporting) >= 2:\n",
    "        is_osm = True\n",
    "        reasons.append(\"supporting_evidence_threshold_met\")\n",
    "    \n",
    "    signals = {\n",
    "        \"dataset_source\": dataset_source,\n",
    "        \"license_title\": license_title,\n",
    "        \"organization\": org_title,\n",
    "        \"tags\": tags,\n",
    "        \"resource_url_sample\": (resources[0].get(\"download_url\") if resources else None),\n",
    "    }\n",
    "    \n",
    "    return OSMDetectionResult(\n",
    "        is_osm=is_osm,\n",
    "        reasons=tuple(sorted(set(reasons))),\n",
    "        signals=signals\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"OSM detection rules defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Scan Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Scan All Dataset Files\n",
    "\n",
    "Process all JSON files and detect OSM datasets.\n",
    "\"\"\"\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in folder, sorted for determinism.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Input folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "\n",
    "def scan_folder_for_osm(input_dir: Path) -> Tuple[List[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Scan folder for OSM-derived datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : Path\n",
    "        Folder containing dataset JSON files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[Dict], List[str]]\n",
    "        (report_rows, excluded_ids)\n",
    "    \"\"\"\n",
    "    report_rows: List[Dict[str, Any]] = []\n",
    "    excluded_ids: List[str] = []\n",
    "    \n",
    "    files = list(iter_json_files(input_dir))\n",
    "    total = len(files)\n",
    "    \n",
    "    print(f\"Scanning {total:,} JSON files in: {input_dir}\")\n",
    "    \n",
    "    # Use tqdm if available\n",
    "    iterator = tqdm(files, desc=\"Scanning for OSM\") if HAS_TQDM else files\n",
    "    \n",
    "    for i, path in enumerate(iterator, start=1):\n",
    "        # Progress for non-tqdm\n",
    "        if not HAS_TQDM and i % 2000 == 0:\n",
    "            print(f\"  Processed {i:,}/{total:,}\")\n",
    "        \n",
    "        try:\n",
    "            txt = read_text(path) if USE_FAST_PREFILTER else \"\"\n",
    "            if USE_FAST_PREFILTER and not prefilter_maybe_osm(txt):\n",
    "                continue\n",
    "            \n",
    "            raw = json.loads(txt) if USE_FAST_PREFILTER else load_json(path)\n",
    "            ds = normalize_dataset_record(raw)\n",
    "            \n",
    "            ds_id = ds.get(\"id\") or \"\"\n",
    "            title = ds.get(\"title\") or \"\"\n",
    "            name = ds.get(\"name\") or \"\"\n",
    "            org = get_org_title(ds)\n",
    "            \n",
    "            result = detect_osm(ds)\n",
    "            \n",
    "            if result.is_osm:\n",
    "                excluded_ids.append(ds_id)\n",
    "                \n",
    "                report_rows.append({\n",
    "                    \"dataset_id\": ds_id,\n",
    "                    \"name\": name,\n",
    "                    \"title\": title,\n",
    "                    \"organization\": org,\n",
    "                    \"dataset_source\": ds.get(\"dataset_source\"),\n",
    "                    \"license_title\": get_license_title(ds),\n",
    "                    \"reasons\": \";\".join(result.reasons),\n",
    "                    \"tags\": \";\".join(get_tags(ds)),\n",
    "                    \"n_resources\": len(get_resources(ds)),\n",
    "                    \"file\": str(path),\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            report_rows.append({\n",
    "                \"dataset_id\": \"\",\n",
    "                \"name\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"organization\": \"\",\n",
    "                \"dataset_source\": \"\",\n",
    "                \"license_title\": \"\",\n",
    "                \"reasons\": f\"ERROR:{type(e).__name__}:{e}\",\n",
    "                \"tags\": \"\",\n",
    "                \"n_resources\": \"\",\n",
    "                \"file\": str(path),\n",
    "            })\n",
    "    \n",
    "    # Deduplicate IDs\n",
    "    excluded_ids = sorted(set([x for x in excluded_ids if x]))\n",
    "    \n",
    "    return report_rows, excluded_ids\n",
    "\n",
    "\n",
    "# Run scan\n",
    "report_rows, excluded_ids = scan_folder_for_osm(DATASET_DIR)\n",
    "\n",
    "print(f\"\\nFlagged OSM-derived datasets: {len(excluded_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Write Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Save Exclusion List and Report\n",
    "\"\"\"\n",
    "\n",
    "def write_ids_txt(path: Path, ids: Sequence[str]) -> None:\n",
    "    \"\"\"Write IDs to text file, one per line.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for x in ids:\n",
    "            f.write(f\"{x}\\n\")\n",
    "\n",
    "\n",
    "def write_report_csv(path: Path, rows: Sequence[Dict[str, Any]]) -> None:\n",
    "    \"\"\"Write report to CSV file.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    header = [\n",
    "        \"dataset_id\", \"name\", \"title\", \"organization\",\n",
    "        \"dataset_source\", \"license_title\", \"reasons\",\n",
    "        \"tags\", \"n_resources\", \"file\"\n",
    "    ]\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in header})\n",
    "\n",
    "\n",
    "# Write outputs\n",
    "write_ids_txt(OUT_IDS_TXT, excluded_ids)\n",
    "write_report_csv(OUT_REPORT_CSV, report_rows)\n",
    "\n",
    "print(f\"Wrote: {OUT_IDS_TXT}\")\n",
    "print(f\"Wrote: {OUT_REPORT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 Create Pilot Shortlist (Optional)\n",
    "\n",
    "Small sample for future OSM pilot experiments.\n",
    "\"\"\"\n",
    "\n",
    "def derive_theme(tags: List[str]) -> str:\n",
    "    \"\"\"Infer theme category from tags.\"\"\"\n",
    "    if any(t in tags for t in (\"roads\", \"railways\", \"transportation\", \"aviation\")):\n",
    "        return \"transport\"\n",
    "    if any(t in tags for t in (\"health facilities\", \"health\")):\n",
    "        return \"health_facilities\"\n",
    "    if any(t in tags for t in (\"waterways\", \"rivers\", \"hydrology\")):\n",
    "        return \"hydrology\"\n",
    "    if any(t in tags for t in (\"administrative boundaries-divisions\", \"gazetteer\")):\n",
    "        return \"boundaries_gazetteer\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def make_pilot_shortlist(rows: List[Dict[str, Any]], max_per_bucket: int = 10) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create small pilot shortlist grouped by org/theme.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rows : List[Dict]\n",
    "        Report rows\n",
    "    max_per_bucket : int\n",
    "        Maximum samples per org/theme combination\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict]\n",
    "        Pilot shortlist\n",
    "    \"\"\"\n",
    "    clean = [r for r in rows if r.get(\"dataset_id\")]\n",
    "    \n",
    "    for r in clean:\n",
    "        tags = (r.get(\"tags\") or \"\").split(\";\") if r.get(\"tags\") else []\n",
    "        tags = [t.strip().lower() for t in tags if t.strip()]\n",
    "        r[\"theme\"] = derive_theme(tags)\n",
    "    \n",
    "    # Group by (org, theme)\n",
    "    buckets: Dict[Tuple[str, str], List[Dict[str, Any]]] = {}\n",
    "    for r in clean:\n",
    "        key = ((r.get(\"organization\") or \"unknown\").strip(), (r.get(\"theme\") or \"other\").strip())\n",
    "        buckets.setdefault(key, []).append(r)\n",
    "    \n",
    "    pilot: List[Dict[str, Any]] = []\n",
    "    for (org, theme), items in sorted(buckets.items(), key=lambda x: (-len(x[1]), x[0])):\n",
    "        items_sorted = sorted(items, key=lambda r: (r.get(\"title\") or \"\"))\n",
    "        pilot.extend(items_sorted[:max_per_bucket])\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"dataset_id\": r[\"dataset_id\"],\n",
    "            \"title\": r[\"title\"],\n",
    "            \"organization\": r[\"organization\"],\n",
    "            \"theme\": r[\"theme\"],\n",
    "            \"reasons\": r[\"reasons\"],\n",
    "        }\n",
    "        for r in pilot\n",
    "    ]\n",
    "\n",
    "\n",
    "pilot_rows = make_pilot_shortlist(report_rows, max_per_bucket=PILOT_MAX_PER_BUCKET)\n",
    "\n",
    "if pilot_rows:\n",
    "    with OUT_PILOT_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"dataset_id\", \"title\", \"organization\", \"theme\", \"reasons\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(pilot_rows)\n",
    "    print(f\"Wrote: {OUT_PILOT_CSV} ({len(pilot_rows)} rows)\")\n",
    "else:\n",
    "    print(\"No pilot rows produced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Display Summary\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OSM EXCLUSION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total flagged: {len(excluded_ids):,}\")\n",
    "print(f\"Pilot candidates: {len(pilot_rows):,}\")\n",
    "\n",
    "print(f\"\\nExample excluded IDs (first 10):\")\n",
    "for ds_id in excluded_ids[:10]:\n",
    "    print(f\"  - {ds_id}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - {OUT_IDS_TXT}\")\n",
    "print(f\"  - {OUT_REPORT_CSV}\")\n",
    "print(f\"  - {OUT_PILOT_CSV}\")\n",
    "\n",
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
