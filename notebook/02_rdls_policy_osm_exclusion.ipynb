{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6effa2",
   "metadata": {},
   "source": [
    "# Step 2 - OSM Policy Exclusion Index (HDX → RDLS Pipeline)\n",
    "\n",
    "## Purpose\n",
    "This notebook scans **HDX dataset-level metadata JSON files** (downloaded in Step 1) and flags datasets that are\n",
    "**derived from OpenStreetMap (OSM)** (including HOT exports and non-HOT OSM-derived datasets like Healthsites).\n",
    "\n",
    "Your team policy is to **exclude OSM-derived datasets** from RDLS conversion for now (to avoid producing a large number\n",
    "of near-duplicate metadata records whose upstream source is OSM). This notebook:\n",
    "\n",
    "- Detects OSM-derived datasets using multiple strong signals (not just tags)\n",
    "- Produces a reproducible **exclusion list** (dataset UUIDs)\n",
    "- Produces an auditable **exclusion report** (CSV) with detection reasons\n",
    "- Optionally prepares a **pilot candidate list** for controlled OSM experiments later\n",
    "\n",
    "## Inputs\n",
    "- Folder of dataset-level metadata JSON files, e.g.:\n",
    "  - `data/raw/hdx_dataset_metadata/*.json`\n",
    "\n",
    "The notebook is compatible with:\n",
    "- HDX `download_metadata?format=json` output (fields like `dataset_source`, `license_title`, `resources[*].download_url`)\n",
    "- CKAN `package_show` output (fallback shape), if your Step 1 stored that format (tags/resources as dicts)\n",
    "\n",
    "## Outputs\n",
    "- `data/policy/osm_excluded_dataset_ids.txt` — one dataset UUID per line\n",
    "- `data/policy/osm_exclusion_report.csv` — dataset_id, title, org, key signals, and detection reasons\n",
    "- `data/policy/osm_candidates_for_pilot.csv` (optional) — a deduplicated shortlist for future OSM pilot work\n",
    "\n",
    "## Notes on detection approach\n",
    "OSM/HOT datasets are detected using a *rule-based* approach with traceable reasons, prioritizing strong evidence such as:\n",
    "- `dataset_source` contains “OpenStreetMap contributors”\n",
    "- license indicates ODbL together with OSM textual cues\n",
    "- organization or title indicates HOT/OSM exports\n",
    "- resource URLs include `hotosm.org` / `export.hotosm.org` / `openstreetmap.org`\n",
    "\n",
    "This yields high precision for policy exclusion while keeping the logic transparent and reviewable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f7aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OSM Policy Exclusion Index (HDX dataset-level metadata)\n",
    "\n",
    "This module (used within the notebook) scans HDX dataset-level metadata JSON files and produces:\n",
    "1) A text file of dataset IDs to exclude (OSM-derived datasets)\n",
    "2) A CSV report explaining why each dataset was flagged\n",
    "\n",
    "Design principles:\n",
    "- Deterministic, transparent rules (no ML black box)\n",
    "- Strong-signal detection (dataset_source/license/resource URLs)\n",
    "- Auditability: each flagged dataset includes a list of reasons\n",
    "- Minimal dependencies: standard library only\n",
    "\n",
    "Author: <YOUR NAME/ORG>\n",
    "License: <YOUR LICENSE>\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831f8c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\dataset_metadata\n",
      "OUTPUT_DIR: C:\\Users\\benny\\OneDrive\\Documents\\Github\\hdx-metadata-crawler\\hdx_dataset_metadata_dump\\policy\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Configuration (EDIT HERE)\n",
    "# =========================\n",
    "\n",
    "# Folder containing HDX dataset-level metadata JSON files (from Step 1).\n",
    "# Example:\n",
    "#   INPUT_DIR = Path(\"data/raw/hdx_dataset_metadata\")\n",
    "INPUT_DIR = Path(\"../hdx_dataset_metadata_dump/dataset_metadata\")\n",
    "\n",
    "# Output folder for policy artifacts.\n",
    "OUTPUT_DIR = Path(\"../hdx_dataset_metadata_dump/policy\")\n",
    "\n",
    "# Output files.\n",
    "OUT_IDS_TXT = OUTPUT_DIR / \"osm_excluded_dataset_ids.txt\"\n",
    "OUT_REPORT_CSV = OUTPUT_DIR / \"osm_exclusion_report.csv\"\n",
    "OUT_PILOT_CSV = OUTPUT_DIR / \"osm_candidates_for_pilot.csv\"  # optional\n",
    "\n",
    "# If True, prefilter files by scanning text for OSM markers before parsing JSON.\n",
    "# This is faster when you have many files.\n",
    "USE_FAST_PREFILTER = True\n",
    "\n",
    "# When generating the pilot file, limit candidates per (org/theme) bucket to keep it small.\n",
    "PILOT_MAX_PER_BUCKET = 10\n",
    "\n",
    "# Create outputs folder\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"INPUT_DIR:\", INPUT_DIR.resolve())\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13807924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers: robust JSON access\n",
    "# =========================\n",
    "\n",
    "def read_text(path: Path) -> str:\n",
    "    \"\"\"Read text safely (UTF-8), ignoring decoding errors.\"\"\"\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def load_json(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load a JSON file into a dict.\"\"\"\n",
    "    return json.loads(read_text(path))\n",
    "\n",
    "def normalize_dataset_record(raw: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize dataset record shape.\n",
    "\n",
    "    Step 1 may store:\n",
    "    - HDX dataset export JSON directly (has 'id' at top-level)\n",
    "    - CKAN package_show fallback wrapped as {'dataset': {...}} (id inside 'dataset')\n",
    "\n",
    "    This function returns the dataset dict that contains canonical fields like 'id', 'title', etc.\n",
    "    \"\"\"\n",
    "    if isinstance(raw, dict) and \"id\" in raw:\n",
    "        return raw\n",
    "    if isinstance(raw, dict) and \"dataset\" in raw and isinstance(raw[\"dataset\"], dict):\n",
    "        return raw[\"dataset\"]\n",
    "    return raw\n",
    "\n",
    "def norm_str(x: Any) -> str:\n",
    "    \"\"\"Normalize any value to a lowercase stripped string.\"\"\"\n",
    "    return (x or \"\").__str__().strip().lower()\n",
    "\n",
    "def get_org_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Get organization title/name across HDX export and CKAN shapes.\"\"\"\n",
    "    org = ds.get(\"organization\")\n",
    "    if isinstance(org, dict):\n",
    "        return (org.get(\"title\") or org.get(\"name\") or \"\").strip()\n",
    "    return (org or \"\").strip()\n",
    "\n",
    "def get_tags(ds: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"Return tags as a list of lowercase strings.\"\"\"\n",
    "    tags = ds.get(\"tags\") or []\n",
    "    out: List[str] = []\n",
    "    if isinstance(tags, list):\n",
    "        for t in tags:\n",
    "            if isinstance(t, dict):\n",
    "                name = t.get(\"name\") or \"\"\n",
    "                if name:\n",
    "                    out.append(name.strip().lower())\n",
    "            elif isinstance(t, str):\n",
    "                out.append(t.strip().lower())\n",
    "    return out\n",
    "\n",
    "def get_resources(ds: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Return resources list (list of dicts).\"\"\"\n",
    "    res = ds.get(\"resources\") or []\n",
    "    return res if isinstance(res, list) else []\n",
    "\n",
    "def get_license_title(ds: Dict[str, Any]) -> str:\n",
    "    \"\"\"Return a normalized license string.\"\"\"\n",
    "    # HDX export uses license_title, CKAN may use license_title and/or license_id.\n",
    "    lt = ds.get(\"license_title\") or ds.get(\"license_id\") or \"\"\n",
    "    return (lt or \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f6c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# OSM detection rules (policy signals)\n",
    "# ===================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OSMDetectionResult:\n",
    "    is_osm: bool\n",
    "    reasons: Tuple[str, ...]\n",
    "    signals: Dict[str, Any]\n",
    "\n",
    "FAST_MARKERS = (\n",
    "    \"openstreetmap contributors\",\n",
    "    '\"dataset_source\":',\n",
    "    '\"dataset_source\":\"',\n",
    "    '\"license_title\": \"odbl\"',\n",
    "    '\"license_title\":\"odbl\"',\n",
    "    \"open database license\",\n",
    "    \"hotosm\",\n",
    "    \"export.hotosm.org\",\n",
    "    \"exports-stage.hotosm.org\",\n",
    "    \"openstreetmap.org\",\n",
    ")\n",
    "\n",
    "# URL/domain markers for OSM/HOT.\n",
    "OSM_URL_MARKERS = (\n",
    "    \"openstreetmap.org\",\n",
    "    \"hotosm.org\",\n",
    "    \"export.hotosm.org\",\n",
    "    \"exports-stage.hotosm.org\",\n",
    "    \"production-raw-data-api\",\n",
    ")\n",
    "\n",
    "# Organization/title markers (used as supporting evidence).\n",
    "OSM_ORG_MARKERS = (\n",
    "    \"humanitarian openstreetmap\",\n",
    "    \"hotosm\",\n",
    "    \"openstreetmap\",\n",
    ")\n",
    "\n",
    "OSM_TITLE_MARKERS = (\n",
    "    \"openstreetmap export\",\n",
    "    \"(openstreetmap export)\",\n",
    "    \"openstreetmap\",\n",
    ")\n",
    "\n",
    "# Notes markers (supporting evidence).\n",
    "OSM_NOTES_MARKERS = (\n",
    "    \"openstreetmap\",\n",
    "    \"wiki.openstreetmap.org\",\n",
    "    \"osm\",\n",
    ")\n",
    "\n",
    "def prefilter_maybe_osm(text: str) -> bool:\n",
    "    \"\"\"Quick text scan: returns True if file likely contains OSM indicators.\"\"\"\n",
    "    t = text.lower()\n",
    "    return any(m in t for m in FAST_MARKERS)\n",
    "\n",
    "def detect_osm(ds: Dict[str, Any]) -> OSMDetectionResult:\n",
    "    \"\"\"\n",
    "    Detect whether a dataset record is derived from OpenStreetMap.\n",
    "\n",
    "    The rules are designed for *policy exclusion* (high precision), using:\n",
    "    1) dataset_source (strongest)\n",
    "    2) license evidence (ODbL) + OSM textual cues\n",
    "    3) resource URL evidence\n",
    "    4) organization/title/notes evidence\n",
    "\n",
    "    Returns an OSMDetectionResult with:\n",
    "    - is_osm: bool\n",
    "    - reasons: tuple of fired rule IDs (human-readable)\n",
    "    - signals: selected fields for auditing\n",
    "    \"\"\"\n",
    "    title = ds.get(\"title\") or \"\"\n",
    "    name = ds.get(\"name\") or \"\"\n",
    "    notes = ds.get(\"notes\") or \"\"\n",
    "    dataset_source = ds.get(\"dataset_source\") or \"\"\n",
    "    org_title = get_org_title(ds)\n",
    "    license_title = get_license_title(ds)\n",
    "    tags = get_tags(ds)\n",
    "    resources = get_resources(ds)\n",
    "\n",
    "    title_l = norm_str(title)\n",
    "    notes_l = norm_str(notes)\n",
    "    dataset_source_l = norm_str(dataset_source)\n",
    "    org_l = norm_str(org_title)\n",
    "    license_l = norm_str(license_title)\n",
    "\n",
    "    reasons: List[str] = []\n",
    "\n",
    "    # Rule 1: dataset_source explicitly references OpenStreetMap contributors.\n",
    "    if \"openstreetmap\" in dataset_source_l:\n",
    "        reasons.append(\"dataset_source_mentions_openstreetmap\")\n",
    "\n",
    "    # Rule 2: ODbL license with OSM cues (covers cases where dataset_source is absent).\n",
    "    # Your example has license_title == 'ODbL'. This alone is not always sufficient,\n",
    "    # but combined with textual cues it is a strong indicator of OSM-derived data.\n",
    "    if license_l in {\"odbl\", \"odc-odbl\"} or \"odbl\" in license_l or \"open database license\" in license_l:\n",
    "        if (\"openstreetmap\" in title_l) or (\"openstreetmap\" in notes_l) or (\"openstreetmap\" in dataset_source_l):\n",
    "            reasons.append(\"odbl_license_plus_osm_cue\")\n",
    "\n",
    "    # Rule 3: Resource URLs point to HOT/OSM infrastructure.\n",
    "    for r in resources:\n",
    "        url = norm_str(r.get(\"download_url\") or r.get(\"url\") or \"\")\n",
    "        if url and any(m in url for m in OSM_URL_MARKERS):\n",
    "            reasons.append(\"resource_url_osm_domain\")\n",
    "            break\n",
    "\n",
    "    # Rule 4: Organization suggests OSM/HOT.\n",
    "    if any(m in org_l for m in OSM_ORG_MARKERS):\n",
    "        reasons.append(\"organization_mentions_osm_or_hot\")\n",
    "\n",
    "    # Rule 5: Title suggests OSM export.\n",
    "    if any(m in title_l for m in OSM_TITLE_MARKERS):\n",
    "        reasons.append(\"title_mentions_osm_export\")\n",
    "\n",
    "    # Rule 6: Tags include openstreetmap (not always present, but strong when present).\n",
    "    if \"openstreetmap\" in tags:\n",
    "        reasons.append(\"tag_openstreetmap_present\")\n",
    "\n",
    "    # Rule 7: Notes include OSM references (weak; keep only as supporting evidence).\n",
    "    if any(m in notes_l for m in OSM_NOTES_MARKERS) and \"openstreetmap\" in notes_l:\n",
    "        reasons.append(\"notes_mentions_openstreetmap\")\n",
    "\n",
    "    # Policy decision: mark OSM if any of the strong rules fire.\n",
    "    # Strong rules: dataset_source, resource_url domain, ODbL+cue.\n",
    "    strong = {\n",
    "        \"dataset_source_mentions_openstreetmap\",\n",
    "        \"resource_url_osm_domain\",\n",
    "        \"odbl_license_plus_osm_cue\",\n",
    "        \"tag_openstreetmap_present\",\n",
    "    }\n",
    "    is_osm = any(r in strong for r in reasons)\n",
    "\n",
    "    # Allow HOT/org/title evidence to upgrade borderline cases:\n",
    "    # If we have 2+ supporting rules, treat as OSM (helps when dataset_source missing).\n",
    "    supporting = set(reasons) - strong\n",
    "    if not is_osm and len(supporting) >= 2:\n",
    "        is_osm = True\n",
    "        reasons.append(\"supporting_evidence_threshold_met\")\n",
    "\n",
    "    signals = {\n",
    "        \"dataset_source\": dataset_source,\n",
    "        \"license_title\": license_title,\n",
    "        \"organization\": org_title,\n",
    "        \"tags\": tags,\n",
    "        \"resource_url_sample\": (resources[0].get(\"download_url\") if resources else None),\n",
    "    }\n",
    "\n",
    "    return OSMDetectionResult(is_osm=is_osm, reasons=tuple(sorted(set(reasons))), signals=signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808b838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 26,246 JSON files in: ..\\hdx_dataset_metadata_dump\\dataset_metadata\n",
      "  processed 500/26,246\n",
      "  processed 1,000/26,246\n",
      "  processed 1,500/26,246\n",
      "  processed 2,000/26,246\n",
      "  processed 2,500/26,246\n",
      "  processed 3,000/26,246\n",
      "  processed 3,500/26,246\n",
      "  processed 4,000/26,246\n",
      "  processed 4,500/26,246\n",
      "  processed 5,000/26,246\n",
      "  processed 5,500/26,246\n",
      "  processed 6,000/26,246\n",
      "  processed 6,500/26,246\n",
      "  processed 7,000/26,246\n",
      "  processed 7,500/26,246\n",
      "  processed 8,000/26,246\n",
      "  processed 8,500/26,246\n",
      "  processed 9,000/26,246\n",
      "  processed 9,500/26,246\n",
      "  processed 10,000/26,246\n",
      "  processed 10,500/26,246\n",
      "  processed 11,000/26,246\n",
      "  processed 11,500/26,246\n",
      "  processed 12,000/26,246\n",
      "  processed 12,500/26,246\n",
      "  processed 13,000/26,246\n",
      "  processed 13,500/26,246\n",
      "  processed 14,000/26,246\n",
      "  processed 14,500/26,246\n",
      "  processed 15,000/26,246\n",
      "  processed 15,500/26,246\n",
      "  processed 16,000/26,246\n",
      "  processed 16,500/26,246\n",
      "  processed 17,000/26,246\n",
      "  processed 17,500/26,246\n",
      "  processed 18,000/26,246\n",
      "  processed 18,500/26,246\n",
      "  processed 19,000/26,246\n",
      "  processed 19,500/26,246\n",
      "  processed 20,000/26,246\n",
      "  processed 20,500/26,246\n",
      "  processed 21,000/26,246\n",
      "  processed 21,500/26,246\n",
      "  processed 22,000/26,246\n",
      "  processed 22,500/26,246\n",
      "  processed 23,000/26,246\n",
      "  processed 23,500/26,246\n",
      "  processed 24,000/26,246\n",
      "  processed 24,500/26,246\n",
      "  processed 25,000/26,246\n",
      "  processed 25,500/26,246\n",
      "  processed 26,000/26,246\n",
      "  processed 26,246/26,246\n",
      "Flagged OSM-derived datasets: 3,649\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Scan dataset JSON folder and write outputs\n",
    "# ==========================================\n",
    "\n",
    "def iter_json_files(folder: Path) -> Iterable[Path]:\n",
    "    \"\"\"Yield JSON files in a folder (non-recursive).\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Input folder not found: {folder}\")\n",
    "    yield from sorted(folder.glob(\"*.json\"))\n",
    "\n",
    "def scan_folder_for_osm(input_dir: Path) -> Tuple[List[Dict[str, Any]], List[str]]:\n",
    "    \"\"\"\n",
    "    Scan a folder of dataset-level metadata JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - report_rows: list of dict rows for CSV\n",
    "    - excluded_ids: list of dataset UUID strings\n",
    "    \"\"\"\n",
    "    report_rows: List[Dict[str, Any]] = []\n",
    "    excluded_ids: List[str] = []\n",
    "\n",
    "    files = list(iter_json_files(input_dir))\n",
    "    total = len(files)\n",
    "    print(f\"Scanning {total:,} JSON files in: {input_dir}\")\n",
    "\n",
    "    for i, path in enumerate(files, start=1):\n",
    "        # Minimal progress indicator (avoids external deps).\n",
    "        if i % 500 == 0 or i == total:\n",
    "            print(f\"  processed {i:,}/{total:,}\")\n",
    "\n",
    "        try:\n",
    "            txt = read_text(path) if USE_FAST_PREFILTER else \"\"\n",
    "            if USE_FAST_PREFILTER and not prefilter_maybe_osm(txt):\n",
    "                continue\n",
    "\n",
    "            raw = json.loads(txt) if USE_FAST_PREFILTER else load_json(path)\n",
    "            ds = normalize_dataset_record(raw)\n",
    "\n",
    "            ds_id = ds.get(\"id\") or \"\"\n",
    "            title = ds.get(\"title\") or \"\"\n",
    "            name = ds.get(\"name\") or \"\"\n",
    "            org = get_org_title(ds)\n",
    "\n",
    "            result = detect_osm(ds)\n",
    "\n",
    "            if result.is_osm:\n",
    "                excluded_ids.append(ds_id)\n",
    "\n",
    "                report_rows.append({\n",
    "                    \"dataset_id\": ds_id,\n",
    "                    \"name\": name,\n",
    "                    \"title\": title,\n",
    "                    \"organization\": org,\n",
    "                    \"dataset_source\": ds.get(\"dataset_source\"),\n",
    "                    \"license_title\": get_license_title(ds),\n",
    "                    \"reasons\": \";\".join(result.reasons),\n",
    "                    \"tags\": \";\".join(get_tags(ds)),\n",
    "                    \"n_resources\": len(get_resources(ds)),\n",
    "                    \"file\": str(path),\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Keep scanning; errors can be audited separately if needed.\n",
    "            report_rows.append({\n",
    "                \"dataset_id\": \"\",\n",
    "                \"name\": \"\",\n",
    "                \"title\": \"\",\n",
    "                \"organization\": \"\",\n",
    "                \"dataset_source\": \"\",\n",
    "                \"license_title\": \"\",\n",
    "                \"reasons\": f\"ERROR:{type(e).__name__}:{e}\",\n",
    "                \"tags\": \"\",\n",
    "                \"n_resources\": \"\",\n",
    "                \"file\": str(path),\n",
    "            })\n",
    "\n",
    "    # De-duplicate IDs (stable ordering)\n",
    "    excluded_ids = sorted(set([x for x in excluded_ids if x]))\n",
    "\n",
    "    return report_rows, excluded_ids\n",
    "\n",
    "report_rows, excluded_ids = scan_folder_for_osm(INPUT_DIR)\n",
    "\n",
    "print(f\"Flagged OSM-derived datasets: {len(excluded_ids):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8eed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ..\\hdx_dataset_metadata_dump\\policy\\osm_excluded_dataset_ids.txt\n",
      "Wrote: ..\\hdx_dataset_metadata_dump\\policy\\osm_exclusion_report.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Write outputs (CSV + TXT)\n",
    "# ==========================\n",
    "\n",
    "def write_ids_txt(path: Path, ids: Sequence[str]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for x in ids:\n",
    "            f.write(f\"{x}\\n\")\n",
    "\n",
    "def write_report_csv(path: Path, rows: Sequence[Dict[str, Any]]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Fixed header for consistent downstream usage\n",
    "    header = [\n",
    "        \"dataset_id\", \"name\", \"title\", \"organization\",\n",
    "        \"dataset_source\", \"license_title\", \"reasons\",\n",
    "        \"tags\", \"n_resources\", \"file\"\n",
    "    ]\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow({k: r.get(k, \"\") for k in header})\n",
    "\n",
    "write_ids_txt(OUT_IDS_TXT, excluded_ids)\n",
    "write_report_csv(OUT_REPORT_CSV, report_rows)\n",
    "\n",
    "print(\"Wrote:\", OUT_IDS_TXT)\n",
    "print(\"Wrote:\", OUT_REPORT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d3c644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ..\\hdx_dataset_metadata_dump\\policy\\osm_candidates_for_pilot.csv (rows=138)\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Optional: create a pilot shortlist\n",
    "# ==================================\n",
    "# The pilot shortlist is useful if you later want to test a controlled strategy for OSM within RDLS,\n",
    "# without generating RDLS metadata for every OSM-derived dataset on HDX.\n",
    "#\n",
    "# Strategy:\n",
    "# - Group by organization (and optionally by tag themes)\n",
    "# - Keep only a small number of examples per group\n",
    "\n",
    "def derive_theme(tags: List[str]) -> str:\n",
    "    \"\"\"Simple theme heuristic for pilot grouping.\"\"\"\n",
    "    if any(t in tags for t in (\"roads\", \"railways\", \"transportation\", \"aviation\")):\n",
    "        return \"transport\"\n",
    "    if any(t in tags for t in (\"health facilities\", \"health\")):\n",
    "        return \"health_facilities\"\n",
    "    if any(t in tags for t in (\"waterways\", \"rivers\", \"hydrology\")):\n",
    "        return \"hydrology\"\n",
    "    if any(t in tags for t in (\"administrative boundaries-divisions\", \"gazetteer\")):\n",
    "        return \"boundaries_gazetteer\"\n",
    "    return \"other\"\n",
    "\n",
    "def make_pilot_shortlist(rows: List[Dict[str, Any]], max_per_bucket: int = 10) -> List[Dict[str, Any]]:\n",
    "    # Keep only datasets with valid IDs (exclude error rows)\n",
    "    clean = [r for r in rows if r.get(\"dataset_id\")]\n",
    "    # Add theme column\n",
    "    for r in clean:\n",
    "        tags = (r.get(\"tags\") or \"\").split(\";\") if r.get(\"tags\") else []\n",
    "        tags = [t.strip().lower() for t in tags if t.strip()]\n",
    "        r[\"theme\"] = derive_theme(tags)\n",
    "\n",
    "    # Bucket by (organization, theme)\n",
    "    buckets: Dict[Tuple[str, str], List[Dict[str, Any]]] = {}\n",
    "    for r in clean:\n",
    "        key = ((r.get(\"organization\") or \"unknown\").strip(), (r.get(\"theme\") or \"other\").strip())\n",
    "        buckets.setdefault(key, []).append(r)\n",
    "\n",
    "    pilot: List[Dict[str, Any]] = []\n",
    "    for (org, theme), items in sorted(buckets.items(), key=lambda x: (-len(x[1]), x[0])):\n",
    "        # stable order by title\n",
    "        items_sorted = sorted(items, key=lambda r: (r.get(\"title\") or \"\"))\n",
    "        pilot.extend(items_sorted[:max_per_bucket])\n",
    "\n",
    "    # minimal header for pilot\n",
    "    out = []\n",
    "    for r in pilot:\n",
    "        out.append({\n",
    "            \"dataset_id\": r[\"dataset_id\"],\n",
    "            \"title\": r[\"title\"],\n",
    "            \"organization\": r[\"organization\"],\n",
    "            \"theme\": r[\"theme\"],\n",
    "            \"reasons\": r[\"reasons\"],\n",
    "        })\n",
    "    return out\n",
    "\n",
    "pilot_rows = make_pilot_shortlist(report_rows, max_per_bucket=PILOT_MAX_PER_BUCKET)\n",
    "\n",
    "# Write pilot CSV\n",
    "if pilot_rows:\n",
    "    with OUT_PILOT_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"dataset_id\", \"title\", \"organization\", \"theme\", \"reasons\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(pilot_rows)\n",
    "    print(\"Wrote:\", OUT_PILOT_CSV, f\"(rows={len(pilot_rows)})\")\n",
    "else:\n",
    "    print(\"No pilot rows produced (no OSM datasets detected or all rows were errors).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "246d5127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example excluded IDs (first 10):\n",
      "003b676c-3e72-4ab2-a33b-2d3b9f9d7857\n",
      "004790b4-4ddd-4d7e-9288-73ae46e643a3\n",
      "0048d3d1-50eb-4428-a506-23882f4ce7ac\n",
      "00503aef-1f17-44fd-8664-679c5ad6e05c\n",
      "00a377de-920d-43c4-8fa1-590e2b369dcf\n",
      "00ad2859-55c5-4e91-bbe6-d79d3fde2dc2\n",
      "00b5e80c-1e32-4603-ae37-04b54655389c\n",
      "00d7616a-c748-41c1-ae80-e65669293924\n",
      "00e423c7-70dd-4a0c-ace7-8996e782a99e\n",
      "00e5fec7-756d-4b1d-8816-21fb89320e48\n",
      "\n",
      "If you want to spot-check a few exclusions, open:\n",
      " - ..\\hdx_dataset_metadata_dump\\policy\\osm_exclusion_report.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Quick sanity-check outputs\n",
    "# ==========================\n",
    "\n",
    "print(\"Example excluded IDs (first 10):\")\n",
    "print(\"\\n\".join(excluded_ids[:10]))\n",
    "\n",
    "print(\"\\nIf you want to spot-check a few exclusions, open:\")\n",
    "print(\" -\", OUT_REPORT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda3b17-f4b0-46ac-ac4a-2b847e6077aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
