{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: RDLS Exposure Block Extractor\n",
    "\n",
    "**Purpose**: Extract and populate RDLS v0.3 Exposure component blocks from HDX metadata.\n",
    "\n",
    "**Input**:\n",
    "- HDX dataset metadata JSON files\n",
    "- Signal Dictionary (`config/signal_dictionary.yaml`)\n",
    "- RDLS Schema (`rdls/schema/rdls_schema_v0.3.json`)\n",
    "\n",
    "**Output**:\n",
    "- Exposure block extractions with confidence scores\n",
    "- Updated RDLS records with populated exposure blocks\n",
    "\n",
    "**RDLS Exposure Block Structure**:\n",
    "```json\n",
    "\"exposure\": [\n",
    "  {\n",
    "    \"id\": \"...\",\n",
    "    \"category\": \"buildings|infrastructure|population|agriculture|natural_environment\",\n",
    "    \"taxonomy\": \"GED4ALL|...\",\n",
    "    \"metrics\": [\n",
    "      {\n",
    "        \"id\": \"...\",\n",
    "        \"dimension\": \"Structure|Content|Product|Other\",\n",
    "        \"quantity_kind\": \"count|area|length|monetary|...\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "**Author**: Benny Istanto/Risk Data Librarian/GFDRR  \n",
    "**Version**: 2026.1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.1 Import Dependencies\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any, Set\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    HAS_TQDM = True\n",
    "except ImportError:\n",
    "    HAS_TQDM = False\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.2 Define Paths\n",
    "\"\"\"\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BASE_DIR = NOTEBOOK_DIR.parent if NOTEBOOK_DIR.name == 'notebook' else NOTEBOOK_DIR\n",
    "\n",
    "DATASET_METADATA_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'dataset_metadata'\n",
    "SIGNAL_DICT_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'config' / 'signal_dictionary.yaml'\n",
    "RDLS_SCHEMA_PATH = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'schema' / 'rdls_schema_v0.3.json'\n",
    "\n",
    "OUTPUT_DIR = BASE_DIR / 'hdx_dataset_metadata_dump' / 'rdls' / 'extracted'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert DATASET_METADATA_DIR.exists(), f\"Not found: {DATASET_METADATA_DIR}\"\n",
    "assert SIGNAL_DICT_PATH.exists(), f\"Not found: {SIGNAL_DICT_PATH}\"\n",
    "\n",
    "print(f\"Base: {BASE_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.3 Load Signal Dictionary and RDLS Schema\n",
    "\"\"\"\n",
    "\n",
    "with open(SIGNAL_DICT_PATH, 'r', encoding='utf-8') as f:\n",
    "    SIGNAL_DICT = yaml.safe_load(f)\n",
    "\n",
    "with open(RDLS_SCHEMA_PATH, 'r', encoding='utf-8') as f:\n",
    "    RDLS_SCHEMA = json.load(f)\n",
    "\n",
    "# Extract valid exposure categories from schema\n",
    "VALID_EXPOSURE_CATEGORIES = RDLS_SCHEMA.get('$defs', {}).get('exposure_category', {}).get('enum', [])\n",
    "VALID_METRIC_DIMENSIONS = RDLS_SCHEMA.get('$defs', {}).get('metric_dimension', {}).get('enum', [])\n",
    "\n",
    "print(f\"Signal Dictionary loaded.\")\n",
    "print(f\"Valid exposure categories: {VALID_EXPOSURE_CATEGORIES}\")\n",
    "print(f\"Valid metric dimensions: {VALID_METRIC_DIMENSIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exposure Extraction Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.1 Data Classes for Exposure Extraction\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ExtractionMatch:\n",
    "    \"\"\"Single pattern match with confidence.\"\"\"\n",
    "    value: str\n",
    "    confidence: float\n",
    "    source_field: str\n",
    "    matched_text: str\n",
    "    pattern: str\n",
    "\n",
    "@dataclass\n",
    "class MetricExtraction:\n",
    "    \"\"\"Extracted metric information.\"\"\"\n",
    "    dimension: str\n",
    "    quantity_kind: str\n",
    "    confidence: float\n",
    "    source_hint: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class ExposureExtraction:\n",
    "    \"\"\"Complete exposure extraction for a dataset.\"\"\"\n",
    "    categories: List[ExtractionMatch] = field(default_factory=list)\n",
    "    metrics: List[MetricExtraction] = field(default_factory=list)\n",
    "    taxonomy_hint: Optional[str] = None\n",
    "    overall_confidence: float = 0.0\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'categories': [asdict(m) for m in self.categories],\n",
    "            'metrics': [asdict(m) for m in self.metrics],\n",
    "            'taxonomy_hint': self.taxonomy_hint,\n",
    "            'overall_confidence': self.overall_confidence\n",
    "        }\n",
    "\n",
    "print(\"Data classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.2 Metric Inference Patterns\n",
    "\n",
    "Additional patterns specific to exposure metrics.\n",
    "\"\"\"\n",
    "\n",
    "# Metric dimension inference patterns\n",
    "METRIC_DIMENSION_PATTERNS = {\n",
    "    'Structure': [\n",
    "        r'\\b(building|structure|footprint|floor.?area)\\b',\n",
    "        r'\\b(construction|built|asset)\\b',\n",
    "    ],\n",
    "    'Content': [\n",
    "        r'\\b(content|inventory|equipment|furnishing)\\b',\n",
    "        r'\\b(stock|goods|material)\\b',\n",
    "    ],\n",
    "    'Product': [\n",
    "        r'\\b(crop|harvest|yield|production)\\b',\n",
    "        r'\\b(output|commodity)\\b',\n",
    "    ],\n",
    "    'Other': [\n",
    "        r'\\b(other|miscellaneous|general)\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Quantity kind inference patterns\n",
    "QUANTITY_KIND_PATTERNS = {\n",
    "    'count': [\n",
    "        r'\\b(count|number|quantity|total)\\b',\n",
    "        r'\\b(units|items|pieces)\\b',\n",
    "    ],\n",
    "    'area': [\n",
    "        r'\\b(area|hectare|acre|sq\\.?\\s*(?:m|km|ft))\\b',\n",
    "        r'\\b(square|coverage|extent)\\b',\n",
    "    ],\n",
    "    'length': [\n",
    "        r'\\b(length|distance|km|kilometer|mile)\\b',\n",
    "        r'\\b(route|corridor|line)\\b',\n",
    "    ],\n",
    "    'monetary': [\n",
    "        r'\\b(value|cost|price|worth|\\$|usd|eur)\\b',\n",
    "        r'\\b(economic|financial|monetary)\\b',\n",
    "        r'\\b(replacement|rebuild)\\b',\n",
    "    ],\n",
    "    'weight': [\n",
    "        r'\\b(weight|mass|ton|kg|kilogram)\\b',\n",
    "        r'\\b(tonnage|cargo)\\b',\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Taxonomy hints\n",
    "TAXONOMY_PATTERNS = {\n",
    "    'GED4ALL': [r'\\b(ged4all|gem.?taxonomy)\\b'],\n",
    "    'HAZUS': [r'\\b(hazus|fema.?taxonomy)\\b'],\n",
    "    'PAGER': [r'\\b(pager|usgs.?pager)\\b'],\n",
    "}\n",
    "\n",
    "print(\"Metric inference patterns defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2.3 Exposure Extractor Class\n",
    "\"\"\"\n",
    "\n",
    "class ExposureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts RDLS Exposure block components from HDX metadata.\n",
    "    \n",
    "    Identifies:\n",
    "    - Exposure categories (buildings, infrastructure, population, etc.)\n",
    "    - Metric dimensions and quantity kinds\n",
    "    - Taxonomy hints\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal_dict : Dict[str, Any]\n",
    "        Loaded signal dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    CONFIDENCE_MAP = {'high': 0.9, 'medium': 0.7, 'low': 0.5}\n",
    "    \n",
    "    def __init__(self, signal_dict: Dict[str, Any]):\n",
    "        self.signal_dict = signal_dict\n",
    "        self._compile_patterns()\n",
    "    \n",
    "    def _compile_patterns(self) -> None:\n",
    "        \"\"\"Pre-compile regex patterns.\"\"\"\n",
    "        self.category_patterns = {}\n",
    "        self.dimension_patterns = {}\n",
    "        self.quantity_patterns = {}\n",
    "        self.taxonomy_patterns = {}\n",
    "        \n",
    "        # Compile exposure category patterns from signal dict\n",
    "        for category, config in self.signal_dict.get('exposure_category', {}).items():\n",
    "            patterns = config.get('patterns', [])\n",
    "            confidence = self.CONFIDENCE_MAP.get(config.get('confidence', 'medium'), 0.7)\n",
    "            self.category_patterns[category] = {\n",
    "                'compiled': [re.compile(p, re.IGNORECASE) for p in patterns],\n",
    "                'confidence': confidence\n",
    "            }\n",
    "        \n",
    "        # Compile metric dimension patterns\n",
    "        for dimension, patterns in METRIC_DIMENSION_PATTERNS.items():\n",
    "            self.dimension_patterns[dimension] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in patterns\n",
    "            ]\n",
    "        \n",
    "        # Compile quantity kind patterns\n",
    "        for kind, patterns in QUANTITY_KIND_PATTERNS.items():\n",
    "            self.quantity_patterns[kind] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in patterns\n",
    "            ]\n",
    "        \n",
    "        # Compile taxonomy patterns\n",
    "        for taxonomy, patterns in TAXONOMY_PATTERNS.items():\n",
    "            self.taxonomy_patterns[taxonomy] = [\n",
    "                re.compile(p, re.IGNORECASE) for p in patterns\n",
    "            ]\n",
    "    \n",
    "    def _extract_text_fields(self, hdx_record: Dict[str, Any]) -> Dict[str, str]:\n",
    "        \"\"\"Extract text fields from HDX record.\"\"\"\n",
    "        fields = {\n",
    "            'title': hdx_record.get('title', ''),\n",
    "            'name': hdx_record.get('name', ''),\n",
    "            'notes': hdx_record.get('notes', ''),\n",
    "            'tags': ' '.join(hdx_record.get('tags', [])),\n",
    "            'methodology': hdx_record.get('methodology_other', '') or '',\n",
    "        }\n",
    "        \n",
    "        resources = hdx_record.get('resources', [])\n",
    "        fields['resources'] = ' '.join(\n",
    "            f\"{r.get('name', '')} {r.get('description', '')}\" \n",
    "            for r in resources\n",
    "        )\n",
    "        \n",
    "        return fields\n",
    "    \n",
    "    def _match_categories(\n",
    "        self, \n",
    "        text_fields: Dict[str, str]\n",
    "    ) -> List[ExtractionMatch]:\n",
    "        \"\"\"Match exposure categories.\"\"\"\n",
    "        matches = {}\n",
    "        \n",
    "        for category, config in self.category_patterns.items():\n",
    "            for compiled in config['compiled']:\n",
    "                for field_name, text in text_fields.items():\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    match = compiled.search(text)\n",
    "                    if match:\n",
    "                        if category not in matches or config['confidence'] > matches[category].confidence:\n",
    "                            matches[category] = ExtractionMatch(\n",
    "                                value=category,\n",
    "                                confidence=config['confidence'],\n",
    "                                source_field=field_name,\n",
    "                                matched_text=match.group(0),\n",
    "                                pattern=compiled.pattern\n",
    "                            )\n",
    "                        break\n",
    "        \n",
    "        return list(matches.values())\n",
    "    \n",
    "    def _infer_metrics(\n",
    "        self, \n",
    "        text_fields: Dict[str, str],\n",
    "        categories: List[ExtractionMatch]\n",
    "    ) -> List[MetricExtraction]:\n",
    "        \"\"\"\n",
    "        Infer metrics based on detected categories and text patterns.\n",
    "        \"\"\"\n",
    "        all_text = ' '.join(text_fields.values()).lower()\n",
    "        metrics = []\n",
    "        \n",
    "        # Default metrics by category\n",
    "        default_metrics = {\n",
    "            'buildings': [('Structure', 'count'), ('Structure', 'area'), ('Structure', 'monetary')],\n",
    "            'infrastructure': [('Structure', 'count'), ('Structure', 'length')],\n",
    "            'population': [('Other', 'count')],\n",
    "            'agriculture': [('Product', 'area'), ('Product', 'monetary')],\n",
    "            'natural_environment': [('Other', 'area')],\n",
    "        }\n",
    "        \n",
    "        # Detect dimension from text\n",
    "        detected_dimension = None\n",
    "        for dimension, patterns in self.dimension_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern.search(all_text):\n",
    "                    detected_dimension = dimension\n",
    "                    break\n",
    "            if detected_dimension:\n",
    "                break\n",
    "        \n",
    "        # Detect quantity kind from text\n",
    "        detected_quantity = None\n",
    "        for kind, patterns in self.quantity_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern.search(all_text):\n",
    "                    detected_quantity = kind\n",
    "                    break\n",
    "            if detected_quantity:\n",
    "                break\n",
    "        \n",
    "        # Generate metrics for each detected category\n",
    "        for cat_match in categories:\n",
    "            category = cat_match.value\n",
    "            \n",
    "            if detected_dimension and detected_quantity:\n",
    "                # Use detected values\n",
    "                metrics.append(MetricExtraction(\n",
    "                    dimension=detected_dimension,\n",
    "                    quantity_kind=detected_quantity,\n",
    "                    confidence=0.8,\n",
    "                    source_hint='detected_from_text'\n",
    "                ))\n",
    "            elif category in default_metrics:\n",
    "                # Use defaults for the category\n",
    "                for dim, qty in default_metrics[category][:1]:  # Just first default\n",
    "                    metrics.append(MetricExtraction(\n",
    "                        dimension=dim,\n",
    "                        quantity_kind=qty,\n",
    "                        confidence=0.5,\n",
    "                        source_hint=f'default_for_{category}'\n",
    "                    ))\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _detect_taxonomy(self, text_fields: Dict[str, str]) -> Optional[str]:\n",
    "        \"\"\"Detect taxonomy scheme from text.\"\"\"\n",
    "        all_text = ' '.join(text_fields.values()).lower()\n",
    "        \n",
    "        for taxonomy, patterns in self.taxonomy_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern.search(all_text):\n",
    "                    return taxonomy\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract(self, hdx_record: Dict[str, Any]) -> ExposureExtraction:\n",
    "        \"\"\"\n",
    "        Extract exposure information from HDX record.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hdx_record : Dict[str, Any]\n",
    "            HDX metadata record\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ExposureExtraction\n",
    "            Extraction results\n",
    "        \"\"\"\n",
    "        text_fields = self._extract_text_fields(hdx_record)\n",
    "        \n",
    "        categories = self._match_categories(text_fields)\n",
    "        metrics = self._infer_metrics(text_fields, categories)\n",
    "        taxonomy = self._detect_taxonomy(text_fields)\n",
    "        \n",
    "        # Calculate confidence\n",
    "        confidences = [c.confidence for c in categories]\n",
    "        confidences.extend([m.confidence for m in metrics])\n",
    "        overall_confidence = np.mean(confidences) if confidences else 0.0\n",
    "        \n",
    "        return ExposureExtraction(\n",
    "            categories=categories,\n",
    "            metrics=metrics,\n",
    "            taxonomy_hint=taxonomy,\n",
    "            overall_confidence=overall_confidence\n",
    "        )\n",
    "\n",
    "# Initialize\n",
    "exposure_extractor = ExposureExtractor(SIGNAL_DICT)\n",
    "print(f\"ExposureExtractor initialized.\")\n",
    "print(f\"  - Categories: {len(exposure_extractor.category_patterns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RDLS Exposure Block Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3.1 Build RDLS Exposure Block\n",
    "\"\"\"\n",
    "\n",
    "def build_exposure_block(\n",
    "    extraction: ExposureExtraction,\n",
    "    dataset_id: str\n",
    ") -> Optional[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Build RDLS exposure block from extraction results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    extraction : ExposureExtraction\n",
    "        Extraction results\n",
    "    dataset_id : str\n",
    "        Dataset identifier\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Optional[List[Dict[str, Any]]]\n",
    "        RDLS exposure array or None\n",
    "    \"\"\"\n",
    "    if not extraction.categories:\n",
    "        return None\n",
    "    \n",
    "    exposure_items = []\n",
    "    \n",
    "    for i, cat in enumerate(extraction.categories):\n",
    "        exposure_item = {\n",
    "            'id': f\"exposure_{dataset_id[:8]}_{i+1}\",\n",
    "            'category': cat.value\n",
    "        }\n",
    "        \n",
    "        # Add taxonomy if detected\n",
    "        if extraction.taxonomy_hint:\n",
    "            exposure_item['taxonomy'] = extraction.taxonomy_hint\n",
    "        \n",
    "        # Add metrics\n",
    "        if extraction.metrics:\n",
    "            metrics = []\n",
    "            for j, m in enumerate(extraction.metrics):\n",
    "                metric = {\n",
    "                    'id': f\"metric_{dataset_id[:8]}_{i+1}_{j+1}\",\n",
    "                    'dimension': m.dimension,\n",
    "                    'quantity_kind': m.quantity_kind\n",
    "                }\n",
    "                metrics.append(metric)\n",
    "            exposure_item['metrics'] = metrics\n",
    "        else:\n",
    "            # Minimal required metric\n",
    "            exposure_item['metrics'] = [{\n",
    "                'id': f\"metric_{dataset_id[:8]}_{i+1}_1\",\n",
    "                'dimension': 'Other',\n",
    "                'quantity_kind': 'count'\n",
    "            }]\n",
    "        \n",
    "        exposure_items.append(exposure_item)\n",
    "    \n",
    "    return exposure_items\n",
    "\n",
    "print(\"Exposure block builder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.1 Load Sample Records\n",
    "\"\"\"\n",
    "\n",
    "# Find exposure-related files\n",
    "exposure_files = list(DATASET_METADATA_DIR.glob('*exposure*.json'))[:10]\n",
    "population_files = list(DATASET_METADATA_DIR.glob('*population*.json'))[:5]\n",
    "building_files = list(DATASET_METADATA_DIR.glob('*building*.json'))[:5]\n",
    "\n",
    "sample_files = exposure_files + population_files + building_files\n",
    "\n",
    "sample_records = []\n",
    "for filepath in sample_files:\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            sample_records.append(json.load(f))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Loaded {len(sample_records)} sample records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.2 Run Extraction on Samples\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPOSURE EXTRACTION TEST RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for record in sample_records[:10]:\n",
    "    extraction = exposure_extractor.extract(record)\n",
    "    \n",
    "    print(f\"\\n{'─' * 80}\")\n",
    "    print(f\"Title: {record.get('title', '')[:70]}\")\n",
    "    print(f\"Confidence: {extraction.overall_confidence:.2f}\")\n",
    "    \n",
    "    if extraction.categories:\n",
    "        print(f\"Categories:\")\n",
    "        for cat in extraction.categories:\n",
    "            print(f\"  - {cat.value} (conf: {cat.confidence:.1f}, match: '{cat.matched_text}')\")\n",
    "    else:\n",
    "        print(f\"Categories: None detected\")\n",
    "    \n",
    "    if extraction.metrics:\n",
    "        print(f\"Metrics:\")\n",
    "        for m in extraction.metrics:\n",
    "            print(f\"  - {m.dimension}/{m.quantity_kind} (conf: {m.confidence:.1f})\")\n",
    "    \n",
    "    if extraction.taxonomy_hint:\n",
    "        print(f\"Taxonomy: {extraction.taxonomy_hint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4.3 Generate Sample RDLS Exposure Blocks\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED RDLS EXPOSURE BLOCKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for record in sample_records[:5]:\n",
    "    extraction = exposure_extractor.extract(record)\n",
    "    \n",
    "    if extraction.categories:\n",
    "        exposure_block = build_exposure_block(extraction, record.get('id', 'unknown'))\n",
    "        \n",
    "        if exposure_block:\n",
    "            print(f\"\\n{'─' * 80}\")\n",
    "            print(f\"Dataset: {record.get('title', '')[:60]}\")\n",
    "            print(json.dumps(exposure_block, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.1 Process All Records\n",
    "\"\"\"\n",
    "\n",
    "def process_exposure_extraction(\n",
    "    metadata_dir: Path,\n",
    "    extractor: ExposureExtractor,\n",
    "    limit: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all HDX records for exposure extraction.\n",
    "    \"\"\"\n",
    "    json_files = list(metadata_dir.glob('*.json'))\n",
    "    if limit:\n",
    "        json_files = json_files[:limit]\n",
    "    \n",
    "    results = []\n",
    "    iterator = tqdm(json_files, desc=\"Extracting\") if HAS_TQDM else json_files\n",
    "    \n",
    "    for filepath in iterator:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                record = json.load(f)\n",
    "            \n",
    "            extraction = extractor.extract(record)\n",
    "            \n",
    "            results.append({\n",
    "                'id': record.get('id'),\n",
    "                'title': record.get('title'),\n",
    "                'organization': record.get('organization'),\n",
    "                'categories': [c.value for c in extraction.categories],\n",
    "                'category_count': len(extraction.categories),\n",
    "                'has_exposure': len(extraction.categories) > 0,\n",
    "                'taxonomy': extraction.taxonomy_hint,\n",
    "                'overall_confidence': extraction.overall_confidence,\n",
    "                'extraction': extraction\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({'id': filepath.stem, 'error': str(e)})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process\n",
    "PROCESS_LIMIT = 1000  # Set to None for full corpus\n",
    "\n",
    "print(f\"Processing {PROCESS_LIMIT or 'all'} records...\")\n",
    "df_exposure = process_exposure_extraction(DATASET_METADATA_DIR, exposure_extractor, limit=PROCESS_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5.2 Extraction Statistics\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPOSURE EXTRACTION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_exposure)\n",
    "with_exposure = df_exposure['has_exposure'].sum()\n",
    "\n",
    "print(f\"\\nTotal records: {total:,}\")\n",
    "print(f\"With exposure signals: {with_exposure:,} ({with_exposure/total*100:.1f}%)\")\n",
    "\n",
    "# Category distribution\n",
    "cat_counts = Counter()\n",
    "for cats in df_exposure['categories'].dropna():\n",
    "    cat_counts.update(cats)\n",
    "\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "for cat, count in cat_counts.most_common():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "# Confidence distribution\n",
    "conf = df_exposure[df_exposure['has_exposure']]['overall_confidence']\n",
    "print(f\"\\nConfidence (exposure records):\")\n",
    "print(f\"  Mean: {conf.mean():.2f}\")\n",
    "print(f\"  High (>=0.8): {(conf >= 0.8).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.1 Export Extraction Results\n",
    "\"\"\"\n",
    "\n",
    "# Prepare export\n",
    "export_df = df_exposure[[\n",
    "    'id', 'title', 'organization', 'categories', 'category_count',\n",
    "    'taxonomy', 'overall_confidence', 'has_exposure'\n",
    "]].copy()\n",
    "\n",
    "export_df['categories'] = export_df['categories'].apply(\n",
    "    lambda x: '|'.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "\n",
    "output_file = OUTPUT_DIR / 'exposure_extraction_results.csv'\n",
    "export_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved: {output_file}\")\n",
    "\n",
    "# High confidence\n",
    "high_conf = export_df[export_df['has_exposure'] & (df_exposure['overall_confidence'] >= 0.8)]\n",
    "high_conf_file = OUTPUT_DIR / 'exposure_extraction_high_confidence.csv'\n",
    "high_conf.to_csv(high_conf_file, index=False)\n",
    "print(f\"Saved: {high_conf_file} ({len(high_conf)} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6.2 Generate Sample RDLS Records\n",
    "\"\"\"\n",
    "\n",
    "top_records = df_exposure[\n",
    "    df_exposure['has_exposure'] & \n",
    "    (df_exposure['overall_confidence'] >= 0.8)\n",
    "].nlargest(10, 'overall_confidence')\n",
    "\n",
    "print(f\"\\nGenerating {len(top_records)} sample RDLS exposure records...\")\n",
    "\n",
    "for idx, row in top_records.iterrows():\n",
    "    extraction = row['extraction']\n",
    "    exposure_block = build_exposure_block(extraction, row['id'])\n",
    "    \n",
    "    if exposure_block:\n",
    "        rdls_record = {\n",
    "            'datasets': [{\n",
    "                'id': f\"rdls_exp-hdx_{row['id'][:8]}\",\n",
    "                'title': row['title'],\n",
    "                'risk_data_type': ['exposure'],\n",
    "                'exposure': exposure_block,\n",
    "                'links': [{\n",
    "                    'href': 'https://docs.riskdatalibrary.org/en/0__3__0/rdls_schema.json',\n",
    "                    'rel': 'describedby'\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "        \n",
    "        output_path = OUTPUT_DIR / f\"rdls_exp-hdx_{row['id'][:8]}.json\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(rdls_record, f, indent=2)\n",
    "        \n",
    "        print(f\"  Created: {output_path.name}\")\n",
    "\n",
    "print(f\"\\nDone. Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNotebook completed: {datetime.now().isoformat()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
