# 11 - Vulnerability & Loss Extractor

**Notebook:** `notebook/11_rdls_vulnerability_loss_extractor.ipynb`

---

## Summary

Extracts detailed vulnerability and loss information from HDX datasets, producing schema-compliant RDLS v0.3 blocks with constraint-validated function types, impact metrics, asset categories, and approach/relationship fields.

**For Decision Makers:**
> This notebook identifies datasets about susceptibility factors (vulnerability functions, fragility curves) and historical/projected impacts (loss records). All output fields are validated against the RDLS schema through constraint tables that enforce correct field combinations — no fabricated or invalid default values.

---

## Inputs

| Input | Path | Description |
|-------|------|-------------|
| Dataset JSONs | `dataset_metadata/*.json` | Raw HDX metadata (26,246 files) |
| Signal dictionary | `config/signal_dictionary.yaml` | Vulnerability and loss signal patterns from Step 08 |
| Classification | `derived/classification_final.csv` | From Step 05 |
| RDLS Schema | `rdls/schema/rdls_schema_v0.3.json` | Codelist enums for validation |

---

## Outputs

| Output | Path | Description |
|--------|------|-------------|
| Vulnerability CSV | `rdls/extracted/vulnerability_extraction_results.csv` | All records with vulnerability flags |
| Vulnerability records | `rdls/extracted/vulnerability_detected_records.csv` | Detected vulnerability records |
| Loss CSV | `rdls/extracted/loss_extraction_results.csv` | All records with loss flags |
| Loss records | `rdls/extracted/loss_detected_records.csv` | Detected loss records |
| V/L JSON blocks | `rdls/extracted/rdls_vln-hdx_*.json`, `rdls/extracted/rdls_lss-hdx_*.json` | Individual RDLS records |

---

## Notebook Structure

The notebook has two major sections sharing a common constraint table:

| Cells | Section | Content |
|-------|---------|---------|
| 1–4 | Setup | Imports, paths, schema loading |
| 5–6 | Shared constraints | `IMPACT_METRIC_CONSTRAINTS` (used by both V and L) |
| 6 | Vulnerability constraints | `FUNCTION_TYPE_CONSTRAINTS`, `VULN_CATEGORY_DEFAULTS`, `FUNCTION_TYPE_APPROACH_DEFAULTS` |
| 7–8 | Vulnerability extractor | Detection patterns, `VulnerabilityExtractor` class |
| 9–10 | Vulnerability builder | `build_vulnerability_block()` |
| 11–20 | Vulnerability batch | Test samples, batch processing, export |
| 21–23 | Loss constraints | `VALID_ASSET_TRIPLETS`, `LOSS_SIGNAL_DEFAULTS`, `LOSS_TYPE_APPROACH_RULES` |
| 24 | Loss extractor | `LossExtractor` class |
| 25–26 | Loss builder | `build_loss_block()` |
| 27–38 | Loss batch | Test samples, batch processing, export |

---

## Shared Constraint: IMPACT_METRIC_CONSTRAINTS

Defined in cell 6 and shared between vulnerability and loss extractors. Maps all **20 RDLS impact_metric values** to their expected `quantity_kind` and valid `impact_type`:

| Impact Metric | Quantity Kind | Valid Impact Types |
|---------------|---------------|--------------------|
| `damage_ratio` | ratio | direct |
| `mean_damage_ratio` | ratio | direct |
| `loss_ratio` | ratio | direct, indirect, total |
| `mean_loss_ratio` | ratio | direct, indirect, total |
| `economic_loss_value` | monetary | direct, indirect, total |
| `insured_loss_value` | monetary | direct, indirect, total |
| `asset_loss` | monetary | direct |
| `casualty_count` | count | direct |
| `casualty_ratio_vulnerability` | ratio | direct |
| `displaced_count` | count | direct, indirect |
| `displaced_ratio` | ratio | direct, indirect |
| `probability` | ratio | direct |
| `damage_index` | count | direct |
| `downtime_vulnerability` | time | indirect |
| `downtime_loss` | time | indirect |
| `crop_loss_value` | monetary | direct |
| `livestock_loss_value` | monetary | direct |
| `content_loss_ratio` | ratio | direct |
| `structural_loss_ratio` | ratio | direct |
| `functionality_loss` | ratio | direct, indirect |

---

## Vulnerability Component

### RDLS Vulnerability Schema

The RDLS v0.3 vulnerability block supports four function types:

```
vulnerability
└── functions[]
    ├── vulnerability     — Damage ratio given intensity
    ├── fragility         — Probability of damage state given intensity
    ├── damage_to_loss    — Loss given damage state
    └── engineering_demand — Engineering response parameters
└── socio_economic
    ├── scheme, indicator_name, indicator_code
    └── description
```

### Constraint Tables

#### Group 1: FUNCTION_TYPE_CONSTRAINTS

Maps each function type to its default impact metric and allowed metric set:

| Function Type | Default Metric | Allowed Metrics |
|---------------|----------------|-----------------|
| **vulnerability** | `damage_ratio` | damage_ratio, mean_damage_ratio, casualty_ratio_vulnerability, downtime_vulnerability |
| **fragility** | `probability` | probability, damage_index, damage_ratio, mean_damage_ratio |
| **damage_to_loss** | `loss_ratio` | loss_ratio, mean_loss_ratio, economic_loss_value, insured_loss_value, asset_loss, downtime_loss, casualty_count, displaced_count |
| **engineering_demand** | `damage_index` | damage_index, damage_ratio, mean_damage_ratio, probability |

#### Group 2: VULN_CATEGORY_DEFAULTS

Maps exposure category context to typical function type and metric overrides:

| Category | Typical Function | Metric Override | Qty Override |
|----------|------------------|-----------------|--------------|
| buildings | vulnerability | damage_ratio | ratio |
| infrastructure | fragility | probability | ratio |
| population | vulnerability | casualty_ratio_vulnerability | ratio |
| agriculture | damage_to_loss | crop_loss_value | monetary |
| natural_environment | damage_to_loss | functionality_loss | ratio |
| economic_indicator | damage_to_loss | economic_loss_value | monetary |
| development_index | vulnerability | damage_index | count |

#### Group 3: IMPACT_METRIC_CONSTRAINTS (shared)

See [Shared Constraint](#shared-constraint-impact_metric_constraints) above.

#### Group 4: FUNCTION_TYPE_APPROACH_DEFAULTS

Maps function type to typical approach and relationship:

| Function Type | Typical Approach | Typical Relationship |
|---------------|------------------|---------------------|
| vulnerability | analytical | mathematical |
| fragility | empirical | mathematical |
| damage_to_loss | analytical | mathematical |
| engineering_demand | analytical | mathematical |

### VulnerabilityExtractor

```python
extractor = VulnerabilityExtractor(signal_dict, schema_enums)
result = extractor.extract(hdx_record)
# result.has_vulnerability: bool
# result.functions: List[FunctionExtraction]
# result.socio_economic: Optional[SocioEconomicExtraction]
```

Key methods:
- `_detect_function_types()` — Identify which function types are present
- `_validate_function_metrics()` — Enforce Groups 1, 2, 3 constraints
- `_infer_approach()` / `_infer_relationship()` — Use Group 4 defaults as fallback
- `_infer_category()` — Detect exposure category context
- `_infer_hazard_context()` — Link to triggering hazard type
- `_extract_socio_economic()` — Detect socioeconomic indicator schemes

### build_vulnerability_block()

Converts extraction to RDLS v0.3 JSON:
- Re-validates against `FUNCTION_TYPE_CONSTRAINTS` (Group 1)
- Re-validates against `IMPACT_METRIC_CONSTRAINTS` (Group 3)
- References `FUNCTION_TYPE_APPROACH_DEFAULTS` for fallbacks
- Populates `approach`, `relationship`, `hazard_primary`, `category`, `impact` fields

---

## Loss Component

### RDLS Loss Schema

```
loss
└── losses[]
    ├── loss_type           (6 closed values)
    ├── loss_approach       (3 closed values)
    ├── asset_category      (7 values, same as exposure_category)
    ├── asset_dimension     (6 values, same as metric_dimension)
    ├── impact
    │   ├── metric          (20 closed values)
    │   ├── quantity_kind
    │   ├── impact_type     (direct/indirect/total)
    │   └── unit, currency
    └── exposure_to_hazard  (links to hazard/exposure)
```

### Constraint Tables

#### Group 1: VALID_ASSET_TRIPLETS

Maps `asset_category` to allowed `asset_dimension` values:

| Asset Category | Allowed Dimensions |
|----------------|-------------------|
| **agriculture** | structure, product |
| **buildings** | structure, content |
| **infrastructure** | structure, disruption |
| **population** | population |
| **natural_environment** | structure |
| **economic_indicator** | product, index |
| **development_index** | index |

#### Group 2: LOSS_SIGNAL_DEFAULTS

Unified per-signal defaults replacing separate lookup dictionaries. Each signal type maps to a complete set of loss fields:

| Signal | Loss Type | Asset Category | Impact Metric | Quantity Kind |
|--------|-----------|----------------|---------------|---------------|
| `casualties` | ground_up | population | casualty_count | count |
| `economic_damage` | ground_up | buildings | economic_loss_value | monetary |
| `displacement` | ground_up | population | displaced_count | count |
| `crop_loss` | ground_up | agriculture | crop_loss_value | monetary |
| `infrastructure_damage` | ground_up | infrastructure | asset_loss | monetary |
| `insured_loss` | insured | buildings | insured_loss_value | monetary |
| `structural_damage` | ground_up | buildings | structural_loss_ratio | ratio |
| `content_damage` | ground_up | buildings | content_loss_ratio | ratio |

#### Group 3: LOSS_TYPE_APPROACH_RULES

Constrains which `loss_approach` values are valid for each `loss_type`:

| Loss Type | Allowed Approaches |
|-----------|-------------------|
| `ground_up` | analytical, empirical, hybrid |
| `insured` | analytical, empirical, hybrid |
| `reinsured` | analytical |
| `gross` | analytical, empirical |
| `net` | analytical |
| `total` | analytical, empirical, hybrid |

### LossExtractor

```python
extractor = LossExtractor(signal_dict, schema_enums)
result = extractor.extract(hdx_record)
# result.has_loss: bool
# result.losses: List[LossExtraction]
```

Key methods:
- `_detect_loss_signals()` — Match signal patterns from dictionary
- `_validate_impact_metric()` — Enforce `IMPACT_METRIC_CONSTRAINTS` (Group 2 shared)
- `_validate_loss_approach()` — Enforce `LOSS_TYPE_APPROACH_RULES` (Group 3)
- `_infer_asset_context()` — Validate against `VALID_ASSET_TRIPLETS` (Group 1)

### build_loss_block()

Converts extraction to RDLS v0.3 JSON:
- Re-validates `VALID_ASSET_TRIPLETS` at build time
- Re-validates `IMPACT_METRIC_CONSTRAINTS` at build time
- Enforces `currency` only when `quantity_kind == monetary`
- Populates `exposure_to_hazard` linking to hazard/exposure context

---

## Constraint Validation Flow

Both extractors follow the same validation chain:

```
1. Detect signals from text patterns
2. Look up initial defaults (LOSS_SIGNAL_DEFAULTS or VULN_CATEGORY_DEFAULTS)
3. Validate impact metric against IMPACT_METRIC_CONSTRAINTS
   → If invalid, fall back to function/signal type default
4. Validate field combinations against type-specific constraints
   → FUNCTION_TYPE_CONSTRAINTS (vulnerability)
   → VALID_ASSET_TRIPLETS + LOSS_TYPE_APPROACH_RULES (loss)
5. Re-validate at block build time (defence in depth)
```

This ensures no fabricated or schema-invalid values appear in the output.

---

## How It Works

```
1. Load all 26,246 dataset metadata files
2. For each dataset (vulnerability):
   a. Scan for vulnerability signal patterns
   b. Detect function types (vulnerability, fragility, damage_to_loss, engineering_demand)
   c. Infer exposure category and hazard context
   d. Validate all fields against constraint tables
   e. Extract socioeconomic indicators if present
3. For each dataset (loss):
   a. Scan for loss signal patterns (casualties, economic damage, etc.)
   b. Look up unified defaults from LOSS_SIGNAL_DEFAULTS
   c. Validate asset category/dimension against VALID_ASSET_TRIPLETS
   d. Validate impact metric against IMPACT_METRIC_CONSTRAINTS
   e. Validate loss type/approach against LOSS_TYPE_APPROACH_RULES
4. Build RDLS blocks with double-validated fields
5. Export extraction CSVs and individual JSON files
```

---

## Troubleshooting

### Unexpected field values
- All fields are constraint-validated — check the constraint tables
- If a value seems wrong, it is the best valid fallback the constraints allow
- Review `FUNCTION_TYPE_CONSTRAINTS` or `LOSS_SIGNAL_DEFAULTS` for the specific signal

### Low vulnerability detection
- Vulnerability datasets are less common (~7–10% of HDX catalogue)
- Many datasets lack explicit vulnerability function terminology
- Detection rate of 5–10% is normal

### Missing hazard link
- Not all loss data specifies the triggering hazard
- Historical event data often lacks structured hazard context
- `exposure_to_hazard` is populated when hazard signals are detected in the same dataset

### Ambiguous vulnerability vs. loss
Some datasets blur the line (e.g., "food insecurity"):
- Measuring susceptibility → vulnerability
- Measuring actual outcomes → loss
- Both components may be extracted if appropriate

---

[← Previous: Exposure Extractor](10_exposure_extractor.md) | [Back to README](../README.md) | [Next: HEVL Integration →](12_hevl_integration.md)
